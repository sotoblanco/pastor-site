{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d8aa8129-0c59-476a-b314-357c443d2475",
   "metadata": {},
   "source": [
    "---\n",
    "title: Basics\n",
    "description: Torch Serve basics\n",
    "filters:\n",
    "   - include-code-files\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b084b870-7092-4adb-b5aa-1da04d8f1699",
   "metadata": {},
   "source": [
    "## Model archiver\n",
    "\n",
    "The key to understanding TorchServe is to first understand [torch-model-archiver](https://github.com/pytorch/serve/blob/master/model-archiver/README.md) which packages model artifacts into a single model archive file (`.mar`).  `torch-model-archive` needs the following inputs:\n",
    "\n",
    "### Torchscript\n",
    "\n",
    "Need a model checkpoint file\n",
    "\n",
    "### Eager Mode (more common)\n",
    "\n",
    "Need a model definition file and a state_dict file.\n",
    "\n",
    "### CLI\n",
    "\n",
    "The CLI produces a `.mar` file.  Below is [an example](https://github.com/pytorch/serve/blob/master/docs/getting_started.md#serve-a-model) of archiving an eager mode model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d5e90e3-6b87-4759-b113-61e16eaacdb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING - Overwriting model_store/densenet161.mar ...\n"
     ]
    }
   ],
   "source": [
    "!torch-model-archiver --model-name densenet161 \\\n",
    "    --version 1.0 \\\n",
    "    --model-file ./_serve/examples/image_classifier/densenet_161/model.py \\\n",
    "    --serialized-file densenet161-8d451a50.pth \\\n",
    "    --export-path model_store \\\n",
    "    --extra-files ./_serve/examples/image_classifier/index_to_name.json \\\n",
    "    --handler image_classifier \\\n",
    "    -f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb16f1e-d726-4592-8d9b-51e1c35a5193",
   "metadata": {},
   "source": [
    "This is the model file:\n",
    "\n",
    "```{.python include=\"./_serve/examples/image_classifier/densenet_161/model.py\" filename=\"_serve/examples/image_classifier/densenet_161/model.py\"}\n",
    "```\n",
    "\n",
    "Options for model archiver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17f5e0ee-2b11-406b-bc5a-6d284f11368a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: torch-model-archiver [-h] --model-name MODEL_NAME\n",
      "                            [--serialized-file SERIALIZED_FILE]\n",
      "                            [--model-file MODEL_FILE] --handler HANDLER\n",
      "                            [--extra-files EXTRA_FILES]\n",
      "                            [--runtime {python,python2,python3}]\n",
      "                            [--export-path EXPORT_PATH]\n",
      "                            [--archive-format {tgz,no-archive,default}] [-f]\n",
      "                            -v VERSION [-r REQUIREMENTS_FILE]\n",
      "\n",
      "Torch Model Archiver Tool\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  --model-name MODEL_NAME\n",
      "                        Exported model name. Exported file will be named as\n",
      "                        model-name.mar and saved in current working directory if no --export-path is\n",
      "                        specified, else it will be saved under the export path\n",
      "  --serialized-file SERIALIZED_FILE\n",
      "                        Path to .pt or .pth file containing state_dict in case of eager mode\n",
      "                        or an executable ScriptModule in case of TorchScript or TensorRT\n",
      "                        or a .onnx file in the case of ORT.\n",
      "  --model-file MODEL_FILE\n",
      "                        Path to python file containing model architecture.\n",
      "                        This parameter is mandatory for eager mode models.\n",
      "                        The model architecture file must contain only one\n",
      "                        class definition extended from torch.nn.modules.\n",
      "  --handler HANDLER     TorchServe's default handler name\n",
      "                         or Handler path to handle custom inference logic.\n",
      "  --extra-files EXTRA_FILES\n",
      "                        Comma separated path to extra dependency files.\n",
      "  --runtime {python,python2,python3}\n",
      "                        The runtime specifies which language to run your inference code on.\n",
      "                        The default runtime is \"python\".\n",
      "  --export-path EXPORT_PATH\n",
      "                        Path where the exported .mar file will be saved. This is an optional\n",
      "                        parameter. If --export-path is not specified, the file will be saved in the\n",
      "                        current working directory. \n",
      "  --archive-format {tgz,no-archive,default}\n",
      "                        The format in which the model artifacts are archived.\n",
      "                        \"tgz\": This creates the model-archive in <model-name>.tar.gz format.\n",
      "                        If platform hosting TorchServe requires model-artifacts to be in \".tar.gz\"\n",
      "                        use this option.\n",
      "                        \"no-archive\": This option creates an non-archived version of model artifacts\n",
      "                        at \"export-path/{model-name}\" location. As a result of this choice, \n",
      "                        MANIFEST file will be created at \"export-path/{model-name}\" location\n",
      "                        without archiving these model files\n",
      "                        \"default\": This creates the model-archive in <model-name>.mar format.\n",
      "                        This is the default archiving format. Models archived in this format\n",
      "                        will be readily hostable on native TorchServe.\n",
      "  -f, --force           When the -f or --force flag is specified, an existing .mar file with same\n",
      "                        name as that provided in --model-name in the path specified by --export-path\n",
      "                        will overwritten\n",
      "  -v VERSION, --version VERSION\n",
      "                        Model's version\n",
      "  -r REQUIREMENTS_FILE, --requirements-file REQUIREMENTS_FILE\n",
      "                        Path to a requirements.txt containing model specific python dependency\n",
      "                         packages.\n"
     ]
    }
   ],
   "source": [
    "! torch-model-archiver --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249917ba-8e79-4dfc-a3a6-017b62e9ae74",
   "metadata": {},
   "source": [
    "### Handler\n",
    "\n",
    "TorchServe has the following handlers built-in that do post and pre-processing:\n",
    "\n",
    "- image_classifier\n",
    "- object_detector\n",
    "- text_classifier\n",
    "- image_segmenter\n",
    "\n",
    "You can implement your own custom handler by following [these docs](https://pytorch.org/serve/custom_service.html?highlight=handlers). Most of the time you only need to subclass `BaseHandler` and override `preprocess` and/or `postprocess`.\n",
    "\n",
    "\n",
    "\n",
    "##### `--extra-files ... index_to_name.json`:\n",
    "\n",
    "From the docs:\n",
    "\n",
    "> `image_classifier`, `text_classifier` and `object_detector` can all automatically map from numeric classes (0,1,2...) to friendly strings. To do this, simply include in your model archive a file, `index_to_name.json`, that contains a mapping of class number (as a string) to friendly name (also as a string)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "314e9875-52ec-4d90-9872-dfc611c19135",
   "metadata": {},
   "source": [
    "## Serving\n",
    "\n",
    "After archiving you can start the modeling server:\n",
    "\n",
    "```bash\n",
    "torchserve --start --ncs \\\n",
    "    --model-store model_store \\\n",
    "    --models densenet161.mar\n",
    "```\n",
    "\n",
    "TorchServe uses default ports `8080` / `8081` / `8082` for REST based inference, management & metrics APIs and `7070` / `7071` for gRPC APIs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34fe7243-dd4f-4636-8571-3c7babb9899e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: torchserve [-h] [-v | --start | --stop] [--ts-config TS_CONFIG]\n",
      "                  [--model-store MODEL_STORE]\n",
      "                  [--workflow-store WORKFLOW_STORE]\n",
      "                  [--models MODEL_PATH1 MODEL_NAME=MODEL_PATH2... [MODEL_PATH1 MODEL_NAME=MODEL_PATH2... ...]]\n",
      "                  [--log-config LOG_CONFIG] [--foreground]\n",
      "                  [--no-config-snapshots] [--plugins-path PLUGINS_PATH]\n",
      "\n",
      "Torchserve\n",
      "\n",
      "optional arguments:\n",
      "  -h, --help            show this help message and exit\n",
      "  -v, --version         Return TorchServe Version\n",
      "  --start               Start the model-server\n",
      "  --stop                Stop the model-server\n",
      "  --ts-config TS_CONFIG\n",
      "                        Configuration file for model server\n",
      "  --model-store MODEL_STORE\n",
      "                        Model store location from where local or default\n",
      "                        models can be loaded\n",
      "  --workflow-store WORKFLOW_STORE\n",
      "                        Workflow store location from where local or default\n",
      "                        workflows can be loaded\n",
      "  --models MODEL_PATH1 MODEL_NAME=MODEL_PATH2... [MODEL_PATH1 MODEL_NAME=MODEL_PATH2... ...]\n",
      "                        Models to be loaded using [model_name=]model_location\n",
      "                        format. Location can be a HTTP URL or a model archive\n",
      "                        file in MODEL_STORE.\n",
      "  --log-config LOG_CONFIG\n",
      "                        Log4j configuration file for model server\n",
      "  --foreground          Run the model server in foreground. If this option is\n",
      "                        disabled, the model server will run in the background.\n",
      "  --no-config-snapshots, --ncs\n",
      "                        Prevents to server from storing config snapshot files.\n",
      "  --plugins-path PLUGINS_PATH, --ppath PLUGINS_PATH\n",
      "                        plugin jars to be included in torchserve class path\n"
     ]
    }
   ],
   "source": [
    "!torchserve --help"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec2552af-5210-4319-98c8-f4b247530090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  7341  100  7341    0     0   108k      0 --:--:-- --:--:-- --:--:--  108k\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://raw.githubusercontent.com/pytorch/serve/master/docs/images/kitten_small.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d12e1315-6044-4aee-921c-56acf8bbf0c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"tabby\": 0.4783327877521515,\n",
      "  \"lynx\": 0.19989627599716187,\n",
      "  \"tiger_cat\": 0.1682717651128769,\n",
      "  \"tiger\": 0.061949197202920914,\n",
      "  \"Egyptian_cat\": 0.05116736516356468\n",
      "}"
     ]
    }
   ],
   "source": [
    "!curl http://127.0.0.1:8080/predictions/densenet161 -T kitten_small.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db6b88b-ea8e-442a-81e0-d3302917e815",
   "metadata": {},
   "source": [
    ":::{.callout-warning}\n",
    "\n",
    "I wouldn't recommend installing torchserve and running it on a VM.  It's probably easier to use Docker.\n",
    "\n",
    "`docker pull pytorch/torchserve`\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acfde23-aacc-4026-972e-d631642ee09a",
   "metadata": {},
   "source": [
    "### Docker\n",
    "\n",
    "See [these docs](https://github.com/pytorch/serve/blob/master/docker/README.md#start-a-container-with-a-torchserve-image).  We have to mount the necessary files and run the same commands.  We also have to expose all the ports, etc.\n",
    "\n",
    ":::{.callout-important}\n",
    "\n",
    "Note that you have to supply the `torchserve` command, which implies you can run other things (but I don't know what those are).\n",
    "\n",
    ":::\n",
    "\n",
    "```bash\n",
    "docker run --rm -it --gpus '\"device=0\"' \\\n",
    "    -p 8080:8080 \\\n",
    "    -p 8081:8081 \\\n",
    "    -p 8082:8082 \\\n",
    "    -p 7070:7070 \\\n",
    "    -p 7071:7071 \\\n",
    "    --mount type=bind,source=/home/hamel/hamel/notes/serving/torchserve/model_store,target=/tmp/models \\\n",
    "    pytorch/torchserve:latest-gpu \\\n",
    "    torchserve \\\n",
    "    --model-store /tmp/models \\\n",
    "    --models densenet161.mar\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6526c9b-fa2f-425a-9906-d0441455c762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"tabby\": 0.4783327877521515,\n",
      "  \"lynx\": 0.19989627599716187,\n",
      "  \"tiger_cat\": 0.1682717651128769,\n",
      "  \"tiger\": 0.061949197202920914,\n",
      "  \"Egyptian_cat\": 0.05116736516356468\n",
      "}"
     ]
    }
   ],
   "source": [
    "!curl http://127.0.0.1:8080/predictions/densenet161 -T kitten_small.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b57e48-144a-4453-819f-28f61b24aea3",
   "metadata": {},
   "source": [
    "## Other Notes\n",
    "\n",
    "I found these articles to be very important:\n",
    "\n",
    "1. Source code for [BaseHandler](https://github.com/pytorch/serve/blob/master/ts/torch_handler/base_handler.py).\n",
    "2. Performance guide: [Concurrency and number of workers](https://pytorch.org/serve/performance_guide.html#concurrency-and-number-of-workers).\n",
    "3. `config.properties` [example 1](https://github.com/pytorch/serve/blob/master/examples/cloud_storage_stream_inference/config.properties) and [example 2](https://github.com/pytorch/serve/blob/master/docker/config.properties) of how you can pass [configuration files](https://pytorch.org/serve/configuration.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4839a73-42d9-4374-a454-75d692508262",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
