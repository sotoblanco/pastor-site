{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d8459cd5-89ad-4290-aa3e-9b1cc1c353b6",
   "metadata": {},
   "source": [
    "---\n",
    "title: FastAPI\n",
    "description: Serving models with FastAPI\n",
    "order: 5\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22b7cc2-7e38-4296-9e2f-5d6117faa5d1",
   "metadata": {},
   "source": [
    "[FastAPI](https://fastapi.tiangolo.com/) is a web framework for Python.  People like to use this framework for serving prototypes of ML models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ca6700-43dd-4b1a-8d3d-10a8d35002e6",
   "metadata": {},
   "source": [
    "## Impressions\n",
    "\n",
    "- Model serving frameworks (TF Serving, TorchServe, etc) are probably the way to go for production / enterprise deployments, especially for larger models.  They offer more features, and latency will be more predictable (even if slower).  I think that for smaller models (< 200MB) FastAPI is fine.\n",
    "- It is super easy to get started with FastAPI. \n",
    "- I was able to confirm [Sayak's Benchmark](https://medium.com/google-developer-experts/load-testing-tensorflow-serving-and-fastapi-on-gke-411bc14d96b2) where FastAPI is faster than TF Serving, but also less consistent overall.  FastAPI is also more likely to fail, although I haven't been able to cause that.  In my experiments **FastAPI was much faster for this small model**, but this could change with larger models. \n",
    "- Memory is consumed linearly as you increase the number of `Uvicorn` workers.  Model serving frameworks like TF-Serving seem to work more efficiently. You should be careful to set the environment variable `TF_FORCE_GPU_ALLOW_GROWTH=true` if you are running inference on GPUs.  I think in many cases you would be doing inference on CPUs, so this might not be relevant most of the time.\n",
    "- FastAPI seems like it could be really nice on smaller models and scoped hardware where there is only one worker per node and you load balance across nodes (because you aren't replicating the model with each worker). \n",
    "- Debugging FastAPI is amazing, as its pure python and you get a nice docs page at `http://<IP>/docs` that lets you test out your endpoints right on the page!  The documentation for FastPI is also amazing.\n",
    "- If you want the request parameters to be sent in the body (as you often do with ML b/c you want to send data to be scored), you have to use Pydantic.  This is very opinionated, but easy enough to use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0895331-19e2-4159-b5b9-9345eee378e5",
   "metadata": {},
   "source": [
    "## Load Model & Make Predictions\n",
    "\n",
    "Going to use the model trained in the [TF Serving tutorial](../tfserving/tf-serving-basics.ipynb).  Furthermore, we are going to load this from the [SavedModel](https://www.tensorflow.org/guide/saved_model) format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1be930-ccf9-4ca9-a932-9a8ae211c421",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "from fastapi import FastAPI, status\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def load_model(model_path='/home/hamel/hamel/notes/serving/tfserving/model/1'):\n",
    "    \"Load the SavedModel Object.\"\n",
    "    sm = tf.saved_model.load(model_path)\n",
    "    return sm.signatures[\"serving_default\"] # this is the default signature when you save a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3899b46e-fefb-41dc-9f8f-92f3a9fc4e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "def pred(model: tf.saved_model, data:np.ndarray, pred_layer_nm='dense_3'):\n",
    "    \"\"\"\n",
    "    Make a prediction from a SavedModel Object.  `pred_layer_nm` is the last layer that emits logits.\n",
    "    \n",
    "    https://www.tensorflow.org/guide/saved_model\n",
    "    \"\"\"\n",
    "    data = tf.convert_to_tensor(data, dtype='int32')\n",
    "    preds = model(data)\n",
    "    return preds[pred_layer_nm].numpy().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56338cc2-2336-4cbd-89f9-5995ff3a4d9c",
   "metadata": {},
   "source": [
    "### Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cc52e2-0308-42e7-9c7b-86d9e581c40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (x_val, _) = tf.keras.datasets.imdb.load_data(num_words=20000)\n",
    "x_val = tf.keras.preprocessing.sequence.pad_sequences(x_val, maxlen=200)[:2, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d836cde-8262-4a86-b1c0-617a725985ed",
   "metadata": {},
   "source": [
    "### Make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628e9a86-875e-42df-8657-6255f229e476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.8761785626411438, 0.12382148206233978],\n",
       " [0.0009457750129513443, 0.9990542531013489]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = load_model()\n",
    "pred(model, x_val[:2, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b882ebb-4957-4560-95e0-714bff7e33b2",
   "metadata": {},
   "source": [
    "## Build The FastApi App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf92c28-6ae4-48ce-8aea-01a240603734",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "app = FastAPI()\n",
    "\n",
    "items = {}\n",
    "\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup_event():\n",
    "    \"Load the model on startup https://fastapi.tiangolo.com/advanced/events/\"\n",
    "    items['model'] = load_model()\n",
    "\n",
    "\n",
    "@app.get(\"/\")\n",
    "def health(status_code=status.HTTP_200_OK):\n",
    "    \"A health-check endpoint\"\n",
    "    return 'Ok'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9959c091-8bf1-42fa-8b29-a7b35af4d0e4",
   "metadata": {},
   "source": [
    "We want to send the data for prediction in the Request Body (not with path parameters).  According [the docs](https://fastapi.tiangolo.com/tutorial/body/#request-body-path-parameters):\n",
    "\n",
    "> FastAPI will recognize that the function parameters that match path parameters should be taken from the path, and that function parameters that are declared to be Pydantic models should be taken from the request body.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324a47eb-aeba-4bb5-bca6-c129fe93dfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|export\n",
    "class Sentence(BaseModel):\n",
    "    tokens: List[List[int]]\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "def predict(data:Sentence, status_code=status.HTTP_200_OK):\n",
    "    preds = pred(items['model'], data.tokens)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd9a9ca-94e4-4f16-9ded-b8d127617dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|echo: false\n",
    "from nbdev.export import nb_export\n",
    "nb_export('index.ipynb', lib_path='.', name='main')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39578e54-3fb7-4d4d-bd8b-0646af6a5f67",
   "metadata": {},
   "source": [
    "# Recap: the FastAPI App\n",
    "\n",
    "Let's look at `main.py` with all the pieces combined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9cbfdd41-0045-41c8-9407-19688ecf08f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```{.python filename=\"main.py\"}\n",
       "# AUTOGENERATED! DO NOT EDIT! File to edit: index.ipynb.\n",
       "\n",
       "# %% auto 0\n",
       "__all__ = ['app', 'items', 'load_model', 'pred', 'startup_event', 'health', 'Sentence', 'predict']\n",
       "\n",
       "# %% index.ipynb 3\n",
       "from fastapi import FastAPI, status\n",
       "from pydantic import BaseModel\n",
       "from typing import List\n",
       "import tensorflow as tf\n",
       "import numpy as np\n",
       "\n",
       "def load_model(model_path='/home/hamel/hamel/notes/serving/tfserving/model/1'):\n",
       "    \"Load the SavedModel Object.\"\n",
       "    sm = tf.saved_model.load(model_path)\n",
       "    return sm.signatures[\"serving_default\"] # this is the default signature when you save a model\n",
       "\n",
       "# %% index.ipynb 4\n",
       "def pred(model: tf.saved_model, data:np.ndarray, pred_layer_nm='dense_3'):\n",
       "    \"\"\"\n",
       "    Make a prediction from a SavedModel Object.  `pred_layer_nm` is the last layer that emits logits.\n",
       "    \n",
       "    https://www.tensorflow.org/guide/saved_model\n",
       "    \"\"\"\n",
       "    data = tf.convert_to_tensor(data, dtype='int32')\n",
       "    preds = model(data)\n",
       "    return preds[pred_layer_nm].numpy().tolist()\n",
       "\n",
       "# %% index.ipynb 10\n",
       "app = FastAPI()\n",
       "\n",
       "items = {}\n",
       "\n",
       "\n",
       "@app.on_event(\"startup\")\n",
       "async def startup_event():\n",
       "    \"Load the model on startup https://fastapi.tiangolo.com/advanced/events/\"\n",
       "    items['model'] = load_model()\n",
       "\n",
       "\n",
       "@app.get(\"/\")\n",
       "def health(status_code=status.HTTP_200_OK):\n",
       "    \"A health-check endpoint\"\n",
       "    return 'Ok'\n",
       "\n",
       "# %% index.ipynb 12\n",
       "class Sentence(BaseModel):\n",
       "    tokens: List[List[int]]\n",
       "\n",
       "@app.post(\"/predict\")\n",
       "def predict(data:Sentence, status_code=status.HTTP_200_OK):\n",
       "    preds = pred(items['model'], data.tokens)\n",
       "    return preds\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|output: asis\n",
    "#|code-fold: true\n",
    "#|code-summary: Code to display source code in Quarto\n",
    "\n",
    "#This is a hack for Quarto for generated scripts\n",
    "from IPython.display import display, Markdown\n",
    "code = !cat main.py\n",
    "\n",
    "display(Markdown('```{.python filename=\"main.py\"}\\n' + '\\n'.join(code) + '\\n```'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a41dfdb-bfad-4e56-a61d-30281ee48637",
   "metadata": {},
   "source": [
    "# Run The App"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302b6926-b4f8-42ea-997e-13125c84a270",
   "metadata": {},
   "source": [
    "We can run the app with the command:\n",
    "\n",
    "`uvicorn main:app --host 0.0.0.0 --port 5701`\n",
    "\n",
    "- `main` corresponds to the file `main.py`\n",
    "- `app` corresponds to the `app` object inside `main.py` - `app = FastAPI()`\n",
    "- `--reload`: makes the server restart if the code changes, for development only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466e2e8a-1c03-414d-bd38-f18d055fd254",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json\n",
    "\n",
    "def predict_rest(json_data, url='http://localhost:5701/predict'):\n",
    "    json_response = requests.post(url, json={'tokens': json_data})\n",
    "    return json.loads(json_response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4288c494-d7c2-4628-bc07-bdbf5a7ddf17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0.8761785626411438, 0.12382148206233978],\n",
       " [0.0009457750129513443, 0.9990542531013489]]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_rest(x_val.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80efe9c1-bbd9-4a53-8ffd-5ebbce251500",
   "metadata": {},
   "source": [
    "# Load Test FastAPI\n",
    "\n",
    "It's really fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5681549-2265-4f11-99e3-aff1c6fa01f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.parallel import parallel\n",
    "from functools import partial\n",
    "parallel_pred = partial(parallel, threadpool=True, n_workers=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bfe9253-c85b-4fa6-b265-e3713aa98558",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = [x_val.tolist()] * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68995306-07b2-41e3-b094-3c6699a00afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.29 s, sys: 252 ms, total: 2.54 s\n",
      "Wall time: 2.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = parallel_pred(predict_rest, sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd479f1-f8e5-4dfc-8394-951129c44b13",
   "metadata": {},
   "source": [
    "# Adding Uvicorn Workers\n",
    "\n",
    "> Uvicorn also has an option to start and run several worker processes. Nevertheless, as of now, Uvicorn's capabilities for handling worker processes are more limited than Gunicorn's. So, if you want to have a process manager at this level (at the Python level), then it might be better to try with Gunicorn as the process manager.\n",
    "\n",
    "\n",
    "You can add Uvicorn workers with the `--workers` flag:\n",
    "\n",
    "```bash\n",
    "uvicorn main:app --host 0.0.0.0 --port 5701 --workers 8\n",
    "```\n",
    "\n",
    "\n",
    "# GPUs \n",
    "\n",
    "When I scaled up to 8 workers on a GPU, I got OOM errors.  In order to avoid this you want to [Limit GPU memory growth](https://www.tensorflow.org/guide/gpu#limiting_gpu_memory_growth) by settting the `TF_FORCE_GPU_ALLOW_GROWTH` to `true`:\n",
    "\n",
    "```bash\n",
    "TF_FORCE_GPU_ALLOW_GROWTH=true uvicorn main:app --host 0.0.0.0 --port 5701 --workers 8\n",
    "```\n",
    "\n",
    "From the docs:\n",
    "\n",
    "> By default, TensorFlow maps nearly all of the GPU memory of all GPUs (subject to CUDA_VISIBLE_DEVICES) visible to the process.  \n",
    "\n",
    "This means if you are running on GPUs and have > 1 worker, you will get OOM workers without setting this env variable!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e629baf3-58cd-44c5-9d19-0de6f74c8b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.26 s, sys: 294 ms, total: 2.55 s\n",
      "Wall time: 2.34 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = parallel_pred(predict_rest, sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a3f906-0fe4-4c1e-a4a6-4a904936cb64",
   "metadata": {},
   "source": [
    "Scaling up workers didn't have any effect in this particular instance.  Could be because the low latency of the model I'm using doesn't challenge the throughput enough."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "42fd40e048e0585f88ec242f050f7ef0895cf845a8dd1159352394e5826cd102"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
