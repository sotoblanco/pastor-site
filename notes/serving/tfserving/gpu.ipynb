{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6aef57f2-0ebc-4667-a847-8f424cb5e6e3",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"GPUs & Batching\"\n",
    "description: Dynamic batching and using a GPU\n",
    "order: 2\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2cf06d11-c74a-4e92-9320-dfc71c7c8f7e",
   "metadata": {},
   "source": [
    "## Batching\n",
    "\n",
    ":::{.callout-warning}\n",
    "\n",
    "I was not able to simulate a situation where dynamic batching is better than not batching.  Apparently it can take time and lots of experiments to get right.  Follow [this guide](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/batching/README.md#batch-scheduling-parameters-and-tuning) for more information.  This is a topic I may revisit in the future.\n",
    "\n",
    ":::\n",
    "\n",
    "According to the docs:\n",
    "\n",
    "> Model Server has the ability to batch requests in a variety of settings in order to realize better throughput. The scheduling for this batching is done globally for all models and model versions on the server to ensure the best possible utilization of the underlying resources no matter how many models or model versions are currently being served by the server.  You can enable this by using the `--enable_batching` flag and control it with the `--batching_parameters_file`.\n",
    "\n",
    "This is an example batching parameters file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "027a6526-a252-43a9-bf56-4875605ab022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting batch-config.cfg\n"
     ]
    }
   ],
   "source": [
    "%%writefile batch-config.cfg\n",
    "max_batch_size { value: 10000 }\n",
    "batch_timeout_micros { value: 1000 }\n",
    "max_enqueued_batches { value: 18 }\n",
    "num_batch_threads { value: 16 }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f81137-b636-42bd-b393-e0e2a503d672",
   "metadata": {},
   "source": [
    "Guidance for these config files is [here](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/batching/README.md) there is no \"right answer\".  For GPUs, the guidance is this:\n",
    "\n",
    "GPU: One Approach\n",
    "\n",
    "If your model uses a GPU device for part or all of your its inference work, consider the following approach:\n",
    "\n",
    "1. Set `num_batch_threads` to the number of CPU cores.\n",
    "\n",
    "2. Temporarily set `batch_timeout_micros` to a really high value while you tune `max_batch_size` to achieve the desired balance between throughput and average latency. Consider values in the hundreds or thousands.\n",
    "\n",
    "3. For online serving, tune `batch_timeout_micros` to rein in tail latency. The idea is that batches normally get filled to max_batch_size, but occasionally when there is a lapse in incoming requests, to avoid introducing a latency spike it makes sense to process whatever's in the queue even if it represents an underfull batch. The best value for `batch_timeout_micros` is typically a few milliseconds, and depends on your context and goals. Zero is a value to consider; it works well for some workloads. (For bulk processing jobs, choose a large value, perhaps a few seconds, to ensure good throughput but not wait too long for the final (and likely underfull) batch.)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dfcd26e6-dd57-40a0-9f25-2d8b9b9266ee",
   "metadata": {},
   "source": [
    "## Test it out\n",
    "\n",
    "The model we are going to serve is generated [in this note](./tf-serving-basics.ipynb)\n",
    "\n",
    "I'm going to start two TF Serving instances, one thats regular CPU and one that does batching on GPU.  I'm running both commands from the `/home/hamel/tf-serving/` directory.\n",
    "\n",
    "\n",
    "### CPU Version\n",
    "\n",
    "```bash\n",
    "docker run \\\n",
    "--mount type=bind,source=/home/hamel/hamel/notes/serving/tfserving/model/,target=/models/model \\\n",
    "--net=host -t tensorflow/serving\n",
    "```\n",
    "\n",
    ":::{.callout-note}\n",
    "\n",
    "`--net=host` binds all ports to the host, which is convenient for testing\n",
    "\n",
    ":::\n",
    "\n",
    "Test the CPU version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4768f89-2d94-4dc2-9a0f-765567c4728e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"model_version_status\": [\n",
      "  {\n",
      "   \"version\": \"1\",\n",
      "   \"state\": \"AVAILABLE\",\n",
      "   \"status\": {\n",
      "    \"error_code\": \"OK\",\n",
      "    \"error_message\": \"\"\n",
      "   }\n",
      "  }\n",
      " ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "! curl http://localhost:8501/v1/models/model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c953c8-f7d1-4ef8-94c5-12ce1f0a9b5f",
   "metadata": {},
   "source": [
    "### GPU Version"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "119f1245-fd0f-4be3-b426-1d6dacfa5140",
   "metadata": {},
   "source": [
    "You can pass additional arguments like `--enable_batching` in [the same way](https://www.tensorflow.org/tfx/serving/docker#passing_additional_arguments):\n",
    "\n",
    "Use the `latest-gpu` tag to enable GPUs as well as the `--port` and `--rest_api_port` so that it doesn't conflict with the other tf serving instance I have running:\n",
    "\n",
    "### Pre-requisites\n",
    "\n",
    "You must [install nvidia-docker](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html) first\n",
    "\n",
    "### Docker Command\n",
    "\n",
    "Note that we need the `--gpus all` flag to enable GPUs with nvidia-Docker:\n",
    "\n",
    "```bash\n",
    "docker run --gpus all \\\n",
    "--mount type=bind,source=/home/hamel/hamel/notes/serving/tfserving,target=/models \\\n",
    "--net=host -t tensorflow/serving:latest-gpu --enable_batching \\\n",
    "--batching_parameters_file=/models/batch-config.cfg --port=8505 --rest_api_port=8506\n",
    "```\n",
    "\n",
    ":::{.callout-note}\n",
    "\n",
    "### Understanding the volume mount\n",
    "\n",
    "On the host, the config file is located at `/home/hamel/hamel/notes/serving/tfserving/batch-config.cfg` and the model is located at `/home/hamel/hamel/notes/serving/tfserving/model/`\n",
    "\n",
    "The [Docker file](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/tools/docker/Dockerfile.gpu) will try to import the model like this:\n",
    "\n",
    "```dockerfile\n",
    "# Set where models should be stored in the container\n",
    "ENV MODEL_BASE_PATH=/models\n",
    "RUN mkdir -p ${MODEL_BASE_PATH}\n",
    "\n",
    "# The only required piece is the model name in order to differentiate endpoints\n",
    "ENV MODEL_NAME=model\n",
    "\n",
    "# Create a script that runs the model server so we can use environment variables\n",
    "# while also passing in arguments from the docker command line\n",
    "RUN echo '#!/bin/bash \\n\\n\\\n",
    "tensorflow_model_server --port=8500 --rest_api_port=8501 \\\n",
    "--model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \\\n",
    "\"$@\"' > /usr/bin/tf_serving_entrypoint.sh \\\n",
    "&& chmod +x /usr/bin/tf_serving_entrypoint.sh\n",
    "```\n",
    "\n",
    "By default it will try to get models from `${MODEL_BASE_PATH}/${MODEL_NAME}` which is `/models/model`. So when we mount `/home/hamel/hamel/notes/serving/tfserving` from the host to `/models` in the container.\n",
    "\n",
    "In the container:\n",
    "\n",
    "- The model files will be available at `models/model` as expected\n",
    "- The config file will be available at `models/batch-config.cfg`\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4310ea94-5af8-4b46-897c-88207fbb8236",
   "metadata": {},
   "source": [
    "Test the TF-Serving GPU api:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ef71c40-0793-4d8f-b98f-f8e48328d925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"model_version_status\": [\n",
      "  {\n",
      "   \"version\": \"1\",\n",
      "   \"state\": \"AVAILABLE\",\n",
      "   \"status\": {\n",
      "    \"error_code\": \"OK\",\n",
      "    \"error_message\": \"\"\n",
      "   }\n",
      "  }\n",
      " ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "! curl http://localhost:8506/v1/models/model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7fb905-8d68-4c37-a54a-12511315bacd",
   "metadata": {},
   "source": [
    "## Benchmark\n",
    "\n",
    "\"All benchmarks are wrong, some are useful\"\n",
    "\n",
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ae546006-469d-420f-9eb4-57ca70974f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 200  # Only consider the first 200 words of each movie review\n",
    "\n",
    "_, (x_val, _) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdc9244-72cf-4127-a1b9-94b99b227157",
   "metadata": {},
   "source": [
    "### Prepare the prediction code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "16bb6ffb-793a-44da-8951-00fed516c66b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, requests\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def predict_rest(data, port):\n",
    "    json_data = json.dumps(\n",
    "    {\"signature_name\": \"serving_default\", \"instances\": data.tolist()}\n",
    "    )\n",
    "    url = f\"http://localhost:{port}/v1/models/model:predict\"\n",
    "\n",
    "    json_response = requests.post(url, data=json_data)\n",
    "    response = json.loads(json_response.text)\n",
    "    rest_outputs = np.array(response[\"predictions\"])\n",
    "    return rest_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a57b53cd-3eec-44d0-82eb-99c2aa97a8ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.89650154, 0.1034985 ],\n",
       "       [0.00330466, 0.9966954 ]])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_data = x_val[:2, :]\n",
    "rest_outputs = predict_rest(sample_data, '8501')\n",
    "rest_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab844eea-8aa3-4e57-ac4d-e353510706e2",
   "metadata": {},
   "source": [
    "### Test the CPU Server\n",
    "\n",
    "The CPU server is running on port `8501`, we are going to send 5 examples to score 10,000 times in parallel with threads and measure the total time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "dba099ca-873f-465e-bed3-13beda48a2c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.parallel import parallel\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "94cd3923-ba0a-4345-8384-dea312a7907c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cpu_pred = partial(predict_rest, port = '8501')\n",
    "parallel_pred = partial(parallel, threadpool=True, n_workers=500)\n",
    "sample_data = x_val[:5, :]\n",
    "data = [sample_data] * 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d1372e0a-10f9-4a6e-a6b9-4fc4e59d9a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 27.6 s, sys: 4.8 s, total: 32.4 s\n",
      "Wall time: 26 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = parallel_pred(cpu_pred, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f06a572-3a9c-40ab-8a6a-1b2a5e59b526",
   "metadata": {},
   "source": [
    "### Test the GPU Server with batching\n",
    "\n",
    "The GPU server is running on port `8506` (we already started it above).  For this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "b1e93266-d86b-4f64-ba8d-4fbe085bd4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_pred = partial(predict_rest, port = '8506')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "27b7f35c-4918-4308-b4d8-b13fdef5dc5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 25.5 s, sys: 3.18 s, total: 28.7 s\n",
      "Wall time: 25.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = parallel_pred(gpu_pred, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a0cb4c-9a0f-4187-ae12-bce50f42319b",
   "metadata": {},
   "source": [
    "### Test the GPU server without batching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68742ea2-ee98-4945-8168-fb693f7441bc",
   "metadata": {},
   "source": [
    "```bash\n",
    "docker run --gpus all --mount type=bind,source=/home/hamel/hamel/notes/serving/tfserving,target=/models --net=host -t tensorflow/serving:latest-gpu --port=8507 --rest_api_port=8508\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "bde84bc1-6be7-4f43-9e6b-33673a629044",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_pred_no_batch = partial(predict_rest, port = '8508')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c5defa54-52e9-4e54-83b9-f42cc65378c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 22.9 s, sys: 3.45 s, total: 26.4 s\n",
      "Wall time: 21.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = parallel_pred(gpu_pred_no_batch, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0992c54-b81d-4226-9fd8-63e515cf9641",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
