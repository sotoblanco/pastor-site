{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b6471437-9d59-4756-911e-c763cff0a285",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Basics\"\n",
    "description: A minimal end-to-end example of TF Serving\n",
    "order: 1\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8c0a10-471b-465d-9a5b-4deac3bf1ba6",
   "metadata": {},
   "source": [
    "These notes use code from [here](https://keras.io/examples/nlp/text_classification_with_transformer/) and this [tutorial on tf serving](https://keras.io/examples/keras_recipes/tf_serving/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2e2f09-bc95-4bf7-93f1-4d87f1dc7dec",
   "metadata": {},
   "source": [
    "## Create The Model\n",
    "\n",
    ":::{.callout-note}\n",
    "\n",
    "I didn't want to use an existing model file from a tfserving tutorial, so I'm creating a new model from scratch.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e766457e-9a5e-42af-b52c-2c6cbe6294a1",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "from train import get_model\n",
    "\n",
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 200  # Only consider the first 200 words of each movie review\n",
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "805fc561-ffbd-4340-b20e-6aecbc613cc6",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 Training sequences\n",
      "25000 Validation sequences\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "print(len(x_train), \"Training sequences\")\n",
    "print(len(x_val), \"Validation sequences\")\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb6f139-1acf-4743-b2d8-7da3a528c7ee",
   "metadata": {},
   "source": [
    ":::{.callout-important}\n",
    "\n",
    "`get_model` is [defined here](https://github.com/hamelsmu/hamel/blob/master/notes/serving/tfserving/train.py)\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d45f0b82-5f3d-4cc2-a553-e5dddafaf985",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "model = get_model(maxlen=maxlen, vocab_size=vocab_size, \n",
    "                  embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca568e7d-b604-4cac-9499-63bc872eb860",
   "metadata": {},
   "source": [
    ":::{.callout-warning}\n",
    "\n",
    "You should be  careful to specify `dtype` properly for the input layer, so that the `tfserving` api validation will work properly. Like this:\n",
    "\n",
    "```python\n",
    "inputs = layers.Input(shape=(maxlen,), dtype='int32')\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "534e90dd-be3a-43c0-8798-8786666afe56",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 200)]             0         \n",
      "                                                                 \n",
      " token_and_position_embeddin  (None, 200, 32)          646400    \n",
      " g (TokenAndPositionEmbeddin                                     \n",
      " g)                                                              \n",
      "                                                                 \n",
      " transformer_block (Transfor  (None, 200, 32)          10656     \n",
      " merBlock)                                                       \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 32)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 20)                660       \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 20)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 42        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 657,758\n",
      "Trainable params: 657,758\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd5f1c4-2f59-42b7-95df-c395ff43cac8",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0f3993e-1ea3-4377-8710-299dc14b8438",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "782/782 [==============================] - 51s 60ms/step - loss: 0.4103 - accuracy: 0.7936 - val_loss: 0.2994 - val_accuracy: 0.8757\n",
      "Epoch 2/2\n",
      "782/782 [==============================] - 21s 26ms/step - loss: 0.2019 - accuracy: 0.9226 - val_loss: 0.3145 - val_accuracy: 0.8745\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(\n",
    "    x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fbeee5-7937-4d00-a031-04e6d695a212",
   "metadata": {},
   "source": [
    "### Save Model\n",
    "\n",
    "You can serialize your tensorflow models to a `SavedModel` format using `tf.saved_model.save(...)`.  This format is [documented here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96428bff-b8ea-4038-bdc4-78b77b6822b2",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "!rm -rf ./model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9b4a7702-96d8-41c9-bd2a-f154557576d2",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, multi_head_attention_layer_call_fn while saving (showing 5 of 26). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SavedModel files: ['fingerprint.pb', 'variables', 'assets', 'saved_model.pb']\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"./model\"\n",
    "model_version = 1\n",
    "model_export_path = f\"{model_dir}/{model_version}\"\n",
    "\n",
    "tf.saved_model.save(\n",
    "    model,\n",
    "    export_dir=model_export_path,\n",
    ")\n",
    "\n",
    "print(f\"SavedModel files: {os.listdir(model_export_path)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7075a1d5-02d0-4c3b-84ed-c846bb180a9e",
   "metadata": {},
   "source": [
    "## Validate the API Schema\n",
    "\n",
    "The output of the below command will show the input schema and shape, as well as the output shape of the API we will create with tfserving.\n",
    "\n",
    "Thie below flags are mostly boilerplate.  I don't know what `signature` really means just yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a91847bc-48b7-4500-a145-c9a11e565fba",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel SignatureDef contains the following input(s):\n",
      "  inputs['input_1'] tensor_info:\n",
      "      dtype: DT_INT32\n",
      "      shape: (-1, 200)\n",
      "      name: serving_default_input_1:0\n",
      "The given SavedModel SignatureDef contains the following output(s):\n",
      "  outputs['dense_3'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 2)\n",
      "      name: StatefulPartitionedCall:0\n",
      "Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir {model_export_path} --tag_set serve --signature_def serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a60d8f-1e56-4773-b048-9bf9056aeef6",
   "metadata": {},
   "source": [
    "## Launch the docker container\n",
    "\n",
    "The [TFServing docs](https://www.tensorflow.org/tfx/serving/setup) really want you to use docker.  But you can use the CLI `tensorflow_model_server` instead, which is what is packaged in the Docker container.  This is what their docs say:\n",
    "\n",
    "> The easiest and most straight-forward way of using TensorFlow Serving is with Docker images. We highly recommend this route unless you have specific needs that are not addressed by running in a container.\n",
    "\n",
    "> TIP: This is also the easiest way to get TensorFlow Serving working with [GPU support](https://www.tensorflow.org/tfx/serving/docker#serving_with_docker_using_your_gpu)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53f88fc-137d-4cc2-af50-4a6485e326ab",
   "metadata": {},
   "source": [
    "It worth looking at [The Dockerfile for TFServing](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/tools/docker/Dockerfile#L54-L58):\n",
    "\n",
    "\n",
    "```dockerfile\n",
    "ENV MODEL_BASE_PATH=/models\n",
    "RUN mkdir -p ${MODEL_BASE_PATH}\n",
    "\n",
    "# The only required piece is the model name in order to differentiate endpoints\n",
    "ENV MODEL_NAME=model\n",
    "\n",
    "\n",
    "RUN echo '#!/bin/bash \\n\\n\\\n",
    "tensorflow_model_server --port=8500 --rest_api_port=8501 \\\n",
    "--model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \\\n",
    "\"$@\"' > /usr/bin/tf_serving_entrypoint.sh \\\n",
    "&& chmod +x /usr/bin/tf_serving_entrypoint.sh\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "this means that it is looking in `/models/model` by default.  We can consider this when mounting the local model directory into the container.\n",
    "\n",
    "Suppose my local model is located at `/home/hamel/tf-serving/model/`.  This is how you would run the Docker container:\n",
    "\n",
    "\n",
    "```bash\n",
    "docker run -p 8500:8500 --mount type=bind,source=/home/hamel/tf-serving/model/,target=/models/model --net=host -t tensorflow/serving\n",
    "```\n",
    "\n",
    "#### TFServing on a GPU\n",
    "\n",
    "See the note on [using GPUs in TF Serving](./gpu.ipynb).\n",
    "\n",
    "**However**, it probably only makes sense to enable the GPU if you are going to [enable batching](https://www.tensorflow.org/tfx/serving/serving_config#batching_configuration), or if a single prediction are GPU intensive (like Stable Diffusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee5e56d-5d97-49c9-a319-ca15a69d56f7",
   "metadata": {},
   "source": [
    "## Testing the API\n",
    "\n",
    "According to [the documentation](https://www.tensorflow.org/tfx/serving/api_rest) we can see the status of our model like this:\n",
    "\n",
    "`GET http://host:port/v1/models/${MODEL_NAME}`, which for us is:\n",
    "    \n",
    "`curl https://localhost:8501/v1/models/model`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d95860f4-2953-4e03-99e4-98fa2f4bdf98",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"model_version_status\": [\n",
      "  {\n",
      "   \"version\": \"1\",\n",
      "   \"state\": \"AVAILABLE\",\n",
      "   \"status\": {\n",
      "    \"error_code\": \"OK\",\n",
      "    \"error_message\": \"\"\n",
      "   }\n",
      "  }\n",
      " ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "! curl http://localhost:8501/v1/models/model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bae6f5f-3770-4bfb-b691-5d6e63b7d8fb",
   "metadata": {},
   "source": [
    "# Make a prediction request\n",
    "\n",
    "## REST\n",
    "\n",
    "Time to make [a prediction request](https://www.tensorflow.org/tfx/serving/api_rest#predict_api).  We will first try the REST API, which says the api endpoint is as follows:  Note that `v1` is just a hardcoded thing that has to do with the version of tfServing, not the version of the model:\n",
    "\n",
    "`POST http://host:port/v1/models/${MODEL_NAME}[/versions/${VERSION}|/labels/${LABEL}]:predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6015e7fc-d4aa-4bbc-9204-57bc1a799d72",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import json, requests\n",
    "import numpy as np\n",
    "\n",
    "sample_data = x_val[:2, :]\n",
    "\n",
    "data = json.dumps(\n",
    "    {\"signature_name\": \"serving_default\", \"instances\": sample_data.tolist()}\n",
    ")\n",
    "url = \"http://localhost:8501/v1/models/model:predict\"\n",
    "\n",
    "\n",
    "def predict_rest(json_data, url):\n",
    "    json_response = requests.post(url, data=json_data)\n",
    "    response = json.loads(json_response.text)\n",
    "    rest_outputs = np.array(response[\"predictions\"])\n",
    "    return rest_outputs\n",
    "\n",
    "rest_outputs = predict_rest(data, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c44cffc-2ce3-4e0f-9220-d4ff8169ebaf",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.97834629, 0.02165373],\n",
       "       [0.00127591, 0.99872404]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aeb5c17d-58c0-46f7-b4c6-76a0886f5b0f",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 168ms/step\n"
     ]
    }
   ],
   "source": [
    "model_outputs = model.predict(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d2ff11-1482-43c0-81f3-e092c90124b0",
   "metadata": {},
   "source": [
    "Let's compare this to our model's output.  It's close enough :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "baebc6b7-580a-4ace-b245-4552e08f757d",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "assert np.allclose(rest_outputs, model_outputs, rtol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6dba99-0de3-428a-be08-0bc7bb1758f8",
   "metadata": {},
   "source": [
    "## gRPC\n",
    "\n",
    "- The payload format for grpc uses Protocol Buffers which are compressed better than JSON, which might make latency lower. This makes a difference for higher payload sizes, like images.  \n",
    "- gRPC has some kind of bi-directional streaming whereas REST is just a response/request model.  I don't know what this means.\n",
    "- gRPC uses a newer HTTP protocol than REST.  I don't know what this means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a34b1f33-bb42-4d83-8292-3b7aa18b80a7",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import grpc\n",
    "\n",
    "# Create a channel that will be connected to the gRPC port of the container\n",
    "channel = grpc.insecure_channel(\"localhost:8500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab4e69d3-d976-49c4-b49b-0d6f06956778",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow_serving.apis import predict_pb2, prediction_service_pb2_grpc\n",
    "stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3da568a5-1e86-4afd-bafa-a3255251ed41",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# Get the serving_input key\n",
    "loaded_model = tf.saved_model.load(model_export_path)\n",
    "input_name = list(\n",
    "    loaded_model.signatures[\"serving_default\"].structured_input_signature[1].keys()\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3a7611d-03c4-4eab-8f24-708df4960bf0",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'input_1'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f72e3432-7026-4770-9686-da4f90c39e4a",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "def predict_grpc(data, input_name, stub):\n",
    "    # Create a gRPC request made for prediction\n",
    "    request = predict_pb2.PredictRequest()\n",
    "\n",
    "    # Set the name of the model, for this use case it is \"model\"\n",
    "    request.model_spec.name = \"model\"\n",
    "\n",
    "    # Set which signature is used to format the gRPC query\n",
    "    # here the default one \"serving_default\"\n",
    "    request.model_spec.signature_name = \"serving_default\"\n",
    "\n",
    "    # Set the input as the data\n",
    "    # tf.make_tensor_proto turns a TensorFlow tensor into a Protobuf tensor\n",
    "    request.inputs[input_name].CopyFrom(tf.make_tensor_proto(data))\n",
    "\n",
    "    # Send the gRPC request to the TF Server\n",
    "    result = stub.Predict(request)\n",
    "    return result\n",
    "\n",
    "sample_data = tf.convert_to_tensor(x_val[:2, :], dtype='int32')\n",
    "\n",
    "grpc_outputs = predict_grpc(sample_data, input_name, stub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d58358-2fc5-4395-88d1-b1435c142b16",
   "metadata": {},
   "source": [
    "### Inspect the gRPC response\n",
    "\n",
    "We can see all the fields that the gRPC response has.  In this situation, the name of the final layer of our model will be the key that containst the predictions, which is `dense_3` in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dc199c21-1daa-4472-9111-15901041bfba",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "outputs {\n",
       "  key: \"dense_3\"\n",
       "  value {\n",
       "    dtype: DT_FLOAT\n",
       "    tensor_shape {\n",
       "      dim {\n",
       "        size: 2\n",
       "      }\n",
       "      dim {\n",
       "        size: 2\n",
       "      }\n",
       "    }\n",
       "    float_val: 0.9783462882041931\n",
       "    float_val: 0.02165372669696808\n",
       "    float_val: 0.0012759148376062512\n",
       "    float_val: 0.9987240433692932\n",
       "  }\n",
       "}\n",
       "model_spec {\n",
       "  name: \"model\"\n",
       "  version {\n",
       "    value: 1\n",
       "  }\n",
       "  signature_name: \"serving_default\"\n",
       "}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grpc_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee56a3e-31bb-4616-bb41-f2d6734271c5",
   "metadata": {},
   "source": [
    "We can also get the name of the last layer of the model like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "113cb92e-792d-404d-89da-49779c03f991",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dense_3': TensorSpec(shape=(None, 2), dtype=tf.float32, name='dense_3')}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.signatures[\"serving_default\"].structured_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b6acaa-b4a4-4983-bfa0-85f472d09007",
   "metadata": {},
   "source": [
    "### Reshaping the Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e306b8d2-1f9d-4c9e-8035-ea4702055e2f",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.97834629, 0.02165373],\n",
       "       [0.00127591, 0.99872404]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = [x.size for x in grpc_outputs.outputs['dense_3'].tensor_shape.dim]\n",
    "\n",
    "grpc_preds = np.reshape(grpc_outputs.outputs['dense_3'].float_val, shape)\n",
    "grpc_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a2aca1-47c2-4bc1-9f3b-c62a60caa4fa",
   "metadata": {},
   "source": [
    "The predictions are close enough.  I am not sure why they wouldn't be exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c7dc73bf-4471-4280-b029-d86d4f7c4723",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "assert np.allclose(model_outputs, grpc_preds,rtol=1e-4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "42fd40e048e0585f88ec242f050f7ef0895cf845a8dd1159352394e5826cd102"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
