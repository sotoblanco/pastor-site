{
 "cells": [
  {
   "cell_type": "raw",
   "id": "b6471437-9d59-4756-911e-c763cff0a285",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Basics\"\n",
    "description: A minimal end-to-end example of TF Serving\n",
    "order: 1\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8c0a10-471b-465d-9a5b-4deac3bf1ba6",
   "metadata": {},
   "source": [
    "These notes use code from [here](https://keras.io/examples/nlp/text_classification_with_transformer/) and this [tutorial on tf serving](https://keras.io/examples/keras_recipes/tf_serving/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2e2f09-bc95-4bf7-93f1-4d87f1dc7dec",
   "metadata": {},
   "source": [
    "## Create The Model\n",
    "\n",
    ":::{.callout-note}\n",
    "\n",
    "I didn't want to use an existing model file from a tfserving tutorial, so I'm creating a new model from scratch.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e766457e-9a5e-42af-b52c-2c6cbe6294a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os\n",
    "from train import get_model\n",
    "\n",
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 200  # Only consider the first 200 words of each movie review\n",
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "805fc561-ffbd-4340-b20e-6aecbc613cc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 Training sequences\n",
      "25000 Validation sequences\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "print(len(x_train), \"Training sequences\")\n",
    "print(len(x_val), \"Validation sequences\")\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb6f139-1acf-4743-b2d8-7da3a528c7ee",
   "metadata": {},
   "source": [
    ":::{.callout-important}\n",
    "\n",
    "`get_model` is [defined here](https://github.com/hamelsmu/hamel/blob/master/notes/serving/tfserving/train.py)\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d45f0b82-5f3d-4cc2-a553-e5dddafaf985",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(maxlen=maxlen, vocab_size=vocab_size, \n",
    "                  embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca568e7d-b604-4cac-9499-63bc872eb860",
   "metadata": {},
   "source": [
    ":::{.callout-warning}\n",
    "\n",
    "You should be  careful to specify `dtype` properly for the input layer, so that the `tfserving` api validation will work properly. Like this:\n",
    "\n",
    "```python\n",
    "inputs = layers.Input(shape=(maxlen,), dtype='int32')\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "534e90dd-be3a-43c0-8798-8786666afe56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 200)]             0         \n",
      "                                                                 \n",
      " token_and_position_embeddin  (None, 200, 32)          646400    \n",
      " g (TokenAndPositionEmbeddin                                     \n",
      " g)                                                              \n",
      "                                                                 \n",
      " transformer_block (Transfor  (None, 200, 32)          10656     \n",
      " merBlock)                                                       \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 32)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 20)                660       \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 20)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 2)                 42        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 657,758\n",
      "Trainable params: 657,758\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd5f1c4-2f59-42b7-95df-c395ff43cac8",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0f3993e-1ea3-4377-8710-299dc14b8438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "782/782 [==============================] - 49s 58ms/step - loss: 0.3977 - accuracy: 0.8056 - val_loss: 0.2856 - val_accuracy: 0.8767\n",
      "Epoch 2/2\n",
      "782/782 [==============================] - 19s 24ms/step - loss: 0.1962 - accuracy: 0.9258 - val_loss: 0.3261 - val_accuracy: 0.8608\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "history = model.fit(\n",
    "    x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fbeee5-7937-4d00-a031-04e6d695a212",
   "metadata": {},
   "source": [
    "### Save Model\n",
    "\n",
    "You can serialize your tensorflow models to a `SavedModel` format using `tf.saved_model.save(...)`.  This format is [documented here](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/saved_model/README.md).  We are saving two versions of the model in order to discuss features of how TF Serving can serve multiple model versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96428bff-b8ea-4038-bdc4-78b77b6822b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf ./model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9b4a7702-96d8-41c9-bd2a-f154557576d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, multi_head_attention_layer_call_fn while saving (showing 5 of 26). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/1/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SavedModel files: ['fingerprint.pb', 'variables', 'assets', 'saved_model.pb']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, embedding_1_layer_call_fn, embedding_1_layer_call_and_return_conditional_losses, multi_head_attention_layer_call_fn while saving (showing 5 of 26). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./model/2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SavedModel files: ['fingerprint.pb', 'variables', 'assets', 'saved_model.pb']\n"
     ]
    }
   ],
   "source": [
    "def save_model(model_version, model_dir=\"./model\"):\n",
    "\n",
    "    model_export_path = f\"{model_dir}/{model_version}\"\n",
    "\n",
    "    tf.saved_model.save(\n",
    "        model,\n",
    "        export_dir=model_export_path,\n",
    "    )\n",
    "\n",
    "    print(f\"SavedModel files: {os.listdir(model_export_path)}\")\n",
    "\n",
    "save_model(model_version=1)\n",
    "save_model(model_version=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4403a18-e5db-4eff-beb2-c220950f4876",
   "metadata": {},
   "source": [
    "Model versioning is done by saving your model into a directory with an integer.  By default, the directory with the highest integer will be served.  You can change this with [config files](#model-versioning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d7c9ddd2-e347-4cc7-af4b-fd05de8d6a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  2\n"
     ]
    }
   ],
   "source": [
    "!ls model/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7075a1d5-02d0-4c3b-84ed-c846bb180a9e",
   "metadata": {},
   "source": [
    "## Validate the API Schema\n",
    "\n",
    "The output of the below command will show the input schema and shape, as well as the output shape of the API we will create with tfserving.\n",
    "\n",
    "Thie below flags are mostly boilerplate.  I don't know what `signature` really means just yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a91847bc-48b7-4500-a145-c9a11e565fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The given SavedModel SignatureDef contains the following input(s):\n",
      "  inputs['input_1'] tensor_info:\n",
      "      dtype: DT_INT32\n",
      "      shape: (-1, 200)\n",
      "      name: serving_default_input_1:0\n",
      "The given SavedModel SignatureDef contains the following output(s):\n",
      "  outputs['dense_3'] tensor_info:\n",
      "      dtype: DT_FLOAT\n",
      "      shape: (-1, 2)\n",
      "      name: StatefulPartitionedCall:0\n",
      "Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "!saved_model_cli show --dir ./model/2 --tag_set serve --signature_def serving_default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a60d8f-1e56-4773-b048-9bf9056aeef6",
   "metadata": {},
   "source": [
    "## Launch the docker container\n",
    "\n",
    "The [TFServing docs](https://www.tensorflow.org/tfx/serving/setup) really want you to use docker.  But you can use the CLI `tensorflow_model_server` instead, which is what is packaged in the Docker container.  This is what their docs say:\n",
    "\n",
    "> The easiest and most straight-forward way of using TensorFlow Serving is with Docker images. We highly recommend this route unless you have specific needs that are not addressed by running in a container.\n",
    "\n",
    "> TIP: This is also the easiest way to get TensorFlow Serving working with [GPU support](https://www.tensorflow.org/tfx/serving/docker#serving_with_docker_using_your_gpu)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53f88fc-137d-4cc2-af50-4a6485e326ab",
   "metadata": {},
   "source": [
    "It worth looking at [The Dockerfile for TFServing](https://github.com/tensorflow/serving/blob/master/tensorflow_serving/tools/docker/Dockerfile#L54-L58):\n",
    "\n",
    "\n",
    "```dockerfile\n",
    "ENV MODEL_BASE_PATH=/models\n",
    "RUN mkdir -p ${MODEL_BASE_PATH}\n",
    "\n",
    "# The only required piece is the model name in order to differentiate endpoints\n",
    "ENV MODEL_NAME=model\n",
    "\n",
    "\n",
    "RUN echo '#!/bin/bash \\n\\n\\\n",
    "tensorflow_model_server --port=8500 --rest_api_port=8501 \\\n",
    "--model_name=${MODEL_NAME} --model_base_path=${MODEL_BASE_PATH}/${MODEL_NAME} \\\n",
    "\"$@\"' > /usr/bin/tf_serving_entrypoint.sh \\\n",
    "&& chmod +x /usr/bin/tf_serving_entrypoint.sh\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "this means that it is looking in `/models/model` by default.  We can consider this when mounting the local model directory into the container.\n",
    "\n",
    "Suppose my local model is located at `/home/hamel/hamel/notes/serving/tfserving/model`.  This is how you would run the Docker container:\n",
    "\n",
    "\n",
    "```bash\n",
    "docker run -p 8500:8500 \\\n",
    "--mount type=bind,source=/home/hamel/hamel/notes/serving/tfserving/model,target=/models/model \\\n",
    "--net=host -t tensorflow/serving\n",
    "```\n",
    "\n",
    "#### TFServing on a GPU\n",
    "\n",
    "See the note on [using GPUs in TF Serving](./gpu.ipynb).\n",
    "\n",
    "**However**, it probably only makes sense to enable the GPU if you are going to [enable batching](https://www.tensorflow.org/tfx/serving/serving_config#batching_configuration), or if a single prediction are GPU intensive (like Stable Diffusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee5e56d-5d97-49c9-a319-ca15a69d56f7",
   "metadata": {},
   "source": [
    "## Testing the API\n",
    "\n",
    "According to [the documentation](https://www.tensorflow.org/tfx/serving/api_rest) we can see the status of our model like this:\n",
    "\n",
    "`GET http://host:port/v1/models/${MODEL_NAME}`, which for us is:\n",
    "    \n",
    "`curl https://localhost:8501/v1/models/model`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d95860f4-2953-4e03-99e4-98fa2f4bdf98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"model_version_status\": [\n",
      "  {\n",
      "   \"version\": \"2\",\n",
      "   \"state\": \"AVAILABLE\",\n",
      "   \"status\": {\n",
      "    \"error_code\": \"OK\",\n",
      "    \"error_message\": \"\"\n",
      "   }\n",
      "  }\n",
      " ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "! curl http://localhost:8501/v1/models/model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032a7f7f-c818-4faa-9edb-1868c6efcff1",
   "metadata": {},
   "source": [
    "Note how this shows the highest version number by default.  You can access different model versions through [different endpoints](#model-versioning) and supplying the right config files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671db461-a848-445c-baa1-05aaa56b5b20",
   "metadata": {},
   "source": [
    "## Model Versioning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579968ed-bcf2-426b-904e-e3683d3e78fa",
   "metadata": {},
   "source": [
    "Models that you save into the directory have a version number, for example our model is saved at `home/hamel/hamel/notes/serving/tfserving/model` with directories with versions `1` and `2`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "be7c7728-6023-4dff-9468-93d98bf2c01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  2\n"
     ]
    }
   ],
   "source": [
    "!ls /home/hamel/hamel/notes/serving/tfserving/model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556ab650-28e2-457d-a908-d145bae23b92",
   "metadata": {},
   "source": [
    " By default, TF Serving will always serve the model with the highest version number.  However, you can change that with [a model server config](https://www.tensorflow.org/tfx/serving/serving_config#serving_a_specific_version_of_a_model).  You can also serve multiple versions of a model, [add labels to models](https://www.tensorflow.org/tfx/serving/serving_config#assigning_string_labels_to_model_versions_to_simplify_canary_and_rollback), etc.  This is probably one of the most useful aspects of TF Serving.  Here are some configs that allow you to serve multiple versions at the same time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "602d55a5-eb4e-4800-b4ec-42e6e46f03ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./model/models.config\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./model/models.config\n",
    "\n",
    "\n",
    "model_config_list {\n",
    " config {\n",
    "    name: 'model'\n",
    "    base_path: '/models/model/'\n",
    "    model_platform: 'tensorflow'\n",
    "    model_version_policy: {all: {}}\n",
    "        }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62f1814-1b99-4c0a-b4b3-c0c17840e897",
   "metadata": {},
   "source": [
    "If you wanted to specify specific models to serve, you could name the versions instead of specifying `all` like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "fbeb557e-75b7-4a37-b9d1-ad7bb8b43342",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./model/models-specific.config\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./model/models-specific.config\n",
    "\n",
    "model_config_list {\n",
    " config {\n",
    "    name: 'model'\n",
    "    base_path: '/models/model/'\n",
    "    model_platform: 'tensorflow'\n",
    "    model_version_policy {\n",
    "      specific {\n",
    "        versions: 1\n",
    "        versions: 2\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc925f08-1007-4a77-9656-8176f194f8a5",
   "metadata": {},
   "source": [
    "To read the config files, we need to pass these additional flags when running the container:\n",
    "\n",
    "```bash\n",
    "docker run \\\n",
    "--mount type=bind,source=/home/hamel/hamel/notes/serving/tfserving/model,target=/models/model \\\n",
    "--net=host \\\n",
    "-t tensorflow/serving \\\n",
    "--model_config_file=/models/model/models-specific.config \\\n",
    "--model_config_file_poll_wait_seconds=60 \n",
    "```\n",
    "\n",
    "The flag `--model_config_file_poll_wait_seconds=60` tells the server to check for a new config file at the path every 60 seconds. This is optional but likely a good idea so you can change your config file without rebooting the server."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92012a8d-54d4-454d-9864-3d82c59d796f",
   "metadata": {},
   "source": [
    "\n",
    " To access a [specific version of the model](https://www.tensorflow.org/tfx/serving/api_rest#url_4), you would make a request to \n",
    "\n",
    "`http://host:port/v1/models/${MODEL_NAME}[/versions/${VERSION}|/labels/${LABEL}]:predict`.  For example, for version 1 the endpoint would be `http://localhost:8501/v1/models/model/versions/1:predict`.\n",
    "\n",
    "\n",
    "If you did not care about the version, and just wanted the highest version we can use the general endpoint without the version which will serve the highest version by default:\n",
    "\n",
    "`http://localhost:8501/v1/models/model:predict`\n",
    "\n",
    "\n",
    "We can test that all of these version is avialable to serve like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "0a1a7876-f815-4ac7-8e68-ce26da5954d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"model_version_status\": [\n",
      "  {\n",
      "   \"version\": \"2\",\n",
      "   \"state\": \"AVAILABLE\",\n",
      "   \"status\": {\n",
      "    \"error_code\": \"OK\",\n",
      "    \"error_message\": \"\"\n",
      "   }\n",
      "  }\n",
      " ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "! curl http://localhost:8501/v1/models/model/versions/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "16f83746-3898-4f3b-be6e-7678c50eef40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"model_version_status\": [\n",
      "  {\n",
      "   \"version\": \"1\",\n",
      "   \"state\": \"AVAILABLE\",\n",
      "   \"status\": {\n",
      "    \"error_code\": \"OK\",\n",
      "    \"error_message\": \"\"\n",
      "   }\n",
      "  }\n",
      " ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "! curl http://localhost:8501/v1/models/model/versions/1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac64aba1-f141-42f7-b432-487e4f273bbf",
   "metadata": {},
   "source": [
    ":::{.callout-warning}\n",
    "\n",
    "TF Serving doesn't make all versions available by default, only the latest one (with the highest number).  You have to supply a config file if you want multiple versions to be made available at once.  You probably should [use labels](https://www.tensorflow.org/tfx/serving/serving_config#assigning_string_labels_to_model_versions_to_simplify_canary_and_rollback) to make URLs consistent in production scenarios.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bae6f5f-3770-4bfb-b691-5d6e63b7d8fb",
   "metadata": {},
   "source": [
    "# Make a prediction request\n",
    "\n",
    "## REST\n",
    "\n",
    "Time to make [a prediction request](https://www.tensorflow.org/tfx/serving/api_rest#predict_api).  We will first try the REST API, which says the api endpoint is as follows:  Note that `v1` is just a hardcoded thing that has to do with the version of tfServing, not the version of the model:\n",
    "\n",
    "`POST http://host:port/v1/models/${MODEL_NAME}[/versions/${VERSION}|/labels/${LABEL}]:predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6015e7fc-d4aa-4bbc-9204-57bc1a799d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, requests\n",
    "import numpy as np\n",
    "\n",
    "sample_data = x_val[:2, :]\n",
    "\n",
    "data = json.dumps(\n",
    "    {\"signature_name\": \"serving_default\", \"instances\": sample_data.tolist()}\n",
    ")\n",
    "url = \"http://localhost:8501/v1/models/model:predict\" # this would be \"http://localhost:8501/v1/models/model/versions/1:predict\" for version 1\n",
    "\n",
    "\n",
    "def predict_rest(json_data, url):\n",
    "    json_response = requests.post(url, data=json_data)\n",
    "    response = json.loads(json_response.text)\n",
    "    rest_outputs = np.array(response[\"predictions\"])\n",
    "    return rest_outputs\n",
    "\n",
    "rest_outputs = predict_rest(data, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8c44cffc-2ce3-4e0f-9220-d4ff8169ebaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.94086391, 0.05913605],\n",
       "       [0.00317052, 0.99682945]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aeb5c17d-58c0-46f7-b4c6-76a0886f5b0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 210ms/step\n"
     ]
    }
   ],
   "source": [
    "model_outputs = model.predict(sample_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d2ff11-1482-43c0-81f3-e092c90124b0",
   "metadata": {},
   "source": [
    "Let's compare this to our model's output.  It's close enough :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "baebc6b7-580a-4ace-b245-4552e08f757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(rest_outputs, model_outputs, rtol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6dba99-0de3-428a-be08-0bc7bb1758f8",
   "metadata": {},
   "source": [
    "## gRPC\n",
    "\n",
    "- The payload format for grpc uses Protocol Buffers which are compressed better than JSON, which might make latency lower. This makes a difference for higher payload sizes, like images.  \n",
    "- gRPC has some kind of bi-directional streaming whereas REST is just a response/request model.  I don't know what this means.\n",
    "- gRPC uses a newer HTTP protocol than REST.  I don't know what this means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a34b1f33-bb42-4d83-8292-3b7aa18b80a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import grpc\n",
    "\n",
    "# Create a channel that will be connected to the gRPC port of the container\n",
    "channel = grpc.insecure_channel(\"localhost:8500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab4e69d3-d976-49c4-b49b-0d6f06956778",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_serving.apis import predict_pb2, prediction_service_pb2_grpc\n",
    "stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3da568a5-1e86-4afd-bafa-a3255251ed41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the serving_input key\n",
    "loaded_model = tf.saved_model.load(model_export_path)\n",
    "input_name = list(\n",
    "    loaded_model.signatures[\"serving_default\"].structured_input_signature[1].keys()\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d3a7611d-03c4-4eab-8f24-708df4960bf0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'input_1'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f72e3432-7026-4770-9686-da4f90c39e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_grpc(data, input_name, stub):\n",
    "    # Create a gRPC request made for prediction\n",
    "    request = predict_pb2.PredictRequest()\n",
    "\n",
    "    # Set the name of the model, for this use case it is \"model\"\n",
    "    request.model_spec.name = \"model\"\n",
    "\n",
    "    # Set which signature is used to format the gRPC query\n",
    "    # here the default one \"serving_default\"\n",
    "    request.model_spec.signature_name = \"serving_default\"\n",
    "\n",
    "    # Set the input as the data\n",
    "    # tf.make_tensor_proto turns a TensorFlow tensor into a Protobuf tensor\n",
    "    request.inputs[input_name].CopyFrom(tf.make_tensor_proto(data))\n",
    "\n",
    "    # Send the gRPC request to the TF Server\n",
    "    result = stub.Predict(request)\n",
    "    return result\n",
    "\n",
    "sample_data = tf.convert_to_tensor(x_val[:2, :], dtype='int32')\n",
    "\n",
    "grpc_outputs = predict_grpc(sample_data, input_name, stub)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d58358-2fc5-4395-88d1-b1435c142b16",
   "metadata": {},
   "source": [
    "### Inspect the gRPC response\n",
    "\n",
    "We can see all the fields that the gRPC response has.  In this situation, the name of the final layer of our model will be the key that containst the predictions, which is `dense_3` in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc199c21-1daa-4472-9111-15901041bfba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "outputs {\n",
       "  key: \"dense_3\"\n",
       "  value {\n",
       "    dtype: DT_FLOAT\n",
       "    tensor_shape {\n",
       "      dim {\n",
       "        size: 2\n",
       "      }\n",
       "      dim {\n",
       "        size: 2\n",
       "      }\n",
       "    }\n",
       "    float_val: 0.9408639073371887\n",
       "    float_val: 0.059136051684617996\n",
       "    float_val: 0.0031705177389085293\n",
       "    float_val: 0.9968294501304626\n",
       "  }\n",
       "}\n",
       "model_spec {\n",
       "  name: \"model\"\n",
       "  version {\n",
       "    value: 2\n",
       "  }\n",
       "  signature_name: \"serving_default\"\n",
       "}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grpc_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee56a3e-31bb-4616-bb41-f2d6734271c5",
   "metadata": {},
   "source": [
    "We can also get the name of the last layer of the model like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "113cb92e-792d-404d-89da-49779c03f991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dense_3': TensorSpec(shape=(None, 2), dtype=tf.float32, name='dense_3')}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.signatures[\"serving_default\"].structured_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b6acaa-b4a4-4983-bfa0-85f472d09007",
   "metadata": {},
   "source": [
    "### Reshaping the Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e306b8d2-1f9d-4c9e-8035-ea4702055e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.94086391, 0.05913605],\n",
       "       [0.00317052, 0.99682945]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape = [x.size for x in grpc_outputs.outputs['dense_3'].tensor_shape.dim]\n",
    "\n",
    "grpc_preds = np.reshape(grpc_outputs.outputs['dense_3'].float_val, shape)\n",
    "grpc_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a2aca1-47c2-4bc1-9f3b-c62a60caa4fa",
   "metadata": {},
   "source": [
    "The predictions are close enough.  I am not sure why they wouldn't be exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c7dc73bf-4471-4280-b029-d86d4f7c4723",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert np.allclose(model_outputs, grpc_preds,rtol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2a76fe-2032-4f47-8d54-83a9b1daf90e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "42fd40e048e0585f88ec242f050f7ef0895cf845a8dd1159352394e5826cd102"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
