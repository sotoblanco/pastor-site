{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ae0d778f-860d-433b-9008-b12137084414",
   "metadata": {},
   "source": [
    "---\n",
    "title: Optimizing LLM latency\n",
    "image: bench_dark.png\n",
    "description: An exploration of inference tools for open source LLMs focused on latency.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2ce13d-189a-4062-a385-6f6e37228cd6",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Below is a summary of my findings:\n",
    "\n",
    "- **[vLLM](https://vllm.readthedocs.io/en/latest/) was the best tool that I tried, with significantly faster latency than everything else.**  The documentation was also great, and it was easy to use.  It only supports specific versions of CUDA, which you can manage using [this approach](../cuda.qmd).  I found that getting things to work to be a bit fiddly, but once I did it was quite fast.\n",
    "- **[Text Generation Inference](https://github.com/huggingface/text-generation-inference) is an ok option (but nowhere near as fast as `vLLM`) if you want to deploy LLMs in a standard way**.  TGI has a bit more features than `vLLM` like telemetry baked in ([via OpenTelemetry](https://opentelemetry.io/docs/concepts/signals/traces/)) and integration with the HF ecosystem like [inference endpoints](https://huggingface.co/inference-endpoints).  Even though its nowhere near as fast as `vLLM`, I expect that these optimization techniques will be integrated into this server over time.  However, one thing to note that as of 7/28/2023, the license for TGI was changed to be more **[restrictive that may interfere with certain commercial uses](https://github.com/huggingface/text-generation-inference/commit/bde25e62b33b05113519e5dbf75abda06a03328e)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b1be69-63d5-46de-ac9a-b673a54df831",
   "metadata": {},
   "source": [
    "### Rough Benchmarks\n",
    "\n",
    "This study focuses on various approaches to optimizing **latency**.  Specifically, I want to know what kind tools are most effective at optimizing latency for open source LLMs. In order to focus on latency, I hold the following variables constant:\n",
    "\n",
    "- batch size of `n = 1` for all prediction requests (holding throughput constant).[^1]  \n",
    "- All experiments were conducted on a `Nvidia A6000` GPU, unless otherwise noted.\n",
    "- The model used is [meta-llama/Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf) on the HuggingFace Hub [^2].\n",
    "\n",
    "[^1]: It is common to explore the inference vs throughput frontier when conducting inference benchmarks.  I did not do this, since I was most interested in latency.  [Here is an example](https://github.com/mosaicml/llm-foundry/tree/main/scripts/inference/benchmarking#different-hw-setups-for-mpt-7b) of how to conduct inference benchmarks that consider both throughput and latency.\n",
    "[^2]: For [Llama v2 models](https://huggingface.co/meta-llama), you must be careful to use the models ending in `-hf` as those are the ones that are compatible with the transformers library.  \n",
    "[^3]: It's not an apples to apples comparison, since the largest OpenAI models are much larger than open source models.  However, I have found that fine-tuning a small model can often be better when you are trying to accomplish a very specific task.\n",
    "\n",
    "In addition to batch size of `n = 1` and using a `A6000` GPU (unless noted otherwise), I also made sure I warmed up the model by sending an initial inference request before measuring latency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a61bcf2-8fb6-4f35-8eb5-68085efb2008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>tok/sec</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>platform</th>\n",
       "      <th>options</th>\n",
       "      <th>gpu</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>HF Hosted Inference Endpoint</th>\n",
       "      <th>-</th>\n",
       "      <th>A10G</th>\n",
       "      <td>30.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">TGI</th>\n",
       "      <th>-</th>\n",
       "      <th>A6000</th>\n",
       "      <td>21.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quantized w/ GPTQ</th>\n",
       "      <th>A6000</th>\n",
       "      <td>23.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quantized w/ bitsandbytes</th>\n",
       "      <th>A6000</th>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">text-generation-webui</th>\n",
       "      <th>exllama</th>\n",
       "      <th>A6000</th>\n",
       "      <td>30.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ggml</th>\n",
       "      <th>A6000</th>\n",
       "      <td>9.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">vllm</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">-</th>\n",
       "      <th>A100 (on Modal Labs)</th>\n",
       "      <td>40.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A6000</th>\n",
       "      <td>46.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                             tok/sec\n",
       "platform                     options                   gpu                          \n",
       "HF Hosted Inference Endpoint -                         A10G                     30.4\n",
       "TGI                          -                         A6000                    21.1\n",
       "                             quantized w/ GPTQ         A6000                    23.6\n",
       "                             quantized w/ bitsandbytes A6000                     1.9\n",
       "text-generation-webui        exllama                   A6000                    30.8\n",
       "                             ggml                      A6000                     9.9\n",
       "vllm                         -                         A100 (on Modal Labs)     40.9\n",
       "                                                       A6000                    46.3"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|echo: false\n",
    "from IPython.display import HTML\n",
    "import pandas as pd\n",
    "df = pd.concat([pd.read_csv('_llama-inference/hf-endpoint/bench-hf-endpoint.csv').assign(platform='HF Hosted Inference Endpoint').assign(options='-').assign(gpu='A10G'),\n",
    "                pd.read_csv('_llama-inference/tgi/bench-default.csv').assign(platform='TGI').assign(options='-').assign(gpu='A6000'), \n",
    "                pd.read_csv('_llama-inference/tgi/bench-quantize-bb.csv').assign(platform='TGI').assign(options='quantized w/ bitsandbytes').assign(gpu='A6000'),\n",
    "                pd.read_csv('_llama-inference/tgi/bench-quantize-gptq.csv').assign(platform='TGI').assign(options='quantized w/ GPTQ').assign(gpu='A6000'),\n",
    "                pd.read_csv('_llama-inference/exllama/bench-exllama.csv').assign(platform='text-generation-webui').assign(options='exllama').assign(gpu='A6000'),\n",
    "                pd.read_csv('_llama-inference/exllama/bench-ggmlcpp.csv').assign(platform='text-generation-webui').assign(options='ggml').assign(gpu='A6000'),\n",
    "                pd.read_csv('_llama-inference/vllm/bench-vllm.csv').assign(platform='vllm').assign(options='-').assign(gpu='A6000'),\n",
    "                pd.read_csv('_llama-inference/vllm/modal-examples/bench-vllm.csv').assign(platform='vllm').assign(options='-').assign(gpu='A100 (on Modal Labs)')]\n",
    "              )\n",
    "\n",
    "df['tok/sec'] = df['tok_count'] / df['time']\n",
    "pd.set_option('display.precision', 1)\n",
    "df.groupby(['platform', 'options', 'gpu']).mean('time')[['tok/sec']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ec64c5-084b-454f-ad16-934d19159381",
   "metadata": {},
   "source": [
    "In some cases I did not use an `A6000` b/c the platform didn't have that particular GPU available.  You can ignore these rows if you like, but I still think it is valuable information.  I had access to a A6000, so I just used what I had.\n",
    "\n",
    "Furthermore, the goal was not to be super precise on these benchmarks but rather to get a general sense of how things work and how they might compare to each other out of the box."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1077594-ace2-4d70-b4a8-e5bea6e30a1e",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "One capability you need to be successful with open source LLMs is the ability to serve models efficiently.  There are two categories of tools for model inference:\n",
    "\n",
    "- **Inference servers:** these help with providing a web server that can provide a REST/grpc or other interface to interact with your model as a service.  These inference servers usually have parameters to help you make [trade-offs between throughput and latency](https://www.simonwenkel.com/notes/ai/practical/latency-vs-throughput-in-machine-learning-pipelines.html). Additionally, some inference servers come with additional features like telemetry, model versioning and more. You can learn more about this topic the [serving section](../serving/index.qmd) of these notes. For LLMs, popular inference servers are the [Text Generation Inference (TGI)](https://github.com/huggingface/text-generation-inference) and [vLLM](https://github.com/vllm-project/vllm).\n",
    "\n",
    "- **Model Optimization**: These modify your model to make them faster for inference.  Examples include [quantization](https://huggingface.co/docs/optimum/concept_guides/quantization),  [Paged Attention](https://vllm.ai/), [Exllama](https://github.com/turboderp/exllama) and more.\n",
    "\n",
    "It is common to use both **Inference servers** and **Model Optimization** techniques in conjunction.  Some inference servers like [TGI](https://github.com/huggingface/text-generation-inference)and [vLLM](https://vllm.readthedocs.io/en/latest/) even help you apply optimization techniques.[^4]\n",
    "\n",
    "[^4]: [The Modular Inference Engine](https://www.modular.com/engine) is another example of an inference server that also applies optimization techniques.  At the time of this writing, this is proprietary technology, but its worth keeping an eye on this in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc13744-23da-4eef-ab09-098a55b596c9",
   "metadata": {},
   "source": [
    "# Notes On Tools\n",
    "\n",
    "An important goal I of this study beyond benchmarking was to try different platforms and take note my impressions and how to use them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37ef450-a795-4b34-8b41-1bdb9cfd7005",
   "metadata": {},
   "source": [
    "## Text Generation Inference (TGI)\n",
    "\n",
    ":::{.callout-warning}\n",
    "### License Restrictions\n",
    "\n",
    "The license for TGI was [recently changed](https://github.com/huggingface/text-generation-inference/commit/bde25e62b33b05113519e5dbf75abda06a03328e) away from Apache 2.0 to be more restrictive.  Be careful when using TGI in commercial applications.\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    "[Text generation inference](https://github.com/huggingface/text-generation-inference) which is often referred to as “TGI” was easy to use without any optimization.  You can run it like this:\n",
    "\n",
    "```{.bash filename=“start_server.sh”}\n",
    "#!/bin/bash\n",
    "\n",
    "if [ -z \"$HUGGING_FACE_HUB_TOKEN\" ]\n",
    "then\n",
    "  echo \"HUGGING_FACE_HUB_TOKEN is not set. Please set it before running this script.\"\n",
    "  exit 1\n",
    "fi\n",
    "\n",
    "model=\"TheBloke/Llama-2-7B-GPTQ\"\n",
    "volume=$PWD/data\n",
    "\n",
    "docker run --gpus all \\\n",
    " -e HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN \\\n",
    " -e GPTQ_BITS=4 -e GPTQ_GROUPSIZE=128 \\\n",
    " --shm-size 5g -p 8081:80 \\\n",
    " -v $volume:/data ghcr.io/huggingface/text-generation-inference \\\n",
    " --max-best-of 1 \"$@\"\n",
    "```\n",
    "\n",
    "We can then run the server with this command:\n",
    "\n",
    "```bash\n",
    "bash start_server.sh --model-id \"meta-llama/Llama-2-7b-hf\"\n",
    "```\n",
    "\n",
    ":::{{.callout-note}}\n",
    "\n",
    "#### Help\n",
    "\n",
    "You can see all the options for the TGI container with the help flag like so:\n",
    "\n",
    "```bash\n",
    "docker run ghcr.io/huggingface/text-generation-inference --help | less\n",
    "```\n",
    ":::\n",
    "\n",
    "### Quantization\n",
    "\n",
    "Quantization was very difficult to get working.  There is a `—quantize` flag with accepts `bitsandbytes` and `gptq`.  The `bitsandbytes` approach makes inference __much__ slower, which [others have reported](https://github.com/huggingface/text-generation-inference/issues/309#issuecomment-1542124381).  \n",
    "\n",
    "To make `gptq` work for llama v2 models requires a bunch of work, you have to [install the text-generation-server](https://github.com/huggingface/text-generation-inference/tree/main/server) which can take a while and is very brittle to get right.  I had to  step through the [Makefile](https://github.com/huggingface/text-generation-inference/blob/main/server/Makefile) carefully.  After that you have to download the weights with:\n",
    "\n",
    "```bash\n",
    "text-generation-server download-weights meta-llama/Llama-2-7b-hf\n",
    "```\n",
    "\n",
    "You can run the following command to perform the quantization (the last argument is the destination directory where the weights are stored).\n",
    "\n",
    "```bash\n",
    "text-generation-server quantize \"meta-llama/Llama-2-7b-hf\" data/quantized/\n",
    "```\n",
    "\n",
    "**However, this step is not needed for the most popular models, as someone will likely already have quantized and uploaded them to the Hub.**\n",
    "\n",
    "#### Pre-Quantized Models\n",
    "\n",
    "Alternatively, you can use a pre-quantized model that has been uploaded to the Hub.  [TheBloke/Llama-2-7B-GPTQ](https://huggingface.co/TheBloke/Llama-2-7B-GPTQ) is a good example of one.  To get this to work, you have to be careful to set the `GPTQ_BITS` and `GPTQ_GROUPSIZE` environment variables to match the config.  For example [This config](https://huggingface.co/TheBloke/Llama-2-7B-GPTQ/blob/main/quantize_config.json#L2-L3) necessitates setting `GPTQ_BITS=4` and `GPTQ_GROUPSIZE=128` These are already set in `start_server.sh` shown above.  [This PR](https://github.com/huggingface/text-generation-inference/pull/671) will eventually fix that.\n",
    "\n",
    "To use the [TheBloke/Llama-2-7B-GPTQ](https://huggingface.co/TheBloke/Llama-2-7B-GPTQ) with TGI, I can use the same bash script with the following arguments:\n",
    "\n",
    "```bash\n",
    "bash start_server.sh --model-id TheBloke/Llama-2-7B-GPTQ --quantize gptq\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad54e87-a359-417a-903c-c8b75b8275fb",
   "metadata": {},
   "source": [
    "## Text Generation WebUI\n",
    "\n",
    "[Aman](https://twitter.com/tmm1/status/1683255057201135616?s=20) let me know about [text-generation-web-ui](https://github.com/oobabooga/text-generation-webui), and also [these instructions](https://github.com/paul-gauthier/aider/issues/110#issuecomment-1644318545) for quickly experimenting with [ExLlama](https://github.com/turboderp/exllama) and [ggml](https://github.com/ggerganov/ggml).\n",
    "\n",
    "From the root of the [text-generation-web-ui](https://github.com/oobabooga/text-generation-webui) repo, you can run the following commands to start an inference server optimized with `ExLlama` or `ggml`, respectively:\n",
    "\n",
    "```bash\n",
    "python3 download-model.py TheBloke/Llama-2-7B-GPTQ\n",
    "python3 server.py --listen --extensions openai --loader exllama_hf --model TheBloke_Llama-2-7B-GPTQ\n",
    "```\n",
    "\n",
    "\n",
    "```bash\n",
    "python3 download-model.py TheBloke/Llama-2-7B-GGML\n",
    "python3 server.py --listen --extensions openai --loader llamacpp --model TheBloke_Llama-2-7B-GGML\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db383b1-310d-48d2-9848-b8f935a59252",
   "metadata": {},
   "source": [
    "After the server was started, I used [this code](https://github.com/hamelsmu/llama-inference/blob/master/exllama/bench.py) to conduct the benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f51263-41d5-4251-8466-1b8cd29b6532",
   "metadata": {},
   "source": [
    "Overall, I didn't like this particular piece of software much.  It's bit bloated because its trying to do too many things at once (An inference server, Web UIs, and other optimizations).  That being said, the documentation is good and it is easy to use.  \n",
    "\n",
    "I don't think there is any particular reason to use this unless you want an end-to-end solution that also comes with a web user-interface (which many people want!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aea8684-64e5-454f-8567-6a4c02ef7e09",
   "metadata": {},
   "source": [
    "## vLLM\n",
    "\n",
    "[vLLM](https://github.com/vllm-project/vllm.git) only works with CUDA 11.8, which I configured using [this approach](https://hamel.dev/notes/cuda.html).  After configuring CUDA and installing the right version of PyTorch, you need to install the bleeding edge from git:\n",
    "\n",
    "```bash\n",
    "pip install -U git+https://github.com/vllm-project/vllm.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8575703-cc73-40d0-a3a7-88d22389fffa",
   "metadata": {},
   "source": [
    "A good recipe to use for vLLM can be find on [these Modal docs](https://modal.com/docs/guide/ex/vllm_inference).  Surprisingly, I had much lower latency when running on a local `A6000` vs. a hosted `V100` on Modal Labs.  It's possible that I did something wrong here.  Either way, **`vLLM` offered the lowest latency compared to everything else by a significant margin.**  If I really wanted to optimize for latency today, I would reach for `vLLM`.\n",
    "\n",
    "`vLLM` [offers a server](https://vllm.readthedocs.io/en/latest/serving/distributed_serving.html), but I benchmarked the model locally using their tools instead.  The code for the benchmarking can be [found here](https://github.com/hamelsmu/llama-inference/blob/master/vllm/bench.py):\n",
    "\n",
    "```python\n",
    "from tqdm import tqdm\n",
    "from vllm import SamplingParams, LLM\n",
    "\n",
    "#from https://modal.com/docs/guide/ex/vllm_inference\n",
    "\n",
    "questions = [\n",
    "    # Coding questions\n",
    "    \"Implement a Python function to compute the Fibonacci numbers.\",\n",
    "    \"Write a Rust function that performs binary exponentiation.\",\n",
    "    \"What are the differences between Javascript and Python?\",\n",
    "    # Literature\n",
    "    \"Write a story in the style of James Joyce about a trip to the Australian outback in 2083, to see robots in the beautiful desert.\",\n",
    "    \"Who does Harry turn into a balloon?\",\n",
    "    \"Write a tale about a time-traveling historian who's determined to witness the most significant events in human history.\",\n",
    "    # Math\n",
    "    \"What is the product of 9 and 8?\",\n",
    "    \"If a train travels 120 kilometers in 2 hours, what is its average speed?\",\n",
    "    \"Think through this step by step. If the sequence a_n is defined by a_1 = 3, a_2 = 5, and a_n = a_(n-1) + a_(n-2) for n > 2, find a_6.\",\n",
    "]\n",
    "\n",
    "MODEL_DIR = \"/home/ubuntu/hamel-drive/vllm-models\"\n",
    "\n",
    "def download_model_to_folder():\n",
    "    from huggingface_hub import snapshot_download\n",
    "    import os\n",
    "\n",
    "    snapshot_download(\n",
    "        \"meta-llama/Llama-2-7b-hf\",\n",
    "        local_dir=MODEL_DIR,\n",
    "        token=os.environ[\"HUGGING_FACE_HUB_TOKEN\"],\n",
    "    )\n",
    "    return LLM(MODEL_DIR)\n",
    "\n",
    "\n",
    "def generate(question, llm, note=None):\n",
    "    response = {'question': question, 'note': note}\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=1.0,\n",
    "        top_p=1,\n",
    "        max_tokens=200,\n",
    "    )\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    result = llm.generate(question, sampling_params)\n",
    "    request_time = time.perf_counter() - start\n",
    "\n",
    "    for output in result:\n",
    "        response['tok_count'] = len(output.outputs[0].token_ids)\n",
    "        response['time'] = request_time\n",
    "        response['answer'] = output.outputs[0].text\n",
    "    \n",
    "    return response\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    llm = download_model_to_folder()\n",
    "    counter = 1\n",
    "    responses = []\n",
    "\n",
    "    for q in tqdm(questions):\n",
    "        response = generate(question=q, llm=llm, note='vLLM')\n",
    "        if counter >= 2:\n",
    "            responses.append(response)\n",
    "        counter += 1\n",
    "    \n",
    "    df = pd.DataFrame(responses)\n",
    "    df.to_csv('bench-vllm.csv', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be58f43-5720-46cc-8434-bf59e994946c",
   "metadata": {},
   "source": [
    "## HuggingFace Inference Endpoint\n",
    "\n",
    "I deployed an [inference endpoint](https://ui.endpoints.huggingface.co/) on HuggingFace for [meta-llama/Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf), on a `Nvidia A10G` GPU. I didn't try to turn on any optimizations like quantization and wanted to see what the default performance would be like.\n",
    "\n",
    "The documentation for these interfaces can be found [here](https://huggingface.github.io/text-generation-inference/#/).  There is also [a python client](https://huggingface.co/docs/huggingface_hub/package_reference/inference_client#huggingface_hub.InferenceClient.text_generation).\n",
    "\n",
    "Their documentation says they are using TGI under the hood.  However, my latency was significantly faster on their hosted inference platform than using TGI locally.  This could be due to the fact that I used a `A10G` with them but only a `A6000` locally.  It's worth looking into why this discrepancy exists further.\n",
    "\n",
    "The code for this benchmark can be found [here](https://github.com/hamelsmu/llama-inference/blob/master/hf-endpoint/bench.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4b9820-7d72-4581-9d10-686b0aeb61d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
