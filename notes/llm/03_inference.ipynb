{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ae0d778f-860d-433b-9008-b12137084414",
   "metadata": {},
   "source": [
    "---\n",
    "title: Optimizing LLM latency\n",
    "image: bench_dark5.png\n",
    "date: last-modified\n",
    "description: An exploration of inference tools for open source LLMs focused on latency.\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2ce13d-189a-4062-a385-6f6e37228cd6",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Below is a summary of my findings:\n",
    "\n",
    "- üèÅ **[exllama via text-generation-webui](https://github.com/oobabooga/text-generation-webui) is the fastest**, but I don't really like the software because installation is brittle and its trying to do too many things at once (front-end, back end etc).  You can also try [exllama](https://github.com/turboderp/exllama) alone without the webserver (but I didn't have time to try this yet).  _This wasn't the fastest up until recently when I updated the newest version and re-ran the benchmarks._\n",
    "- ‚ù§Ô∏è **[CTranslate2](https://github.com/OpenNMT/CTranslate2) is my favorite tool, which is among the fastest but is also the easiest to use**.  The documentation is the best out of all of the solutions I tried.  Furthermore, I think that the ergonomics are excellent for the models that they support.  Unlike vLLM, CTranslate doesn't seem to support distributed inference just yet.\n",
    "- üõ†Ô∏è **[vLLM](https://vllm.readthedocs.io/en/latest/) is really fast, but CTranslate can be much faster.**  On other hand, **vLLM supports distributed inference**, which is something you will need for larger models. **vLLM might be the sweet spot for serving very large models.**\n",
    "- üòê **[Text Generation Inference](https://github.com/huggingface/text-generation-inference) is an ok option (but nowhere near as fast as `vLLM`) if you want to deploy HuggingFace LLMs in a standard way**.  TGI has some nice features like telemetry baked in ([via OpenTelemetry](https://opentelemetry.io/docs/concepts/signals/traces/)) and integration with the HF ecosystem like [inference endpoints](https://huggingface.co/inference-endpoints). One thing to note that as of 7/28/2023, the license for TGI was changed to be more **[restrictive that may interfere with certain commercial uses](https://github.com/huggingface/text-generation-inference/commit/bde25e62b33b05113519e5dbf75abda06a03328e)**. I am personally not a fan of the license. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b1be69-63d5-46de-ac9a-b673a54df831",
   "metadata": {},
   "source": [
    "### Rough Benchmarks\n",
    "\n",
    "This study focuses on various approaches to optimizing **latency**.  Specifically, I want to know which tools are the most effective at optimizing latency for open source LLMs. In order to focus on latency, I hold the following variables constant:\n",
    "\n",
    "- batch size of `n = 1` for all prediction requests (holding throughput constant).[^1]  \n",
    "- All experiments were conducted on a `Nvidia A6000` GPU, unless otherwise noted.\n",
    "- Max output tokens were always set to `200`.\n",
    "- All numbers are calculated as an average over a fixed set of 9 prompts.\n",
    "- The model used is [meta-llama/Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf) on the HuggingFace Hub [^2].\n",
    "\n",
    "[^1]: It is common to explore the inference vs throughput frontier when conducting inference benchmarks.  I did not do this, since I was most interested in latency.  [Here is an example](https://github.com/mosaicml/llm-foundry/tree/main/scripts/inference/benchmarking#different-hw-setups-for-mpt-7b) of how to conduct inference benchmarks that consider both throughput and latency.\n",
    "[^2]: For [Llama v2 models](https://huggingface.co/meta-llama), you must be careful to use the models ending in `-hf` as those are the ones that are compatible with the transformers library.  \n",
    "[^3]: It's not an apples to apples comparison, since the largest OpenAI models are much larger than open source models.  However, I have found that fine-tuning a small model can often be better when you are trying to accomplish a very specific task.\n",
    "\n",
    "In addition to batch size of `n = 1` and using a `A6000` GPU (unless noted otherwise), I also made sure I warmed up the model by sending an initial inference request before measuring latency."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b1effb-65cc-46e9-ac9a-209f74c50cb3",
   "metadata": {},
   "source": [
    "<center>Llama-v2-7b benchmark: <i>batch size = 1, max output tokens = 200</i></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a61bcf2-8fb6-4f35-8eb5-68085efb2008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>avg tok/sec</th>\n",
       "      <th>avg time (seconds)</th>\n",
       "      <th>avg output token count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>platform</th>\n",
       "      <th>options</th>\n",
       "      <th>gpu</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">CTranslate2</th>\n",
       "      <th>float16 quantization</th>\n",
       "      <th>A6000</th>\n",
       "      <td>44.8</td>\n",
       "      <td>4.5</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>int8 quantization</th>\n",
       "      <th>A6000</th>\n",
       "      <td>62.6</td>\n",
       "      <td>3.2</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HF Hosted Inference Endpoint</th>\n",
       "      <th>-</th>\n",
       "      <th>A10G</th>\n",
       "      <td>30.4</td>\n",
       "      <td>6.6</td>\n",
       "      <td>202.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">HuggingFace Transformers (no server)</th>\n",
       "      <th>-</th>\n",
       "      <th>A6000</th>\n",
       "      <td>24.6</td>\n",
       "      <td>7.5</td>\n",
       "      <td>181.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nf4 4bit quantization bitsandbytes</th>\n",
       "      <th>A6000</th>\n",
       "      <td>24.3</td>\n",
       "      <td>7.6</td>\n",
       "      <td>181.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">TGI</th>\n",
       "      <th>-</th>\n",
       "      <th>A6000</th>\n",
       "      <td>21.1</td>\n",
       "      <td>9.5</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quantized w/ GPTQ</th>\n",
       "      <th>A6000</th>\n",
       "      <td>23.6</td>\n",
       "      <td>8.8</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>quantized w/ bitsandbytes</th>\n",
       "      <th>A6000</th>\n",
       "      <td>1.9</td>\n",
       "      <td>103.0</td>\n",
       "      <td>200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text-generation-webui</th>\n",
       "      <th>exllama</th>\n",
       "      <th>A6000</th>\n",
       "      <td>77.0</td>\n",
       "      <td>1.7</td>\n",
       "      <td>134.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">vllm</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">-</th>\n",
       "      <th>A100 (on Modal Labs)</th>\n",
       "      <td>41.5</td>\n",
       "      <td>3.4</td>\n",
       "      <td>143.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>A6000</th>\n",
       "      <td>46.4</td>\n",
       "      <td>3.8</td>\n",
       "      <td>178.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              avg tok/sec  \\\n",
       "platform                             options                            gpu                                 \n",
       "CTranslate2                          float16 quantization               A6000                        44.8   \n",
       "                                     int8 quantization                  A6000                        62.6   \n",
       "HF Hosted Inference Endpoint         -                                  A10G                         30.4   \n",
       "HuggingFace Transformers (no server) -                                  A6000                        24.6   \n",
       "                                     nf4 4bit quantization bitsandbytes A6000                        24.3   \n",
       "TGI                                  -                                  A6000                        21.1   \n",
       "                                     quantized w/ GPTQ                  A6000                        23.6   \n",
       "                                     quantized w/ bitsandbytes          A6000                         1.9   \n",
       "text-generation-webui                exllama                            A6000                        77.0   \n",
       "vllm                                 -                                  A100 (on Modal Labs)         41.5   \n",
       "                                                                        A6000                        46.4   \n",
       "\n",
       "                                                                                              avg time (seconds)  \\\n",
       "platform                             options                            gpu                                        \n",
       "CTranslate2                          float16 quantization               A6000                                4.5   \n",
       "                                     int8 quantization                  A6000                                3.2   \n",
       "HF Hosted Inference Endpoint         -                                  A10G                                 6.6   \n",
       "HuggingFace Transformers (no server) -                                  A6000                                7.5   \n",
       "                                     nf4 4bit quantization bitsandbytes A6000                                7.6   \n",
       "TGI                                  -                                  A6000                                9.5   \n",
       "                                     quantized w/ GPTQ                  A6000                                8.8   \n",
       "                                     quantized w/ bitsandbytes          A6000                              103.0   \n",
       "text-generation-webui                exllama                            A6000                                1.7   \n",
       "vllm                                 -                                  A100 (on Modal Labs)                 3.4   \n",
       "                                                                        A6000                                3.8   \n",
       "\n",
       "                                                                                              avg output token count  \n",
       "platform                             options                            gpu                                           \n",
       "CTranslate2                          float16 quantization               A6000                                  200.0  \n",
       "                                     int8 quantization                  A6000                                  200.0  \n",
       "HF Hosted Inference Endpoint         -                                  A10G                                   202.0  \n",
       "HuggingFace Transformers (no server) -                                  A6000                                  181.4  \n",
       "                                     nf4 4bit quantization bitsandbytes A6000                                  181.4  \n",
       "TGI                                  -                                  A6000                                  200.0  \n",
       "                                     quantized w/ GPTQ                  A6000                                  200.0  \n",
       "                                     quantized w/ bitsandbytes          A6000                                  200.0  \n",
       "text-generation-webui                exllama                            A6000                                  134.0  \n",
       "vllm                                 -                                  A100 (on Modal Labs)                   143.1  \n",
       "                                                                        A6000                                  178.0  "
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#|echo: false\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.precision\", 1)\n",
    "df = pd.concat([pd.read_csv('_llama-inference/hf-endpoint/bench-hf-endpoint.csv').assign(platform='HF Hosted Inference Endpoint').assign(options='-').assign(gpu='A10G'),\n",
    "                pd.read_csv('_llama-inference/hf/bench-hf.csv').assign(platform='HuggingFace Transformers (no server)').assign(options='-').assign(gpu='A6000'),\n",
    "                pd.read_csv('_llama-inference/hf/bench-hf-bb.csv').assign(platform='HuggingFace Transformers (no server)').assign(options='nf4 4bit quantization bitsandbytes').assign(gpu='A6000'),\n",
    "                pd.read_csv('_llama-inference/tgi/bench-default.csv').assign(platform='TGI').assign(options='-').assign(gpu='A6000'), \n",
    "                pd.read_csv('_llama-inference/tgi/bench-quantize-bb.csv').assign(platform='TGI').assign(options='quantized w/ bitsandbytes').assign(gpu='A6000'),\n",
    "                pd.read_csv('_llama-inference/tgi/bench-quantize-gptq.csv').assign(platform='TGI').assign(options='quantized w/ GPTQ').assign(gpu='A6000'),\n",
    "                pd.read_csv('_llama-inference/exllama/bench-exllama.csv').assign(platform='text-generation-webui').assign(options='exllama').assign(gpu='A6000'),\n",
    "                pd.read_csv('_llama-inference/ctranslate/bench-ctranslate.csv').assign(platform='CTranslate2').assign(options='float16 quantization').assign(gpu='A6000'),\n",
    "                pd.read_csv('_llama-inference/ctranslate/bench-ctranslate-int8.csv').assign(platform='CTranslate2').assign(options='int8 quantization').assign(gpu='A6000'),\n",
    "                pd.read_csv('_llama-inference/vllm/bench-vllm.csv').assign(platform='vllm').assign(options='-').assign(gpu='A6000'),\n",
    "                pd.read_csv('_llama-inference/vllm/modal-examples/bench-vllm.csv').assign(platform='vllm').assign(options='-').assign(gpu='A100 (on Modal Labs)')]\n",
    "              )\n",
    "\n",
    "df['tok/sec'] = df['tok_count'] / df['time']\n",
    "\n",
    "(df.groupby(['platform', 'options', 'gpu']).mean('time')[['tok/sec', 'time', 'tok_count']]\n",
    " .rename(columns={'tok/sec': 'avg tok/sec', 'time': 'avg time (seconds)', 'tok_count': 'avg output token count'})\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ec64c5-084b-454f-ad16-934d19159381",
   "metadata": {},
   "source": [
    "In some cases I did not use an `A6000` b/c the platform didn't have that particular GPU available.  You can ignore these rows if you like, but I still think it is valuable information.  I had access to a A6000, so I just used what I had.\n",
    "\n",
    "I noticed that the output of the LLM was quite different (less tokens) when using [vLLM](https://github.com/vllm-project/vllm.git).  I am not sure if I did something wrong here, or it changes the behavior of the LLM.\n",
    "\n",
    "Furthermore, the goal was not to be super precise on these benchmarks but rather to get a general sense of how things work and how they might compare to each other out of the box. Some of the tools above are inference servers which perform logging, tracing etc. in addition to optimizing models which effect latency.  The idea is to see where there are significant differences between tools.  I discussed this more [here](#comparison-without-tgi-server)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1077594-ace2-4d70-b4a8-e5bea6e30a1e",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "One capability you need to be successful with open source LLMs is the ability to serve models efficiently.  There are two categories of tools for model inference:\n",
    "\n",
    "- **Inference servers:** these help with providing a web server that can provide a REST/grpc or other interface to interact with your model as a service.  These inference servers usually have parameters to help you make [trade-offs between throughput and latency](https://www.simonwenkel.com/notes/ai/practical/latency-vs-throughput-in-machine-learning-pipelines.html). Additionally, some inference servers come with additional features like telemetry, model versioning and more. You can learn more about this topic the [serving section](../serving/index.qmd) of these notes. For LLMs, popular inference servers are the [Text Generation Inference (TGI)](https://github.com/huggingface/text-generation-inference) and [vLLM](https://github.com/vllm-project/vllm).\n",
    "\n",
    "- **Model Optimization**: These modify your model to make them faster for inference.  Examples include [quantization](https://huggingface.co/docs/optimum/concept_guides/quantization),  [Paged Attention](https://vllm.ai/), [Exllama](https://github.com/turboderp/exllama) and more.\n",
    "\n",
    "It is common to use both **Inference servers** and **Model Optimization** techniques in conjunction.  Some inference servers like [TGI](https://github.com/huggingface/text-generation-inference)and [vLLM](https://vllm.readthedocs.io/en/latest/) even help you apply optimization techniques.[^4]\n",
    "\n",
    "[^4]: [The Modular Inference Engine](https://www.modular.com/engine) is another example of an inference server that also applies optimization techniques.  At the time of this writing, this is proprietary technology, but its worth keeping an eye on this in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc13744-23da-4eef-ab09-098a55b596c9",
   "metadata": {},
   "source": [
    "# Notes On Tools\n",
    "\n",
    "Other than benchmarking, an important goal of this study was to understand how to use different platforms & tools. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81863a0e-ef0d-4f25-901e-dacc5ba557fd",
   "metadata": {},
   "source": [
    "## CTranslate2\n",
    "\n",
    "[CTranslate2](https://github.com/OpenNMT/CTranslate2) is an optimization tool that can make models ridiculously fast.  h/t to [Anton](https://twitter.com/abacaj/status/1685107222097903617?s=20). The documentation for CTranslate2 contains [specific instructions for llama models](https://opennmt.net/CTranslate2/guides/transformers.html#llama-2).\n",
    "\n",
    "\n",
    "To optimize `llama v2`, we first need to quantize the model. This can be done like so:\n",
    "\n",
    "```bash\n",
    "ct2-transformers-converter --model meta-llama/Llama-2-7b-hf --quantization int8 --output_dir llama-2-7b-ct2 --force\n",
    "```\n",
    "\n",
    "`meta-llama/Llama-2-7b-hf` refers to the [HuggingFace repo for this model](https://huggingface.co/meta-llama/Llama-2-7b-hf).  The benchmarking code is as follows (can also be found [here](https://github.com/hamelsmu/llama-inference/blob/master/ctranslate/bench.py)):\n",
    "\n",
    "```python\n",
    "import time\n",
    "import ctranslate2\n",
    "import transformers\n",
    "import sys\n",
    "sys.path.append('../common/')\n",
    "from questions import questions\n",
    "import pandas as pd\n",
    "\n",
    "generator = ctranslate2.Generator(\"llama-2-7b-ct2\", device=\"cuda\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "\n",
    "def predict(prompt:str):\n",
    "    \"Generate text give a prompt\"\n",
    "    start = time.perf_counter()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenizer.encode(prompt))\n",
    "    results = generator.generate_batch([tokens], sampling_topk=1, max_length=200, include_prompt_in_result=False)\n",
    "    tokens = results[0].sequences_ids[0]\n",
    "    output = tokenizer.decode(tokens)\n",
    "    request_time = time.perf_counter() - start\n",
    "    return {'tok_count': len(tokens),\n",
    "            'time': request_time,\n",
    "            'question': prompt,\n",
    "            'answer': output,\n",
    "            'note': 'CTranslate2 int8 quantization'}\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    counter = 1\n",
    "    responses = []\n",
    "\n",
    "    for q in questions:\n",
    "        if counter >= 2: responses.append(predict(q))\n",
    "        counter += 1\n",
    "\n",
    "    df = pd.DataFrame(responses)\n",
    "    df.to_csv('bench-ctranslate-int8.csv', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37ef450-a795-4b34-8b41-1bdb9cfd7005",
   "metadata": {},
   "source": [
    "## Text Generation Inference (TGI)\n",
    "\n",
    ":::{.callout-warning}\n",
    "### License Restrictions\n",
    "\n",
    "The license for TGI was [recently changed](https://github.com/huggingface/text-generation-inference/commit/bde25e62b33b05113519e5dbf75abda06a03328e) away from Apache 2.0 to be more restrictive.  Be careful when using TGI in commercial applications.\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    "[Text generation inference](https://github.com/huggingface/text-generation-inference) which is often referred to as ‚ÄúTGI‚Äù was easy to use without any optimization.  You can run it like this:\n",
    "\n",
    "```{.bash filename=‚Äústart_server.sh‚Äù}\n",
    "#!/bin/bash\n",
    "\n",
    "if [ -z \"$HUGGING_FACE_HUB_TOKEN\" ]\n",
    "then\n",
    "  echo \"HUGGING_FACE_HUB_TOKEN is not set. Please set it before running this script.\"\n",
    "  exit 1\n",
    "fi\n",
    "\n",
    "model=\"TheBloke/Llama-2-7B-GPTQ\"\n",
    "volume=$PWD/data\n",
    "\n",
    "docker run --gpus all \\\n",
    " -e HUGGING_FACE_HUB_TOKEN=$HUGGING_FACE_HUB_TOKEN \\\n",
    " -e GPTQ_BITS=4 -e GPTQ_GROUPSIZE=128 \\\n",
    " --shm-size 5g -p 8081:80 \\\n",
    " -v $volume:/data ghcr.io/huggingface/text-generation-inference \\\n",
    " --max-best-of 1 \"$@\"\n",
    "```\n",
    "\n",
    "We can then run the server with this command:\n",
    "\n",
    "```bash\n",
    "bash start_server.sh --model-id \"meta-llama/Llama-2-7b-hf\"\n",
    "```\n",
    "\n",
    ":::{.callout-note}\n",
    "\n",
    "#### Help\n",
    "\n",
    "You can see all the options for the TGI container with the help flag like so:\n",
    "\n",
    "```bash\n",
    "docker run ghcr.io/huggingface/text-generation-inference --help | less\n",
    "```\n",
    ":::\n",
    "\n",
    "### Quantization\n",
    "\n",
    "Quantization was very difficult to get working.  There is a `‚Äîquantize` flag with accepts `bitsandbytes` and `gptq`.  The `bitsandbytes` approach makes inference __much__ slower, which [others have reported](https://github.com/huggingface/text-generation-inference/issues/309#issuecomment-1542124381).  \n",
    "\n",
    "To make `gptq` work for llama v2 models requires a bunch of work, you have to [install the text-generation-server](https://github.com/huggingface/text-generation-inference/tree/main/server) which can take a while and is very brittle to get right.  I had to  step through the [Makefile](https://github.com/huggingface/text-generation-inference/blob/main/server/Makefile) carefully.  After that you have to download the weights with:\n",
    "\n",
    "```bash\n",
    "text-generation-server download-weights meta-llama/Llama-2-7b-hf\n",
    "```\n",
    "\n",
    "You can run the following command to perform the quantization (the last argument is the destination directory where the weights are stored).\n",
    "\n",
    "```bash\n",
    "text-generation-server quantize \"meta-llama/Llama-2-7b-hf\" data/quantized/\n",
    "```\n",
    "\n",
    "**However, this step is not needed for the most popular models, as someone will likely already have quantized and uploaded them to the Hub.**\n",
    "\n",
    "#### Pre-Quantized Models\n",
    "\n",
    "Alternatively, you can use a pre-quantized model that has been uploaded to the Hub.  [TheBloke/Llama-2-7B-GPTQ](https://huggingface.co/TheBloke/Llama-2-7B-GPTQ) is a good example of one.  To get this to work, you have to be careful to set the `GPTQ_BITS` and `GPTQ_GROUPSIZE` environment variables to match the config.  For example [This config](https://huggingface.co/TheBloke/Llama-2-7B-GPTQ/blob/main/quantize_config.json#L2-L3) necessitates setting `GPTQ_BITS=4` and `GPTQ_GROUPSIZE=128` These are already set in `start_server.sh` shown above.  [This PR](https://github.com/huggingface/text-generation-inference/pull/671) will eventually fix that.\n",
    "\n",
    "To use the [TheBloke/Llama-2-7B-GPTQ](https://huggingface.co/TheBloke/Llama-2-7B-GPTQ) with TGI, I can use the same bash script with the following arguments:\n",
    "\n",
    "```bash\n",
    "bash start_server.sh --model-id TheBloke/Llama-2-7B-GPTQ --quantize gptq\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f8b335-63b5-4952-8010-8bb661666445",
   "metadata": {},
   "source": [
    "### Comparison Without TGI Server\n",
    "\n",
    "When I first drafted this study I got the following response on twitter:\n",
    "\n",
    "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Based on your code (<a href=\"https://t.co/hSYaPTsEaK\">https://t.co/hSYaPTsEaK</a>) it seems like you measure the full HTTP request, which is like comparing trees to an apple.</p>&mdash; Philipp Schmid (@_philschmid) <a href=\"https://twitter.com/_philschmid/status/1685187971400470528?ref_src=twsrc%5Etfw\">July 29, 2023</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n",
    "\n",
    "Phillip certainly has a point!  I am indeed testing both!  I'm looking for big differences in tools here, and since some inference servers have optimization tools, and some optimization tools do not have an inference server I cannot do a true apples to apples comparison.  However, I think its still useful to try different things as advertised to see what is possible, and also take note of really significant gaps in latency between tools.\n",
    "\n",
    "Therefore, I ran the following tests to perform the similar optimizations as TGI, but without the server to see what happened:\n",
    "\n",
    "#### HuggingFace Transformers\n",
    "\n",
    "I was able to get slightly better performance without the TGI server as predicted by Phillip, **but it did not account for the the massive gap between some tools** (which is exactly the kind of thing I was looking for). \n",
    "\n",
    "To benchmark quantization with bitsandbytes, I [followed this blog post](https://huggingface.co/blog/4bit-transformers-bitsandbytes) and wrote [this benchmarking code](https://github.com/hamelsmu/llama-inference/blob/master/hf/bench.py).  I quantized the model by loading it like this:\n",
    "\n",
    "```python\n",
    "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "nf4_config = BitsAndBytesConfig(\n",
    "   load_in_4bit=True,\n",
    "   bnb_4bit_quant_type=\"nf4\",\n",
    "   bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "model_nf4 = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=nf4_config)\n",
    "```\n",
    "\n",
    "Unlike TGI, I was able to get bitsandbytes to work properly here, but just like TGI it didn't speed anything up for me with respect to inference latency. As reflected in the benchmark table, I got nearly the same results with transformers [without any optimizations](https://github.com/hamelsmu/llama-inference/blob/master/hf/bench.py).  \n",
    "\n",
    "#### GPTQ\n",
    "\n",
    "I also quantized the model using [AutoGPTQ](https://github.com/PanQiWei/AutoGPTQ) without an inference server to compare against TGI.  The code for that is [here](https://github.com/hamelsmu/llama-inference/blob/master/hf/bench-gptq.py).\n",
    "\n",
    "The results were so bad ~ 5 tok/sec that I decided not to put this in the table, because it seemed quite off to me. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad54e87-a359-417a-903c-c8b75b8275fb",
   "metadata": {},
   "source": [
    "## Text Generation WebUI\n",
    "\n",
    "[Aman](https://twitter.com/tmm1/status/1683255057201135616?s=20) let me know about [text-generation-web-ui](https://github.com/oobabooga/text-generation-webui), and also [these instructions](https://github.com/paul-gauthier/aider/issues/110#issuecomment-1644318545) for quickly experimenting with [ExLlama](https://github.com/turboderp/exllama) and [ggml](https://github.com/ggerganov/ggml).  I wasn't able to get the `ggml` variant to work properly, unfortunately.  If you are really serious about using exllama, I recommend trying to use it without the text generation UI and look at the [exllama](https://github.com/turboderp/exllama/tree/master) repo, specifically at [test_benchmark_inference.py](https://github.com/turboderp/exllama/blob/master/test_benchmark_inference.py).  (I didn't have time for this, but if I was going to use exllama for anything serious I would go this route).\n",
    "\n",
    "From the root of the [text-generation-web-ui](https://github.com/oobabooga/text-generation-webui) repo, you can run the following commands to start an inference server optimized with `ExLlama`:\n",
    "\n",
    "```bash\n",
    "python3 download-model.py TheBloke/Llama-2-7B-GPTQ\n",
    "python3 server.py --listen --extensions openai --loader exllama_hf --model TheBloke_Llama-2-7B-GPTQ\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db383b1-310d-48d2-9848-b8f935a59252",
   "metadata": {},
   "source": [
    "After the server was started, I used [this code](https://github.com/hamelsmu/llama-inference/blob/master/exllama/bench.py) to conduct the benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f51263-41d5-4251-8466-1b8cd29b6532",
   "metadata": {},
   "source": [
    "Overall, I didn't like this particular piece of software much.  It's bit bloated because its trying to do too many things at once (An inference server, Web UIs, and other optimizations).  That being said, the documentation is good and it is easy to use.  \n",
    "\n",
    "I don't think there is any particular reason to use this unless you want an end-to-end solution that also comes with a web user-interface (which many people want!)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aea8684-64e5-454f-8567-6a4c02ef7e09",
   "metadata": {},
   "source": [
    "## vLLM\n",
    "\n",
    "[vLLM](https://github.com/vllm-project/vllm.git) only works with CUDA 11.8, which I configured using [this approach](https://hamel.dev/notes/cuda.html).  After configuring CUDA and installing the right version of PyTorch, you need to install the bleeding edge from git:\n",
    "\n",
    "```bash\n",
    "pip install -U git+https://github.com/vllm-project/vllm.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8575703-cc73-40d0-a3a7-88d22389fffa",
   "metadata": {},
   "source": [
    "A good recipe to use for vLLM can be find on [these Modal docs](https://modal.com/docs/guide/ex/vllm_inference).  Surprisingly, I had much lower latency when running on a local `A6000` vs. a hosted `V100` on Modal Labs.  It's possible that I did something wrong here.  Either way, **`vLLM` offered the lowest latency compared to everything else by a significant margin.**  If I really wanted to optimize for latency today, I would reach for `vLLM`.\n",
    "\n",
    "`vLLM` [offers a server](https://vllm.readthedocs.io/en/latest/serving/distributed_serving.html), but I benchmarked the model locally using their tools instead.  The code for the benchmarking can be [found here](https://github.com/hamelsmu/llama-inference/blob/master/vllm/bench.py):\n",
    "\n",
    "```python\n",
    "from vllm import SamplingParams, LLM\n",
    "\n",
    "#from https://modal.com/docs/guide/ex/vllm_inference\n",
    "\n",
    "questions = [\n",
    "    # Coding questions\n",
    "    \"Implement a Python function to compute the Fibonacci numbers.\",\n",
    "    \"Write a Rust function that performs binary exponentiation.\",\n",
    "    \"What are the differences between Javascript and Python?\",\n",
    "    # Literature\n",
    "    \"Write a story in the style of James Joyce about a trip to the Australian outback in 2083, to see robots in the beautiful desert.\",\n",
    "    \"Who does Harry turn into a balloon?\",\n",
    "    \"Write a tale about a time-traveling historian who's determined to witness the most significant events in human history.\",\n",
    "    # Math\n",
    "    \"What is the product of 9 and 8?\",\n",
    "    \"If a train travels 120 kilometers in 2 hours, what is its average speed?\",\n",
    "    \"Think through this step by step. If the sequence a_n is defined by a_1 = 3, a_2 = 5, and a_n = a_(n-1) + a_(n-2) for n > 2, find a_6.\",\n",
    "]\n",
    "\n",
    "MODEL_DIR = \"/home/ubuntu/hamel-drive/vllm-models\"\n",
    "\n",
    "def download_model_to_folder():\n",
    "    from huggingface_hub import snapshot_download\n",
    "    import os\n",
    "\n",
    "    snapshot_download(\n",
    "        \"meta-llama/Llama-2-7b-hf\",\n",
    "        local_dir=MODEL_DIR,\n",
    "        token=os.environ[\"HUGGING_FACE_HUB_TOKEN\"],\n",
    "    )\n",
    "    return LLM(MODEL_DIR)\n",
    "\n",
    "\n",
    "def generate(question, llm, note=None):\n",
    "    response = {'question': question, 'note': note}\n",
    "    sampling_params = SamplingParams(\n",
    "        temperature=1.0,\n",
    "        top_p=1,\n",
    "        max_tokens=200,\n",
    "    )\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    result = llm.generate(question, sampling_params)\n",
    "    request_time = time.perf_counter() - start\n",
    "\n",
    "    for output in result:\n",
    "        response['tok_count'] = len(output.outputs[0].token_ids)\n",
    "        response['time'] = request_time\n",
    "        response['answer'] = output.outputs[0].text\n",
    "    \n",
    "    return response\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    llm = download_model_to_folder()\n",
    "    counter = 1\n",
    "    responses = []\n",
    "\n",
    "    for q in questions:\n",
    "        response = generate(question=q, llm=llm, note='vLLM')\n",
    "        if counter >= 2:\n",
    "            responses.append(response)\n",
    "        counter += 1\n",
    "    \n",
    "    df = pd.DataFrame(responses)\n",
    "    df.to_csv('bench-vllm.csv', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be58f43-5720-46cc-8434-bf59e994946c",
   "metadata": {},
   "source": [
    "## HuggingFace Inference Endpoint\n",
    "\n",
    "I deployed an [inference endpoint](https://ui.endpoints.huggingface.co/) on HuggingFace for [meta-llama/Llama-2-7b-hf](https://huggingface.co/meta-llama/Llama-2-7b-hf), on a `Nvidia A10G` GPU. I didn't try to turn on any optimizations like quantization and wanted to see what the default performance would be like.\n",
    "\n",
    "The documentation for these interfaces can be found [here](https://huggingface.github.io/text-generation-inference/#/).  There is also [a python client](https://huggingface.co/docs/huggingface_hub/package_reference/inference_client#huggingface_hub.InferenceClient.text_generation).\n",
    "\n",
    "Their documentation says they are using TGI under the hood.  However, my latency was significantly faster on their hosted inference platform than using TGI locally.  This could be due to the fact that I used a `A10G` with them but only a `A6000` locally.  It's worth looking into why this discrepancy exists further.\n",
    "\n",
    "The code for this benchmark can be found [here](https://github.com/hamelsmu/llama-inference/blob/master/hf-endpoint/bench.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4b9820-7d72-4581-9d10-686b0aeb61d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
