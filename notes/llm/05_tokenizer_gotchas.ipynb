{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d422c117",
   "metadata": {},
   "source": [
    "---\n",
    "title: Tokenization Gotchas\n",
    "description: Footguns with tokenizers and inferencing LLMs\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecbd872",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Lots of people experience fiddly behavior when using LLMs.  For example:\n",
    "\n",
    "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Unironically I found this to be very helpful when prompting LLMs. Giving them spaces and new lines <a href=\"https://t.co/vVuxcCuDzB\">pic.twitter.com/vVuxcCuDzB</a></p>&mdash; anton (@abacaj) <a href=\"https://twitter.com/abacaj/status/1728190808191537604?ref_src=twsrc%5Etfw\">November 24, 2023</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n",
    "\n",
    "If you aren't careful, these can be very hard to debug.  This is because of the subtle ways tokenizers work that is not always easy to tell by looking at the text.  \n",
    "\n",
    "## Example\n",
    "\n",
    "The below example demonstrates how things can get confusing and can drift between training and inference time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14be044-f279-4856-8980-e7b15912f6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#|echo: false\n",
    "from transformers import AutoTokenizer\n",
    "from functools import partial\n",
    "model_id = 'Open-Orca/Mistral-7B-OpenOrca'\n",
    "tok = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0817615-1661-4fda-8757-4d0c7c51a2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = partial(tok.encode, add_special_tokens=False)\n",
    "dec = partial(tok.decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f9cd56-f598-4b4c-ad65-d96cf8fda8d3",
   "metadata": {},
   "source": [
    "#### Many frameworks do prompt construction by concatenating tokens\n",
    "\n",
    "Popular frameworks like [axolotl](https://github.com/OpenAccess-AI-Collective/axolotl) construct prompts by concatenating tokens instead of strings.[^1]  It is reasonable to decode the training data to check what the prompt template is:\n",
    "\n",
    "[^1]: This is for good reason, as masking must also be done at the token level.\n",
    "\n",
    "For example, a prompt may be constructed like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c960919-7671-4493-b9c5-381da8d77ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n",
      "<|im_start|>\n"
     ]
    }
   ],
   "source": [
    "axolotl = enc('Ok\\n') + enc('<|im_start|>')\n",
    "print(dec(axolotl))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339ff220-ca48-473a-8c14-41cc38914d63",
   "metadata": {},
   "source": [
    "#### Let's say you have an inference server\n",
    "\n",
    "It's common for inference servers to assemble the prompt for you.  The below looks Like it should be fine, right? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93f0316-971e-44fd-8c97-a5c8df57f610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n",
      "<|im_start|>\n"
     ]
    }
   ],
   "source": [
    "def inf_server(inp): \n",
    "    return f'{inp}\\n<|im_start|>'\n",
    "\n",
    "srv = inf_server('Ok')\n",
    "print(srv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b4eb97-cbdd-4c36-af8e-bb0edc79f2f7",
   "metadata": {},
   "source": [
    "#### Drift between your server and the way the model is trained\n",
    "\n",
    "Wrong!  Notice the difference in the decoding of the prompt vs the training data.  This is a subtle bug that can be hard to debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7870389c-4a67-4542-bedb-e506e55993df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "axolotl training data:  [6504, 13, 32001]\n",
      "your server's decoding: [6504, 32001]\n"
     ]
    }
   ],
   "source": [
    "print(f'axolotl training data:  {axolotl}')\n",
    "print(f\"your server's decoding: {enc(srv)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c818ca1-4258-4ec3-8e08-1e5a765574d3",
   "metadata": {},
   "source": [
    "## Solutions\n",
    "\n",
    "1. Decode your inference data right before your forward pass.  For example, you'll notice the newline is missing if you do this.  This is one way to tell that something fishy is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246702e3-1282-4250-80c4-5976db96e42e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ok<|im_start|>'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec(enc(srv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016274e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ca8964f71c140c79fcf124caba96f01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#|echo: false\n",
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1556437f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
