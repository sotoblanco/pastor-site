{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d422c117",
   "metadata": {},
   "source": [
    "---\n",
    "title: Tokenization Gotchas\n",
    "description: Footguns with tokenizers and inferencing LLMs\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecbd872",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Lots of people experience fiddly behavior when using LLMs.  For example:\n",
    "\n",
    "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Unironically I found this to be very helpful when prompting LLMs. Giving them spaces and new lines <a href=\"https://t.co/vVuxcCuDzB\">pic.twitter.com/vVuxcCuDzB</a></p>&mdash; anton (@abacaj) <a href=\"https://twitter.com/abacaj/status/1728190808191537604?ref_src=twsrc%5Etfw\">November 24, 2023</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n",
    "\n",
    "If you aren't careful, these can be very hard to debug.  This is because of the subtle ways tokenizers work that is not always easy to tell by looking at the text.  \n",
    "\n",
    "## Example\n",
    "\n",
    "The below example demonstrates how things can get confusing and can drift between training and inference time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a14be044-f279-4856-8980-e7b15912f6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamel/mambaforge/envs/honeycomb/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#|echo: false\n",
    "from transformers import AutoTokenizer\n",
    "from functools import partial\n",
    "model_id = 'Open-Orca/Mistral-7B-OpenOrca'\n",
    "tok = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0817615-1661-4fda-8757-4d0c7c51a2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = partial(tok.encode, add_special_tokens=False)\n",
    "dec = partial(tok.decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f9cd56-f598-4b4c-ad65-d96cf8fda8d3",
   "metadata": {},
   "source": [
    "#### Many frameworks do prompt construction by concatenating tokens\n",
    "\n",
    "Popular frameworks like [axolotl](https://github.com/OpenAccess-AI-Collective/axolotl) construct prompts by concatenating tokens instead of strings.[^1]  It is reasonable to decode the training data to check what the prompt template is:\n",
    "\n",
    "[^1]: This is for good reason, as masking must also be done at the token level.\n",
    "\n",
    "For example, a prompt may be constructed like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c960919-7671-4493-b9c5-381da8d77ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n",
      "<|im_start|>\n"
     ]
    }
   ],
   "source": [
    "axolotl = enc('Ok\\n') + enc('<|im_start|>')\n",
    "print(dec(axolotl))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339ff220-ca48-473a-8c14-41cc38914d63",
   "metadata": {},
   "source": [
    "#### Let's say you have an inference server\n",
    "\n",
    "It's common for inference servers to assemble the prompt for you.  The below looks Like it should be fine, right? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f93f0316-971e-44fd-8c97-a5c8df57f610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n",
      "<|im_start|>\n"
     ]
    }
   ],
   "source": [
    "def inf_server(inp): \n",
    "    return f'{inp}\\n<|im_start|>'\n",
    "\n",
    "srv = inf_server('Ok')\n",
    "print(srv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b4eb97-cbdd-4c36-af8e-bb0edc79f2f7",
   "metadata": {},
   "source": [
    "#### Drift between your server and the way the model is trained\n",
    "\n",
    "Wrong!  Notice the difference in the decoding of the prompt vs the training data.  This is a subtle bug that can be hard to debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7870389c-4a67-4542-bedb-e506e55993df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "axolotl training data:  [6504, 13, 32001]\n",
      "your server's decoding: [6504, 32001]\n"
     ]
    }
   ],
   "source": [
    "print(f'axolotl training data:  {axolotl}')\n",
    "print(f\"your server's decoding: {enc(srv)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c818ca1-4258-4ec3-8e08-1e5a765574d3",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "Decode your inference data right before your forward pass.  For example, you'll notice the newline is missing if you do this.  This is one way to tell that something fishy is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "246702e3-1282-4250-80c4-5976db96e42e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ok<|im_start|>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec(enc(srv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "016274e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamel/mambaforge/envs/honeycomb/lib/python3.11/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at /opt/conda/conda-bld/pytorch_1699449201450/work/c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:05<00:00,  3.00s/it]\n"
     ]
    }
   ],
   "source": [
    "#|include: false\n",
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24c063f8-48a5-45e0-bfec-985e6b31b581",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|include: false\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def prompt_w_activations(input_ids):\n",
    "    \"See activations for all tokens in the pre-fill\"\n",
    "    ids = torch.tensor([input_ids])\n",
    "    with torch.no_grad():\n",
    "        outputs = model(ids)\n",
    "        logits = outputs.logits\n",
    "        probs = torch.nn.functional.softmax(logits, dim=1).squeeze()\n",
    "\n",
    "    data = []\n",
    "    for i,idx in enumerate(ids.squeeze()):\n",
    "        argmax = torch.argmax(probs[i, :]).item()\n",
    "        data.append({'id': idx.item(), \n",
    "                     'argmax_id': argmax,\n",
    "                     'argmax_prob': probs[i, argmax].item(),\n",
    "                     'argmax_tok': dec([argmax]),\n",
    "                     'prob': probs[i,idx].item(),\n",
    "                     'token': dec([idx.item()])})\n",
    "    return pd.DataFrame(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
