{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d422c117",
   "metadata": {},
   "source": [
    "---\n",
    "title: Tokenization Gotchas\n",
    "description: Footguns with tokenizers and inferencing LLMs\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aecbd872",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "Lots of people experience fiddly behavior when using LLMs.  For example:\n",
    "\n",
    "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Unironically I found this to be very helpful when prompting LLMs. Giving them spaces and new lines <a href=\"https://t.co/vVuxcCuDzB\">pic.twitter.com/vVuxcCuDzB</a></p>&mdash; anton (@abacaj) <a href=\"https://twitter.com/abacaj/status/1728190808191537604?ref_src=twsrc%5Etfw\">November 24, 2023</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n",
    "\n",
    "If you aren't careful, these can be very hard to debug.  This is because of the subtle ways tokenizers work that is not always easy to see by looking at the text.  \n",
    "\n",
    "## Example\n",
    "\n",
    "The below example demonstrates how things can get confusing and can drift between training and inference time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14be044-f279-4856-8980-e7b15912f6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamel/mambaforge/envs/honeycomb/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "#|output: false\n",
    "from transformers import AutoTokenizer\n",
    "from functools import partial\n",
    "model_id = 'Open-Orca/Mistral-7B-OpenOrca'\n",
    "tok = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0817615-1661-4fda-8757-4d0c7c51a2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = partial(tok.encode, add_special_tokens=False)\n",
    "dec = partial(tok.decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f9cd56-f598-4b4c-ad65-d96cf8fda8d3",
   "metadata": {},
   "source": [
    "### Many frameworks do prompt construction by concatenating tokens\n",
    "\n",
    "Popular frameworks like [axolotl](https://github.com/OpenAccess-AI-Collective/axolotl) construct prompts by concatenating tokens instead of strings.[^1]  It is reasonable to decode the training data to check what the prompt template is:\n",
    "\n",
    "[^1]: This is for good reason, as masking must also be done at the token level.\n",
    "\n",
    "For example, a prompt may be constructed like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c960919-7671-4493-b9c5-381da8d77ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n",
      "<|im_start|>\n"
     ]
    }
   ],
   "source": [
    "axolotl = enc('Ok\\n') + enc('<|im_start|>')\n",
    "print(dec(axolotl))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339ff220-ca48-473a-8c14-41cc38914d63",
   "metadata": {},
   "source": [
    "### Let's say you have an inference server\n",
    "\n",
    "It's common for inference servers to assemble the prompt for you.  The below looks like it should be fine, right? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93f0316-971e-44fd-8c97-a5c8df57f610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ok\n",
      "<|im_start|>\n"
     ]
    }
   ],
   "source": [
    "def inf_server(inp): \n",
    "    return f'{inp}\\n<|im_start|>'\n",
    "\n",
    "srv = inf_server('Ok')\n",
    "print(srv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b4eb97-cbdd-4c36-af8e-bb0edc79f2f7",
   "metadata": {},
   "source": [
    "### Drift between your server and the way the model is trained\n",
    "\n",
    "Wrong!  Notice the difference in the decoding of the prompt vs the training data.  This is a subtle problem that can be hard to debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7870389c-4a67-4542-bedb-e506e55993df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "axolotl training data:  [6504, 13, 32001]\n",
      "your server's decoding: [6504, 32001]\n"
     ]
    }
   ],
   "source": [
    "print(f'axolotl training data:  {axolotl}')\n",
    "print(f\"your server's decoding: {enc(srv)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c818ca1-4258-4ec3-8e08-1e5a765574d3",
   "metadata": {},
   "source": [
    "## Solutions\n",
    "\n",
    "### 1. Decode your inference data\n",
    "\n",
    "Decode your inference data right before your forward pass.  For example, you'll notice the newline is missing if you do this.  This is one way to tell that something fishy is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246702e3-1282-4250-80c4-5976db96e42e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ok<|im_start|>'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec(enc(srv))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73096053",
   "metadata": {},
   "source": [
    "### 2. Use HF chat templating\n",
    "\n",
    "Use the new HuggingFace [chat template](https://huggingface.co/docs/transformers/chat_templating) when possible.  This will help avoid these issues (however, I would still check using method #1 to be sure!).  Related GitHub Issue [comment](https://github.com/huggingface/transformers/issues/25304#issuecomment-1728111915)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8fc071-03eb-4eae-bb9e-061a0b488c6e",
   "metadata": {},
   "source": [
    "## Example: Axolotl vs. HuggingFace Chat Templates\n",
    "\n",
    "This is real example of how tokenization drift can bite you.\n",
    "\n",
    "### Chat Template From HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8375302-88c3-43f4-863d-2240283cb3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] <<SYS>>\n",
      "lorem\n",
      "<</SYS>>\n",
      "\n",
      "abc [/INST] ipsum</s><s>[INST] 123 [/INST] sit</s>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NousResearch/Llama-2-7b-chat-hf\")\n",
    "\n",
    "chat = [\n",
    "   {\"role\": \"system\", \"content\": \"lorem\"},\n",
    "   {\"role\": \"user\", \"content\": \"abc\"},\n",
    "   {\"role\": \"assistant\", \"content\": \"ipsum\"},\n",
    "   {\"role\": \"user\", \"content\": \"123\"},\n",
    "   {\"role\": \"assistant\", \"content\": \"sit\"},\n",
    "]\n",
    "\n",
    "ids = tokenizer.apply_chat_template(chat)\n",
    "print(tokenizer.decode(ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78caf6cc-f6d4-4729-a12b-89186c02f732",
   "metadata": {},
   "source": [
    "### Same thing decoded from Axolotl (with a space after `<s>`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cd6a66-3bc0-4575-8f52-6e425ad901be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] <<SYS>>\n",
      "lorem\n",
      "<</SYS>>\n",
      "\n",
      "abc [/INST] ipsum</s><s> [INST] 123 [/INST] sit</s>\n"
     ]
    }
   ],
   "source": [
    "axolotl_ids = [1, 518, 25580, 29962, 3532, 14816, 29903, 6778, 13, \n",
    "                29880, 3668, 13, 29966, 829, 14816, 29903, 6778, 13, \n",
    "                13, 10736, 518, 29914, 25580, 29962, 23421, 2, 1, \n",
    "                518, 25580, 29962, 29871, 29896, 29906, 29941, 518, \n",
    "                29914, 25580, 29962, 7845, 2]\n",
    "print(tokenizer.decode(axolotl_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bab7176-c6a2-4cf4-88fe-1f350e59c2e9",
   "metadata": {},
   "source": [
    "### Let's decode HF tokens one at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbc326c-802b-42f3-a173-19aedc31baaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: <s>\n",
      "29961: [\n",
      "25580: INST\n",
      "29962: ]\n",
      "3532: <<\n",
      "14816: SY\n",
      "29903: S\n",
      "6778: >>\n",
      "13: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in ids[:9]:\n",
    "    print(f'{i}: {tokenizer.decode(i)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f950d1-9593-41a6-9a63-6e18fa17c692",
   "metadata": {},
   "source": [
    "### Let's decode Axolotl tokens one at a time\n",
    "\n",
    "See the second token `518` this is a mismatch with the HF Chat template which is `29961`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1409ec09-89eb-429b-a846-c30a802a8647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1: <s>\n",
      "518: [\n",
      "25580: INST\n",
      "29962: ]\n",
      "3532: <<\n",
      "14816: SY\n",
      "29903: S\n",
      "6778: >>\n",
      "13: \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in axolotl_ids[:9]:\n",
    "    print(f'{i}: {tokenizer.decode(i)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e409f4-997c-4674-b547-43aba8d2a121",
   "metadata": {},
   "source": [
    "## Why does this happen?\n",
    "\n",
    "Axolotl assembles prompts in token space rather than string space.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00acd59-7f69-49ad-ada7-286ca7b13c63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 518, 25580, 29962]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('<s>', add_special_tokens=False) + tokenizer.encode('[INST]', add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813fdb78-6477-4b47-9888-6f8a4f0ffb2d",
   "metadata": {},
   "source": [
    "HF Chat templates interpolate strings instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db856a19-cbdb-4ae7-a515-675ee31ff21f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 29961, 25580, 29962]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('<s>[INST]', add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb3224e-a6bc-4073-992b-d48262452790",
   "metadata": {},
   "source": [
    "## Other Examples\n",
    "\n",
    "These are other examples of people being bitten by drift between differences in tokenization between training and inference time:\n",
    "\n",
    "1. This [GitHub Issue](https://github.com/huggingface/transformers/issues/25304).\n",
    "2. This [Tweet](https://twitter.com/johnowhitaker/status/1732097798286475578)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
