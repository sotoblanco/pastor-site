{
 "cells": [
  {
   "cell_type": "raw",
   "id": "254ac5f2-c854-4994-ba1c-a6498d87bd08",
   "metadata": {},
   "source": [
    "---\n",
    "title: Max Inference Engine\n",
    "description: \"Attempting to load Mistral-7b in Modular's new Max Inference Engine\"\n",
    "categories: [llm, serving]\n",
    "date: 2024-02-29\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156bdf4b-38d1-4b75-bca0-267f1e41ac39",
   "metadata": {},
   "source": [
    "## Impressions\n",
    "\n",
    "[Mark Saroufim](https://twitter.com/marksaroufim?lang=en) and I attempted to load Mistral-7b into the Max Engine for an hour to no avail.\n",
    "\n",
    "Here are my initial impressions.\n",
    "\n",
    "- The Modular team is super engaging and friendly!  I recommend going to [their discord](https://discord.gg/e6kQTD4k) with questions.\n",
    "- Pytorch feels (and is as far as we can tell) a second class citizen.\n",
    "    - You have to do many more steps to potentially load a Pytorch model vs. TF, and those steps are not clear.[^1]\n",
    "- I'm not sure why in 2024 you would lead with BERT/Tensorflow.  This is an odd choice, and makes on-boarding much less exciting for me.  I would like to see paved paths for modern LLMs like Llama or Mistral - the fact that they are not gives me pause.\n",
    "- Model compilation and loading took 5 minutes.  Compilation needs a progress bar given that it is so long.\n",
    "- Torchscript as a serialization format is older and in maintenance mode compared to more recent `torch.compile` or `torch.export` but Max doesn't support that yet.  Discussion [is here](https://discord.com/channels/1087530497313357884/1212827597323509870/1212889053821796382).\n",
    "- Printing the model is not informative, like it is when you print a torch model (doesn't show you all the layers and shapes).\n",
    "- We couldn't quite understand the output of the model and we eventually hypothesized that `torch.script` is not the right serialization path for Mistral, but we aren't sure.  I think users will get quite confused by this.\n",
    "\n",
    "I'm hoping that the above changes soon, as I'm pretty bullish on the talent level and team working on these things overall.\n",
    "\n",
    "[^1]: The [documentation states](https://docs.modular.com/engine/python/get-started): _\"This example uses is a TensorFlow model (which must be converted to SavedModel format), and it's just as easy to load a model from PyTorch (which must be converted to TorchScript format).\"_ The **just as easy** part raised my expectations a bit too high but I discovered that the Pytorch path is _much_ more onerous."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7799dca-abe5-4772-8d6e-d3cd08917879",
   "metadata": {},
   "source": [
    "## Attempting To Load Mistral In The Max Engine\n",
    "\n",
    "Today, the Modular team released the [Max Inference Engine](https://docs.modular.com/engine/overview):\n",
    "\n",
    "> MAX Engine is a next-generation compiler and runtime system for neural network graphs. It supercharges the execution of AI models in any format (including TensorFlow, PyTorch, and ONNX), on a wide variety of hardware. MAX Engine also allows you to extend these models with custom ops that MAX Engine can analyze and optimize with other ops in the graph.\n",
    "\n",
    "[These docs](https://docs.modular.com/engine/python/get-started) show how to load a TensorFlow model, but I want to load a pytorch LLM like Mistral-7b.  The Modular team [helped me figure out](https://discord.com/channels/1087530497313357884/1212827597323509870/1212844710817824848) how to do this.  I wrote down all the steps here. \n",
    "\n",
    "### 1. Serialize Model as Torchscript\n",
    "\n",
    "In order to load your model in the Max engine we must serialize the model as torchscript.  We can do this by tracing the model graph and then using `torch.jit.save` to save the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f1075b-9a37-4157-9fc5-a65d6e3fff14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51711fc315e74da08a0100b3066cd94c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamel/mambaforge/lib/python3.10/site-packages/transformers/modeling_utils.py:4193: FutureWarning: `_is_quantized_training_enabled` is going to be deprecated in transformers 4.39.0. Please use `model.hf_quantizer.is_trainable` instead\n",
      "  warnings.warn(\n",
      "/home/hamel/mambaforge/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:114: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if (input_shape[-1] > 1 or self.sliding_window is not None) and self.is_causal:\n",
      "/home/hamel/mambaforge/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:162: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if past_key_values_length > 0:\n",
      "/home/hamel/mambaforge/lib/python3.10/site-packages/transformers/modeling_attn_mask_utils.py:411: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if query_length > 1 and not is_tracing:\n",
      "/home/hamel/mambaforge/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:120: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if seq_len > self.max_seq_len_cached:\n",
      "/home/hamel/mambaforge/lib/python3.10/site-packages/transformers/models/mistral/modeling_mistral.py:676: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n",
      "/home/hamel/mambaforge/lib/python3.10/site-packages/torch/jit/_trace.py:1102: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
      "Tensor-likes are not close!\n",
      "\n",
      "Mismatched elements: 3807957 / 4096000 (93.0%)\n",
      "Greatest absolute difference: 8.579869270324707 at index (0, 118, 17554) (up to 1e-05 allowed)\n",
      "Greatest relative difference: 1273150.52 at index (0, 73, 19823) (up to 1e-05 allowed)\n",
      "  _check_trace(\n"
     ]
    }
   ],
   "source": [
    "#|warning: false\n",
    "import time\n",
    "from functools import partial\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "# load model artifacts from the hub\n",
    "hf_path=\"mistralai/Mistral-7B-v0.1\"\n",
    "model = AutoModelForCausalLM.from_pretrained(hf_path,torchscript=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(hf_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# trace the model and save it with torchscript\n",
    "max_seq_len=128\n",
    "model_path=\"mistral.pt\"\n",
    "text = \"This is text to be used for tracing\"\n",
    "# I'm setting the arguments for tokenizer once so I can reuse it later (they need to be consistent)\n",
    "max_tokenizer = partial(tokenizer, return_tensors=\"pt\",padding=\"max_length\", max_length=max_seq_len)\n",
    "inputs = max_tokenizer(text)\n",
    "traced_model = torch.jit.trace(model, [inputs['input_ids'], inputs['attention_mask']])\n",
    "torch.jit.save(traced_model, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95668da1-81c5-44f2-a22f-16ff701e5cfa",
   "metadata": {},
   "source": [
    "### 2. Specify Input Shape\n",
    "\n",
    "Having a set input shape is required for compilation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb9a02e-2d12-4a90-8146-e256929bbef0",
   "metadata": {},
   "source": [
    "This next bit is from [This code](https://github.com/modularml/max/blob/main/examples/inference/bert-python-torchscript/simple-inference.py#L37-L40).  Apparently there is a way to specify dynamic values for the sequence len and batch size, but we couldn't figure that out easily from the docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2dc8d1-cb1e-4be5-bb5f-918571c916b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from max import engine\n",
    "input_spec_list = [\n",
    "    engine.TorchInputSpec(shape=tensor.size(), dtype=engine.DType.int64)\n",
    "    for tensor in inputs.values()\n",
    "]\n",
    "options = engine.TorchLoadOptions(input_spec_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8478461-b53e-42b3-be03-9d70269ac6c7",
   "metadata": {},
   "source": [
    "### 3. Compile and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab67867-bfb4-41a6-9576-d49f3af2b67c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compiling model..   [2349485:2367566:20240229,165403.360772:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2367565:20240229,165403.381422:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "Compiling model..   [2349485:2373801:20240229,165656.931613:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373802:20240229,165656.933624:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373803:20240229,165656.933650:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373804:20240229,165656.933685:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373805:20240229,165656.933712:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373806:20240229,165656.933731:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373807:20240229,165656.933752:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373808:20240229,165656.933777:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373809:20240229,165656.933796:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373810:20240229,165656.933819:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373811:20240229,165656.933836:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373812:20240229,165656.933861:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373813:20240229,165656.933879:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373814:20240229,165656.933901:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373815:20240229,165656.933920:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373816:20240229,165656.933939:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373817:20240229,165656.933956:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373818:20240229,165656.933976:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373819:20240229,165656.933995:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373820:20240229,165656.934017:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373821:20240229,165656.934038:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373822:20240229,165656.934057:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373823:20240229,165656.934075:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373824:20240229,165656.934094:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373825:20240229,165656.934114:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373826:20240229,165656.934135:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373827:20240229,165656.934151:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373828:20240229,165656.934170:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373829:20240229,165656.934187:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373830:20240229,165656.934205:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373831:20240229,165656.934223:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373832:20240229,165656.934243:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373833:20240229,165656.934259:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373800:20240229,165656.934280:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373834:20240229,165656.934297:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373835:20240229,165656.934314:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373836:20240229,165656.934338:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373837:20240229,165656.934358:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373838:20240229,165656.934376:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373839:20240229,165656.934394:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373840:20240229,165656.934413:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373841:20240229,165656.934433:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373842:20240229,165656.934452:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373843:20240229,165656.934469:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373844:20240229,165656.934486:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373845:20240229,165656.934504:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373846:20240229,165656.934519:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373847:20240229,165656.934536:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373848:20240229,165656.934555:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373849:20240229,165656.934574:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373850:20240229,165656.934591:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373851:20240229,165656.934617:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373852:20240229,165656.934634:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373853:20240229,165656.934650:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373854:20240229,165656.934665:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373855:20240229,165656.934685:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373856:20240229,165656.934701:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373857:20240229,165656.934719:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373858:20240229,165656.934734:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373859:20240229,165656.934752:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373860:20240229,165656.934778:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373861:20240229,165656.934797:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373862:20240229,165656.934813:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373863:20240229,165656.934833:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373864:20240229,165656.934853:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373865:20240229,165656.934871:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373866:20240229,165656.934885:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373799:20240229,165656.934979:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373798:20240229,165656.935000:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373797:20240229,165656.943824:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373867:20240229,165656.966056:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "[2349485:2373872:20240229,165657.047675:ERROR crashpad_client_linux.cc:632] sigaltstack: Cannot allocate memory (12)\n",
      "Compiling model...  \n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#|warning: false\n",
    "start_time = time.time()\n",
    "session = engine.InferenceSession()\n",
    "model = session.load(model_path, options)\n",
    "\n",
    "end_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b69ea32-3971-42ac-a17d-f208ba96382d",
   "metadata": {},
   "source": [
    "Wow! The model takes ~5 minutes to compile and load.  Subsequent compilations are faster, but NOT if I restart the Jupyter Kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c4421f-6a2e-4107-9cad-dfde6308a7cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 371.86387610435486 seconds\n"
     ]
    }
   ],
   "source": [
    "elapsed_time = end_time - start_time\n",
    "print(f\"Elapsed time: {elapsed_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b21a03-f579-4dbe-a127-40ff637322e6",
   "metadata": {},
   "source": [
    "### 4. Inference\n",
    "\n",
    ":::{.callout-note}\n",
    "#### We failed to get this to work\n",
    "Even though we could call `model.execute` the outputs we got didn't make much sense to us, even after some investigation.  Our hypothesis is that `execute` is not calling `model.generate`.  But this is where we gave up.\n",
    "\n",
    ":::\n",
    "\n",
    "Be sure to set `return_token_type_ids=False`, note that I'm using the same arguments for `padding` and `max_length` that I used for tracing the model (because I'm using the `max_tokenizer` which I defined) so the shape is consistent.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa922586-8676-4866-95fc-8300cc139d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT=\"Why did the chicken cross the road?\"\n",
    "inp = max_tokenizer(INPUT, return_token_type_ids=False)\n",
    "out = model.execute(**inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d688f3-ad5d-46de-9fa7-fadd85e078ea",
   "metadata": {},
   "source": [
    "Get the token ids (predictions) and decode them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bc115d-e153-4cd9-9ea2-2d2e78186cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = out['result0'].argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf58585f-14fc-4721-b7f0-40b96b933f26",
   "metadata": {},
   "source": [
    "We tried to debug this but could not figure out what was wrong, so we gave up here. We aren't sure why the output looks like this.  See what the output is supposed to look like in [this section](#huggingface-comparison).\n",
    "\n",
    "(Scroll to the right to see the full output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a947b5-6ea4-4230-a22a-93dadec0d642",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ммммммммммммммммммммммммммммммммммммммммммммммммммммммммммммммммммммммммммммммммммммммммммммммммммммммммммммммммммммммм # do you chicken cross the road?\\n'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| code-overflow: wrap\n",
    "' '.join(tokenizer.batch_decode(preds, skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234332cf-b841-44fb-97ce-6c1ae686c373",
   "metadata": {},
   "source": [
    "We were intrigued by the `M` and Mark joked that it is some interesting illuminati secret code injected into the model for (i.e. `M` for `Modular`) which I thought was funny :)\n",
    "\n",
    "Our theory is that torchscript is not the right way to serialize this model and this is some kind of silent failure, but it is hard to know."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19342106-4d14-4ea2-b531-89c71ebee871",
   "metadata": {},
   "source": [
    "## HuggingFace Comparison\n",
    "\n",
    "As a sanity check, let's do inference with `transformers`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ededf7-1c81-4b5c-a255-c9d5a6088a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamel/mambaforge/envs/modular-max/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:01<00:00,  1.50it/s]\n",
      "Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "#|warning: false\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "hf_path=\"mistralai/Mistral-7B-v0.1\"\n",
    "hfmodel = AutoModelForCausalLM.from_pretrained(hf_path,torchscript=True).cuda()\n",
    "hftokenizer = AutoTokenizer.from_pretrained(hf_path)\n",
    "hftokenizer.pad_token = hftokenizer.eos_token\n",
    "\n",
    "_p=\"Why did the chicken cross the road?\"\n",
    "input_ids = hftokenizer(_p, return_tensors=\"pt\", \n",
    "                      padding=\"max_length\",\n",
    "                      truncation=True).input_ids.cuda()\n",
    "out_ids = hfmodel.generate(input_ids=input_ids, max_new_tokens=15, \n",
    "                          do_sample=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd26e4c1-d4c2-4c0f-934f-21df76620c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "To get to the other side.\n",
      "\n",
      "Why did the chicken\n"
     ]
    }
   ],
   "source": [
    "out = hftokenizer.batch_decode(out_ids.detach().cpu().numpy(), \n",
    "                                  skip_special_tokens=True)[0][len(_p):]\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93f6e18-cc2d-437d-8c75-cd7b44b9d781",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
