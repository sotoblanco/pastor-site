{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: vLLM & large models\n",
        "date: last-modified\n",
        "comments:\n",
        "    utterances:\n",
        "        repo: hamelsmu/hamel\n",
        "description: Using tensor parallelism w/ vLLM & Modal to run Llama 70b\n",
        "---"
      ],
      "id": "10ae5c10"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::{.callout-warning}\n",
        "### Correction\n",
        "\n",
        "A previous version of this note suggested that you could run Llama 70b on a single A100.  This was incorrect.  The Modal container was caching the download of the much smaller 7b model.  I have updated the post to reflect this. _h/t to [Cade Daniel](https://twitter.com/edacih) for finding the mistake._\n",
        ":::\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Large models like [Llama-2-70b](https://huggingface.co/meta-llama/Llama-2-70b-hf) may not fit in a single GPU.  I previously [profiled](03_inference.ipynb) the smaller 7b model against various inference tools.  When a model is too big to fit on a single GPU, we can use [various techniques](https://huggingface.co/docs/transformers/v4.15.0/parallelism) to split the model across multiple GPUs.  \n",
        "\n",
        "\n",
        "### Compute & Reproducibility\n",
        "\n",
        "I used [Modal Labs](https://modal.com/) for serverless compute.  Modal is very [economical](https://modal.com/pricing) and built for machine learning use cases.  Unlike other clouds, there are plenty of A100s available.  They even give you $30 of free credits, which is more than enough to run the experiments in this note. Thanks to Modal, the scripts I reference in this note are reproducible.\n",
        "\n",
        "_In this note, I'm using `modal client` version: `0.50.2889`_\n",
        "\n",
        "## Distributed Inference w/ `vLLM`\n",
        "\n",
        "`vLLM` [supports tensor parallelism](https://vllm.readthedocs.io/en/latest/serving/distributed_serving.html), which you can enable by passing the `tensor_parallel_size` argument to the `LLM` constructor.\n",
        "\n",
        "I modified [this example Modal code](https://modal.com/docs/guide/ex/vllm_inference) for `Llama v2 13b` to run `Llama v2 70b` on 4 GPUs with tensor parallelism. Below is a simplified diff with the most important changes:\n",
        "\n",
        "```diff\n",
        "def download_model_to_folder():\n",
        "    from huggingface_hub import snapshot_download\n",
        "\n",
        "    snapshot_download(\n",
        "-        \"meta-llama/Llama-2-13b-chat-hf\",\n",
        "+        \"meta-llama/Llama-2-70b-chat-hf\",\n",
        "        local_dir=\"/model\",\n",
        "        token=os.environ[\"HUGGINGFACE_TOKEN\"],\n",
        "    )\n",
        "\n",
        "image = (\n",
        "    Image.from_dockerhub(\"nvcr.io/nvidia/pytorch:22.12-py3\")\n",
        "    .pip_install(\"torch==2.0.1\", index_url=\"https://download.pytorch.org/whl/cu118\")\n",
        "+    # Pin vLLM to 8/2/2023\n",
        "+    .pip_install(\"vllm @ git+https://github.com/vllm-project/vllm.git@79af7e96a0e2fc9f340d1939192122c3ae38ff17\")\n",
        "-    # Pin vLLM to 07/19/2023\n",
        "-    .pip_install(\"vllm @ git+https://github.com/vllm-project/vllm.git@bda41c70ddb124134935a90a0d51304d2ac035e8\")\n",
        "    # Use the barebones hf-transfer package for maximum download speeds. No progress bar, but expect 700MB/s.\n",
        "-    .pip_install(\"hf-transfer~=0.1\")\n",
        "+     #Force a rebuild to invalidate the cache (you can remove `force_build=True` after the first time)\n",
        "+    .pip_install(\"hf-transfer~=0.1\", force_build=True)\n",
        "    .run_function(\n",
        "        download_model_to_folder,\n",
        "        secret=Secret.from_name(\"huggingface\"),\n",
        "        timeout=60 * 20)\n",
        ")\n",
        "...\n",
        "\n",
        "-@stub.cls(gpu=\"A100\", secret=Secret.from_name(\"huggingface\"))\n",
        "+# You need a minimum of 4 A100s that are the 40GB version\n",
        "+@stub.cls(gpu=gpu.A100(count=4, memory=40), secret=Secret.from_name(\"huggingface\"))\n",
        "class Model:\n",
        "    def __enter__(self):\n",
        "        from vllm import LLM\n",
        "\n",
        "        # Load the model. Tip: MPT models may require `trust_remote_code=true`.\n",
        "-       self.llm = LLM(MODEL_DIR)\n",
        "+       self.llm = LLM(MODEL_DIR, tensor_parallel_size=4)\n",
        "...  \n",
        "```\n",
        "\n",
        "See [big-inference-vllm.py](https://github.com/hamelsmu/hamel/blob/master/notes/llm/inference/big-inference-vllm.py) for the actual script I used.\n",
        "\n",
        ":::{.callout-warning}\n",
        "### Be Careful To Mind The Cache When Downloading Files\n",
        "\n",
        "I found that when I ran the above code and changed the model name, I had to force a rebuild of the image to invalidate the cache.  Otherwise, the old version of the model would be used.  You can force a rebuild by adding `force_build=True` to the `.pip_install` call.\n",
        "\n",
        "When I initially wrote this note, I was fooled into believing I could load `meta-llama/Llama-2-70b-chat-hf` on a single A100.  It was this tricky issue of the container that cached the download of the much smaller `7b` model. ðŸ¤¦\n",
        ":::\n",
        "\n",
        "After setting the appropriate [secrets](https://modal.com/docs/guide/secrets) for HuggingFace and Weights & Biases, You can run this code on Modal with the following command:\n",
        "\n",
        "```bash\n",
        "modal run big-inference-vllm.py\n",
        "```\n",
        "You need **at least 4 A100 GPUs** to serve Llama v2 70b.\n",
        "\n",
        "\n",
        "## What Happens With Smaller Models?\n",
        "\n",
        "Even though distributed inference is interesting for big models that do not fit on a single GPU, interesting things happen when you serve smaller models this way.  Below, I test throughput for `Llama v2 7b` on 1, 2, and 4 GPUs. The throughput is measured by passsing [these 59 prompts](https://github.com/hamelsmu/hamel/blob/master/notes/llm/inference/questions.py) to `llm.generate`.  `llm.generate` is described in the [vLLM documentation](https://vllm.readthedocs.io/en/latest/getting_started/quickstart.html):\n",
        "\n",
        "> Call `llm.generate` to generate the outputs. It adds the input prompts to vLLM engineâ€™s waiting queue and executes the vLLM engine to generate the outputs with high throughput.\n",
        "\n",
        "Here are the results, averaged over 5 runs for each row:\n"
      ],
      "id": "e72fff0f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#|echo: false\n",
        "import os, wandb\n",
        "import pandas as pd\n",
        "pd.set_option('display.precision', 1)\n",
        "assert os.getenv('WANDB_API_KEY'), 'You must set the WANDB_API_KEY environment variable'\n",
        "path = 'hamelsmu/llama-inference'\n",
        "api = wandb.Api()\n",
        "runs = api.runs(path=path)\n",
        "def get_data(run):\n",
        "    return {k:v for k,v in run.summary.items() if not k.startswith('_') and k not in ['outputs']}\n",
        "data = pd.DataFrame([get_data(run) for run in runs])\n",
        "\n",
        "data['model'] = data['model'].str.replace('meta-llama/', '')\n",
        "\n",
        "summary = data.groupby(['model', 'GPU', 'num_gpus', ]).mean()[['tok/sec']]\n",
        "summary.rename(columns={'tok/sec': 'avg tok/sec'})"
      ],
      "id": "4f3a370a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can see all the individual runs [here](https://wandb.ai/hamelsmu/llama-inference?workspace=user-hamelsmu). In my experiments, the 70b model needed a minimum of 4 A100s to run, so that's why there is only one row for that model (Modal only has instances with 1, 2, or 4 GPUs).\n",
        "\n",
        "\n",
        "\n",
        ":::{.callout-warning}\n",
        "\n",
        "### Do Not Compare To Latency Benchmark\n",
        "\n",
        "The tok/sec number you see here is VERY different than the latency benchmark shown on [this note](03_inference.ipynb).  This particular benchmark maximizes throughput by running multiple requests in parallel.  The previous latency benchmark measures the time it takes to process a single request.\n",
        "\n",
        ":::\n",
        "\n",
        "### Observations\n",
        "\n",
        "- A100s are much faster than A10s, but A10s are significantly cheaper.[^1] \n",
        "- On A10s, scaling up to more GPUs increases throughput at first, but then seems to diminish.  It appears like there is a Goldilocks zone in terms of the right number of GPUs to maximize throughput.  I did not explore this in detail, as Modal only has instances with specific numbers of GPUs.[^2]\n",
        "- The much larger `Llama v2 70b` model is only ~2x slower than its 7b counterpart.\n",
        "\n",
        "[^1]: As of 8/6/2023 2 A10s costs `.000612 / sec` on Modal, whereas 1 A100 40GB will cost `0.001036 / sec`.  See [this pricing chart](https://modal.com/pricing)\n",
        "[^2]: For A10 and A100s you can only get up to 4 GPUs.  Furthermore, I ran into an issue with vLLM and llama 70b, where it doesn't like an odd number of GPUs.\n",
        "\n",
        "## Aside: Pipeline Parallelism\n",
        "\n",
        "In theory, Pipeline Parallelism (\"PP\") is slower than Tensor Parallelism, but tools for PP are compatible with a wider range of models from the HuggingFace Hub.  By default, HuggingFace [accelerate](https://huggingface.co/docs/accelerate/index) will [automatically](https://huggingface.co/docs/accelerate/usage_guides/big_modeling) split the model across multiple GPUs when you pass `device_map=\"auto\"`.  (Accelerate offers other kinds of parallelism as well, like integrations with [DeepSpeed](https://huggingface.co/docs/accelerate/usage_guides/deepspeed)).\n",
        "\n",
        "[This blog post](https://huggingface.co/blog/accelerate-large-models) and [these docs](https://huggingface.co/docs/accelerate/usage_guides/big_modeling) are an excellent place to start. I will explore this and other kinds of parallelism in future notes."
      ],
      "id": "64f9c8d3"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}