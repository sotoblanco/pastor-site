{
 "cells": [
  {
   "cell_type": "raw",
   "id": "159e9fe6",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "---\n",
    "title: Concurrency For Starlette Apps (e.g FastAPI / FastHTML)\n",
    "description: Concurreny fundamentals for Python web apps\n",
    "date: 2024-10-11\n",
    "image: edison_async.png\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d759004d",
   "metadata": {},
   "source": [
    "**Motivation**: we often want to call LLMs in Starlette based apps ([FastHTML](https://fastht.ml/), [FastAPI](https://fastapi.tiangolo.com/tutorial/security/), etc.) apps, and we don't want to block the server on network calls to APIs.\n",
    "\n",
    "This post documents my explorations of various approaches that can run tasks in the background without blocking the main process.\n",
    "\n",
    ":::{.callout-tip}\n",
    "\n",
    "You can [see this notebook here](https://github.com/hamelsmu/hamel-site/blob/master/notes/fasthtml/concurrency.ipynb). \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b783ab",
   "metadata": {},
   "source": [
    "## Using A SQL Database As A Queue\n",
    "\n",
    " We will use [fastlite](https://github.com/AnswerDotAI/fastlite) as the interface to our SQL database.\n",
    "\n",
    "### Why\n",
    "\n",
    "You are often already using a database for your web application, and if you need to process items in that database with some kind queue, its convenient to use the database itself!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11696ab8",
   "metadata": {},
   "source": [
    "![Screenshot is from this [HN Comment](https://news.ycombinator.com/item?id=27482402)](hn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ed4037",
   "metadata": {},
   "source": [
    "### First, let's define our queue table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15087c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastlite import *\n",
    "Path('queue.db').delete()\n",
    "\n",
    "db = Database('queue.db')\n",
    "\n",
    "class QueueItem:\n",
    "    id: int\n",
    "    data: str\n",
    "    expire: int  # Unix timestamp\n",
    "\n",
    "queue = db.create(QueueItem, pk='id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18182591",
   "metadata": {},
   "source": [
    "### Now, let's implement the enqueue operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6449b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enqueue(data): return queue.insert(data=data, expire=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e95e8f6",
   "metadata": {},
   "source": [
    "### For the dequeue operation, we'll implement the logic described in the comment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff185083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def dequeue():\n",
    "    available_items = queue(where=\"expire = 0\", limit=1)\n",
    "    \n",
    "    if not available_items: return None  # Queue is empty\n",
    "    \n",
    "    item = available_items[0]\n",
    "    future_time = int(time.time()) + 300  # 5 minutes from now\n",
    "    \n",
    "    # Step 2: UPDATE SET expire = future_time WHERE id = item.id AND expire = 0\n",
    "    updated_item = queue.update(id=item.id, expire=future_time)\n",
    "    \n",
    "    if updated_item.expire == future_time: return updated_item\n",
    "    else: return dequeue()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b34833",
   "metadata": {},
   "source": [
    "### Let's See It In Action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2c077ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Task 1\n",
      "Processing Task 2\n",
      "Processing Task 3\n",
      "Queue is empty\n"
     ]
    }
   ],
   "source": [
    "# Enqueue some items\n",
    "enqueue(\"Task 1\")\n",
    "enqueue(\"Task 2\")\n",
    "enqueue(\"Task 3\")\n",
    "\n",
    "# Dequeue and process items\n",
    "while True:\n",
    "    item = dequeue()\n",
    "    if item is None:\n",
    "        print(\"Queue is empty\")\n",
    "        break\n",
    "    \n",
    "    print(f\"Processing {item.data}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2c6023",
   "metadata": {},
   "source": [
    "## Using Threads To Run Tasks In Background"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09266d7d",
   "metadata": {},
   "source": [
    "Next, we want perform proceessing on items from the queue, but do so in the background.  We can use the `ThreadPoolExecutor` from Python's `concurrent.futures` module to process items in a thread pool without blocking the main process. Here's how we can modify our implementation to achieve this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdce3dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e61d557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proc_item(item): # Simulate some work\n",
    "    print(f\"Processing {item.data}\")\n",
    "    time.sleep(2)\n",
    "    print(f\"Finished processing {item.data}\")\n",
    "\n",
    "def worker():\n",
    "    while True:\n",
    "        item = dequeue()\n",
    "        if item is None: break\n",
    "        yield item\n",
    "\n",
    "def run_queue_processor_background(n_workers=3):\n",
    "    def background_task():\n",
    "        with ThreadPoolExecutor(n_workers) as ex: ex.map(proc_item, worker())\n",
    "        print(\"Queue processing completed\")\n",
    "        \n",
    "    # Start the background thread\n",
    "    thread = threading.Thread(target=background_task)\n",
    "    thread.start()\n",
    "    return thread  # Return the thread object in case we want to join it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "72d18aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5): enqueue(f\"Task {i+1}\") # Enqueue some items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33144308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main process continues...\n",
      "Processing Task 1\n",
      "Processing Task 2\n",
      "Processing Task 3\n",
      "Finished processing Task 2Finished processing Task 1\n",
      "Processing Task 4\n",
      "\n",
      "Processing Task 5\n",
      "Finished processing Task 3\n",
      "Finished processing Task 4Finished processing Task 5\n",
      "\n",
      "Queue processing completed\n"
     ]
    }
   ],
   "source": [
    "processor_thread = run_queue_processor_background()\n",
    "print(\"Main process continues...\") # Main process can continue immediately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df83428c",
   "metadata": {},
   "source": [
    "## Async Processing\n",
    "\n",
    "On a completely separate note, we can use async processing, which is very similar to threads.  The main benefit of async over threads is that async is easier to debug (stacktrace, breakpoints, etc).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df21e39",
   "metadata": {},
   "source": [
    "In the code below, we are calling openai library with asyncio.  You will see that async is faster than sync in this case, because the majority of the work involves waiting for the response, which is perfect for async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "197bd0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import time\n",
    "from openai import AsyncOpenAI, OpenAI\n",
    "\n",
    "prompts = [\"Tell me a joke\", \"What's the capital of France?\", \"Explain quantum computing\", \"How many planets are in the solar system?\", \"What is the meaning of life?\", \"How many bytes are in a kilobyte?\", \"When was the first iPhone released?\", \"What is the capital of Canada?\", \"What is the capital of Australia?\", \"What is the capital of the United Kingdom?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc1988c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sync processing time: 9.15 seconds\n",
      "Async processing time: 3.99 seconds\n"
     ]
    }
   ],
   "source": [
    "async def async_process_prompt(client, prompt):\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def sync_process_prompt(client, prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "async def async_main():\n",
    "    client = AsyncOpenAI()\n",
    "    start_time = time.time()\n",
    "    tasks = [async_process_prompt(client, prompt) for prompt in prompts]\n",
    "    # you can modify this code (see below) if you wish to just run this completely in the background.\n",
    "    await asyncio.gather(*tasks) \n",
    "    end_time = time.time()\n",
    "    async_time = end_time - start_time\n",
    "    print(f\"Async processing time: {async_time:.2f} seconds\")\n",
    "    return async_time\n",
    "\n",
    "def sync_main():\n",
    "    client = OpenAI()\n",
    "    start_time = time.time()\n",
    "    results = [sync_process_prompt(client, prompt) for prompt in prompts]\n",
    "    end_time = time.time()\n",
    "    sync_time = end_time - start_time\n",
    "    print(f\"Sync processing time: {sync_time:.2f} seconds\")\n",
    "    return sync_time\n",
    "\n",
    "sync_time = sync_main()\n",
    "async_time = await async_main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4bf27e57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Synchronous execution time: 9.15 seconds\n",
      "Asynchronous execution time: 3.99 seconds\n",
      "Time saved with async: 5.16 seconds\n",
      "Speedup factor: 2.29x\n"
     ]
    }
   ],
   "source": [
    "# Compare execution times\n",
    "print(f\"\\nSynchronous execution time: {sync_time:.2f} seconds\")\n",
    "print(f\"Asynchronous execution time: {async_time:.2f} seconds\")\n",
    "print(f\"Time saved with async: {sync_time - async_time:.2f} seconds\")\n",
    "print(f\"Speedup factor: {sync_time / async_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0594ba3b",
   "metadata": {},
   "source": [
    "In the code above, async is only as slow as the slowest single task.  calling `await asyncio.gather(*tasks)` waits until all tasks are finished.  **However, if you just want to run tasks in the background, you can make the following change:**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ba47e7",
   "metadata": {},
   "source": [
    "```diff\n",
    "- tasks = [async_process_prompt(client, prompt) for prompt in prompts]\n",
    "- await asyncio.gather(*tasks)\n",
    "+ tasks = [asyncio.create_task(async_process_prompt(client, prompt)) for prompt in prompts]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9731fc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#|include: false\n",
    "# fh_docs = ck.ctx_fasthtml()['fasthtml_llms_ctx']\n",
    "\n",
    "# %%ai 0\n",
    "\n",
    "# Have a look at $`fh_docs`, let me know if you have any questions\n",
    "\n",
    "# %%ai 0\n",
    "\n",
    "# How would you build a minimal fasthtml app that incorporates the async approach described earlier, specifically I want a handler that can kick off a long running task in the background.  The handler should return a htmx component, but before that kick off the async task.  When the async task is done, it should print \"I'm done\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7dcf554",
   "metadata": {},
   "source": [
    "### Limiting Async Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea28efcb",
   "metadata": {},
   "source": [
    "To limit the number of tasks that can be running concurrently, we can use a `asyncio.Semaphore`. A semaphore allows us to control access to a shared resource, in this case, the number of concurrent tasks. [^1] Here's how it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c09596d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting task 0\n",
      "Starting task 1\n",
      "Starting task 2\n",
      "Starting task 3\n",
      "Starting task 4\n",
      "Finished task 0\n",
      "Finished task 1\n",
      "Finished task 2\n",
      "Finished task 3\n",
      "Finished task 4\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "\n",
    "# Create a semaphore with the maximum number of concurrent tasks\n",
    "max_concurrent_tasks = 5\n",
    "semaphore = asyncio.Semaphore(max_concurrent_tasks)\n",
    "\n",
    "async def limited_task(task_id):\n",
    "    async with semaphore:\n",
    "        print(f\"Starting task {task_id}\")\n",
    "        await asyncio.sleep(2)  # Simulate some work\n",
    "        print(f\"Finished task {task_id}\")\n",
    "\n",
    "\n",
    "tasks = [limited_task(i) for i in range(5)]\n",
    "_ = await asyncio.gather(*tasks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31915a8",
   "metadata": {},
   "source": [
    "[^1]: Thanks to Krisztian for the [suggestion](https://x.com/kk1694/status/1844336706058846496)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b94ab669",
   "metadata": {},
   "source": [
    "## FastHTML App With Async"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018641ec",
   "metadata": {},
   "source": [
    "Here's a minimal FastHTML app that incorporates async.  You have to run this in a notebook to try it!\n",
    "\n",
    ":::{.callout-tip}\n",
    "\n",
    "You can run FastHTML webapps in a Jupyter notebook!  This is nice for learning, interactive development, and writing documentation (like we are doing here)!  See [these docs](https://docs.fastht.ml/tutorials/jupyter_and_fasthtml.html) for more info.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fbd4395",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fasthtml.common import *\n",
    "from fasthtml.jupyter import *\n",
    "from fastcore.utils import *\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6d9eac",
   "metadata": {},
   "source": [
    "### Define the server\n",
    "\n",
    ":::{.callout-note}\n",
    "\n",
    "The \"I'm done\" messages will be printed after this cell, because all of the console output is printed where the server is defined in a Jupyter notebook.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d47e2b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm done\n",
      "I'm done\n"
     ]
    }
   ],
   "source": [
    "#The \"I'm done\" messages will be printed after this cell\n",
    "if IN_JUPYTER:\n",
    "    from fasthtml.jupyter import JupyUvi, jupy_app, HTMX\n",
    "    app, rt = jupy_app()\n",
    "    server = JupyUvi(app) \n",
    "else:\n",
    "    app,rt = fast_app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e32cc679",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def long_running_task():\n",
    "    await asyncio.sleep(5)  # Simulate a long-running task\n",
    "    print(\"I'm done\")\n",
    "\n",
    "@rt(\"/\")\n",
    "def get():\n",
    "    return P(\"Async Task Demo\",\n",
    "        Div(\n",
    "            Button(\"Start Task\", hx_post=\"/start-task\", hx_swap=\"outerHTML\"),\n",
    "            id=\"task-button\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "@rt(\"/start-task\")\n",
    "async def post():\n",
    "    # These will run in the background since we aren't calling await.\n",
    "    # There are multiple tasks, and asyncio.gather is one way of kicking them off\n",
    "    asyncio.gather(long_running_task(), long_running_task())\n",
    "    return Div(\n",
    "        P(\"Task started! Check your console in 5 seconds.\"),\n",
    "        id=\"task-button\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d1fe8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTMX()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8cc516",
   "metadata": {},
   "source": [
    "![](ex_1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f365184b",
   "metadata": {},
   "outputs": [],
   "source": [
    "server.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909c4e82",
   "metadata": {},
   "source": [
    "## Async OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3a595b",
   "metadata": {},
   "source": [
    "Let's show a more realistic example by using OpenAI instead of the sleep. We'll use the OpenAI API to generate a response, and then print it when it's done. Here's the updated code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad60f9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fasthtml.common import *\n",
    "from fasthtml.jupyter import *\n",
    "import asyncio\n",
    "from openai import AsyncOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2e6454d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI response: Why don't skeletons fight each other? They don't have the guts.\n"
     ]
    }
   ],
   "source": [
    "## The console output (from the background task) will be printed after this cell\n",
    "if IN_JUPYTER:\n",
    "    from fasthtml.jupyter import JupyUvi, jupy_app, HTMX\n",
    "    app, rt = jupy_app()\n",
    "    server = JupyUvi(app) \n",
    "else:\n",
    "    app,rt = fast_app()\n",
    "\n",
    "# Initialize the AsyncOpenAI client\n",
    "client = AsyncOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99c1eb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def openai_task():\n",
    "    response = await client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": \"Tell me a short joke\"}]\n",
    "    )\n",
    "    joke = response.choices[0].message.content\n",
    "    print(f\"OpenAI response: {joke}\")\n",
    "\n",
    "@rt(\"/\")\n",
    "def get():\n",
    "    return P(\"Async OpenAI Demo\",\n",
    "        Div(\n",
    "            Button(\"Get a Joke\", hx_post=\"/get-joke\", hx_swap=\"outerHTML\"),\n",
    "            id=\"joke-button\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "@rt(\"/get-joke\")\n",
    "async def post():\n",
    "    asyncio.create_task(openai_task())\n",
    "    return Div(\n",
    "        P(\"Joke request sent! Check your console in a minute.\"),\n",
    "        id=\"joke-button\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8972e2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTMX()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c13118",
   "metadata": {},
   "source": [
    "![](ex_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88d8a9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "server.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f447aea",
   "metadata": {},
   "source": [
    "## Threads & Processes\n",
    "\n",
    "Note: Async tasks can be started in the background with threads or processes.  You can also spawn threads or processes from other threads or processes as well.\n",
    "\n",
    "Let's see the basic functionality of threads and processes in python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "996f9dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from fastcore.parallel import parallel\n",
    "\n",
    "def f(x): time.sleep(1); print(x)\n",
    "a = [1,2,3,4,5,6,7,8]\n",
    "\n",
    "# parallel starts a new thread when threadpool=True.\n",
    "def g(): parallel(f, a, threadpool=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ffb0d1",
   "metadata": {},
   "source": [
    "### Run with a process\n",
    "\n",
    "We are starting a thread inside a new process so it runs in the background. Remember, `parallel` will execute `f` in a new thread.\n",
    "\n",
    "It will print kinda wierd because of the threading and things completing at the same time.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95e9c8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154832\n",
      "76\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from multiprocess import Process\n",
    "p = Process(target=g)\n",
    "p.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941232d3",
   "metadata": {},
   "source": [
    "### Run with a thread\n",
    "\n",
    "Instaed of starting a thread in the background with a process, we can also start it with another thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cdc9e371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51\n",
      "8\n",
      "7\n",
      "6\n",
      "\n",
      "4\n",
      "2\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "from threading import Thread\n",
    "t = Thread(target=g)\n",
    "t.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984399be",
   "metadata": {},
   "source": [
    "### How to choose Threads vs. Processes\n",
    "\n",
    "See [my blog post](https://python.hamel.dev/concurrency/).\n",
    "\n",
    "If your tasks involves network calls, consider using threads.  For CPU intensive tasks, use processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063d192b",
   "metadata": {},
   "source": [
    "## Fastcore\n",
    "\n",
    "Fastcore has goodies for threads and processes\n",
    "\n",
    "### `@threaded` decorator\n",
    "\n",
    "This will make functions run in the background in a new thread or process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34c071a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.parallel import threaded\n",
    "\n",
    "@threaded # casuses g to be threaded\n",
    "def g(): parallel(f, a, threadpool=True)\n",
    "    \n",
    "    \n",
    "@threaded(process=True) # casuses h to be run in a process\n",
    "def h(): parallel(f, a, threadpool=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5cec8579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Thread(Thread-9 (g), started 6248116224)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "4\n",
      "6\n",
      "3\n",
      "5\n",
      "7\n",
      "8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "865e155f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Process name='Process-3' pid=40899 parent=40147 started>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42538761\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "h()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb235b0",
   "metadata": {},
   "source": [
    "### `startthread`\n",
    "\n",
    "We can also start a thread by calling the `startthread` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f6ec36a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "12\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fastcore.parallel import startthread\n",
    "\n",
    "def g(): parallel(f, a, threadpool=True)\n",
    "startthread(g)\n",
    "\n",
    "# this will run right away in the main process, since the other code is running in the background\n",
    "print('hello') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65e462a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/hamelsmu/92fd223178522b256776ea3d8787d549"
  },
  "gist": {
   "data": {
    "description": "dataroom/nbs/db_queue.ipynb",
    "public": true
   },
   "id": "92fd223178522b256776ea3d8787d549"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
