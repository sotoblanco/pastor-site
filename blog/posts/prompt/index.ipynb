{
 "cells": [
  {
   "cell_type": "raw",
   "id": "55255c4c-2722-4494-adb5-32485356246c",
   "metadata": {},
   "source": [
    "---\n",
    "title: Fuck You, Show Me The Prompt.\n",
    "description: Quickly understand inscrutable LLM frameworks by intercepting LLM API calls.\n",
    "categories: [llms, ml]\n",
    "author: Hamel Husain\n",
    "toc-location: right-body\n",
    "toc-title: Table Of Contents\n",
    "date: 2024-02-14\n",
    "image: slap_1.jpg\n",
    "draft: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a07dc970-9ee6-42f0-bd0b-77579fe87cbc",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "There are many libraries that aim to make the output of your LLMs better by **re-writing or constructing the prompt for you**.  These libraries purport to make the output of your LLMs:\n",
    "\n",
    "- safer [(ex: guardrails)](https://github.com/guardrails-ai/guardrails)\n",
    "- deterministic [(ex: guidance)](https://github.com/guidance-ai/guidance)\n",
    "- structured [(ex: instructor)](https://github.com/jxnl/instructor)\n",
    "- resilient [(ex: langchain)](https://www.langchain.com/)\n",
    "- ... or even optimized for an arbitrary metric [(ex: DSPy)](https://github.com/stanfordnlp/dspy).\n",
    "\n",
    "A common theme among _some_ of these tools is they encourage users to disintermediate themselves from prompting.\n",
    "\n",
    "> [DSPy](https://github.com/stanfordnlp/dspy): \"This is a new paradigm in which LMs and their prompts fade into the background .... you can compile your program again DSPy will create new effective prompts\"\n",
    "\n",
    "> [guidance](https://github.com/guidance-ai/guidance) \"guidance is a programming paradigm that offers superior control and efficiency compared to conventional prompting ... \"\n",
    "\n",
    "Even when tools don't discourage prompting, I've often found it difficult to retrieve the final prompt(s) these tools send to the language model.  **The prompts sent by these tools to the LLM is a natural language description of what these tools are doing, and is the fastest way to understand how they work.**  Furthermore, some tools have [dense terminology](https://github.com/stanfordnlp/dspy?tab=readme-ov-file#4-two-powerful-concepts-signatures--teleprompters) to describe internal constructs which can further obfuscate what they are doing.  \n",
    "\n",
    "For reasons I'll explain below, I think most people would benefit from the following mindset:\n",
    "\n",
    "![](slap_3.jpeg){fig-align=\"center\"}\n",
    "\n",
    "In this blog post, I'll show you how you can **intercept API calls w/prompts for any tool, without having to fumble through docs or read source code.**  I'll show you how to setup and operate [mitmproxy](https://mitmproxy.org/) with examples from the LLM the tools I previously mentioned.\n",
    "\n",
    "## Motivation: Minimize accidental complexity\n",
    "\n",
    "Before adopting an abstraction, its important to consider the dangers of taking on [accidental complexity](https://dev.to/alexbunardzic/software-complexity-essential-accidental-and-incidental-3i4d). This is acute for LLM abstractions relative to typical programming abstractions.\n",
    "\n",
    "<center>\n",
    "<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">Programming abstraction -&gt; a human-like language you can use to translate your task into machine code<br><br>LLM abstraction -&gt; an unintelligible framework you can use to translate your task into human language</p>&mdash; Hamel Husain (@HamelHusain) <a href=\"https://twitter.com/HamelHusain/status/1754315254413361553\">February 5, 2024</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n",
    "</center>\n",
    "\n",
    "While this is a cheeky comment, it's worth keeping this in mind while evaluating tools.  There are two primary types of automation that tools provide:\n",
    "\n",
    "- **Interleaving code and LLMs:** Expressing this automation is often best done through code, since code must be run to carry out the task.  Examples include routing, executing functions, retries, chaining, etc.\n",
    "- **Re-Writing and constructing prompts**:  Expressing your intent is often best done through natural language.  However, there are exceptions! For example, it is convenient to express a function definition or schema from code instead of natural language.\n",
    "\n",
    "Many frameworks offer both types of automation.  However, going too far with the second type can have negative consequences.  Seeing the prompt allows you decide:\n",
    "\n",
    "1. Is this framework really necessary?\n",
    "2. Should I just steal the final prompt (a string) and jettison the framework?\n",
    "3. Can we write a better prompt than this (shorter, aligned with your intent, etc)?\n",
    "4. Is this the best approach (do the # of API calls seem appropriate)?\n",
    "\n",
    "In my experience, seeing the prompts and API calls are essential to making an informed decisions.\n",
    "\n",
    "## Intercepting LLM API calls\n",
    "\n",
    "There are many possible ways to intercept LLM API calls, such as monkey patching source code or finding a user-facing option.  I've found that those approaches take far too much time since the quality of source code and documentation can vary greatly.  After all, I just want to see API calls without worrying about how the code works!\n",
    "\n",
    "- Monkey patching\n",
    "- Looking for a user-facing option that allows this in the documentation.\n",
    "\n",
    "However, I've found this can be very time consuming.  A framework agnostic way to see API calls is to setup a proxy that logs your outgoing API requests.  This is easy to do with [mitmproxy](https://mitmproxy.org/), an free, open-source HTTPS proxy.\n",
    "\n",
    "\n",
    "### Setting Up mitmproxy\n",
    "\n",
    "This is an opinionated way to setup `mitmproxy`that's beginner-friendly for our intended purposes:\n",
    "\n",
    "1. Follow the installation instructions [on the website](https://mitmproxy.org/)\n",
    "2. Start the interactive UI by running `mitmweb` in the terminal.  Pay attention to the url of the interactive UI in the logs which will look something like this: `Web server listening at http://127.0.0.1:8081/`\n",
    "3. Next, you need to configure your device (i.e. your laptop) to route all traffic through `mitproxy`, which listens on `http://localhost:8080`.  Per the documentation:\n",
    "\n",
    "    >  We recommend to simply search the web on how to configure an HTTP proxy for your system. Some operating system have a global settings, some browser have their own, other applications use environment variables, etc.\n",
    "\n",
    "    In my case, A [google search for \"set proxy for macos\"](https://www.google.com/search?q=set+proxy+for+macos&sca_esv=c51a80de1a7d45f0&rlz=1C5CHFA_enUS1048US1049&sxsrf=ACQVn0_ysjr6Kma2_lX8WbB06iPbDi5gUQ%3A1707764982232&ei=9mzKZYXoDcfy0PEPpJqb2Ao&ved=0ahUKEwiFu4CpwKaEAxVHOTQIHSTNBqsQ4dUDCBA&uact=5&oq=set+proxy+for+macos&gs_lp=Egxnd3Mtd2l6LXNlcnAiE3NldCBwcm94eSBmb3IgbWFjb3MyBBAjGCcyBhAAGBYYHjIGEAAYFhgeMgYQABgWGB4yBhAAGBYYHjILEAAYgAQYigUYhgMyCxAAGIAEGIoFGIYDSMk-UMU7WMU7cAd4AZABAJgBVaABVaoBATG4AQPIAQD4AQHCAgoQABhHGNYEGLAD4gMEGAAgQYgGAZAGCA&sclient=gws-wiz-serp) returned these results:\n",
    "\n",
    "    > choose Apple menu > System Settings, click Network in the sidebar, click a network service on the right, click Details, then click Proxies.\n",
    "\n",
    "    I then insert `localhost` and `8080` in the following places in the UI:\n",
    "\n",
    "    ![](mac.png){fig-align=\"center\"}\n",
    "\n",
    "4. Next, navigate to [http://mitm.it](http://mitm.it) and it will give you instructions on how to install the mitmproxy Certificate Authority (CA), which you will need for intercepting HTTPS requests.  (You can also do this manually [here](https://docs.mitmproxy.org/stable/concepts-certificates/#quick-setup).)  Also, take note of the location of the CA file as we will reference it later.\n",
    "\n",
    "5. You can test that everything works by browsing to a website like [https://mitmproxy.org/](https://mitmproxy.org/), and seeing the corresponding output in the mtimweb UI which for me is located at [http://127.0.0.1:8081/](http://127.0.0.1:8081/) (look at the logs in your terminal to get the URL).\n",
    "\n",
    "6. Now that you set everything up, you can disable the proxy that you previously enabled on your network.  I do this on my mac by toggling the proxy buttons in the screenshot I showed above.  This is because we want to scope the proxy to only the python program to eliminate unnecessary noise.\n",
    "\n",
    ":::{.callout-tip}\n",
    "\n",
    "Networking related software commonly allows you to proxy outgoing requests by setting environment variables.  This is the approach we will use to scope our proxy to specific Python programs.  However, I encourage you to play with other types of programs to see what you find after you are comfortable!\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be749f34-bab7-4081-964d-c971466aba53",
   "metadata": {},
   "source": [
    "### Environment variables for Python\n",
    "\n",
    "We need to set the following environment variables so that the `requests` and `httpx` libraries will direct traffic to the proxy and reference the CA file for HTTPS traffic: \n",
    "\n",
    ":::{.callout-important}\n",
    "\n",
    "Make sure you set these environment variables before running any of the code snippets in this blog post.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c4617c-1d52-4c99-9bed-645efd73ba71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# The location of my CA File\n",
    "cert_file = '/Users/hamel/Downloads/mitmproxy-ca-cert.pem' \n",
    "os.environ['REQUESTS_CA_BUNDLE'] = cert_file\n",
    "os.environ['SSL_CERT_FILE'] = cert_file\n",
    "os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:8080'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3a9753-a6a6-4d7b-895a-4059abcdfc7d",
   "metadata": {},
   "source": [
    "You can do a minimal test by running the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf4a70a-bde5-47c0-b649-3179c233c189",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "requests.post('https://httpbin.org/post', \n",
    "              data={'key': 'value'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fb6c76-81b2-4193-9441-f586e9adcf8c",
   "metadata": {},
   "source": [
    "This will appear in the UI like so:\n",
    "\n",
    "![](mitm_01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88267e7-4324-45dc-9401-8737f7a5b15a",
   "metadata": {},
   "source": [
    "## Examples\n",
    "\n",
    "Now for the fun part, let's run through some examples of LLM libraries and intercept their API calls!\n",
    "\n",
    "### Guardrails\n",
    "\n",
    "Guardrails allows you specify structure and types, which it uses to validate and correct the outputs of large language models.  This is a hello world example from the [`guardrails-ai/guardrails` README](https://github.com/guardrails-ai/guardrails):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c746dbe7-e96f-4ea0-89d5-ca9b0b0685e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"pet_type\": \"dog\",\n",
      "    \"name\": \"Buddy\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from guardrails import Guard\n",
    "import openai\n",
    "\n",
    "class Pet(BaseModel):\n",
    "    pet_type: str = Field(description=\"Species of pet\")\n",
    "    name: str = Field(description=\"a unique pet name\")\n",
    "\n",
    "prompt = \"\"\"\n",
    "    What kind of pet should I get and what should I name it?\n",
    "\n",
    "    ${gr.complete_json_suffix_v2}\n",
    "\"\"\"\n",
    "guard = Guard.from_pydantic(output_class=Pet, prompt=prompt)\n",
    "\n",
    "validated_output, *rest = guard(\n",
    "    llm_api=openai.completions.create,\n",
    "    engine=\"gpt-3.5-turbo-instruct\"\n",
    ")\n",
    "\n",
    "print(f\"{validated_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b1bca2-3a0e-45e3-8afb-9c35c9a86cb7",
   "metadata": {},
   "source": [
    "What is happening here?  How is this structured output and validation working?  Looking at the mitmproxy UI, I can see that the above code resulted in two LLM API calls, the first one with these arguments:\n",
    "\n",
    "```{.json .code-overflow-wrap}\n",
    "{\n",
    "    \"model\": \"gpt-3.5-turbo-instruct\",\n",
    "    \"prompt\": \"\\n    What kind of pet should I get and what should I name it?\\n\\n    \\nGiven below is XML that describes the information to extract from this document and the tags to extract it into.\\n\\n<output>\\n    <string name=\\\"pet_type\\\" description=\\\"Species of pet\\\"/>\\n    <string name=\\\"name\\\" description=\\\"a unique pet name\\\"/>\\n</output>\\n\\n\\nONLY return a valid JSON object (no other text is necessary), where the key of the field in JSON is the `name` attribute of the corresponding XML, and the value is of the type specified by the corresponding XML's tag. The JSON MUST conform to the XML format, including any types and format requests e.g. requests for lists, objects and specific types. Be correct and concise.\\n\\nHere are examples of simple (XML, JSON) pairs that show the expected behavior:\\n- `<string name='foo' format='two-words lower-case' />` => `{'foo': 'example one'}`\\n- `<list name='bar'><string format='upper-case' /></list>` => `{\\\"bar\\\": ['STRING ONE', 'STRING TWO', etc.]}`\\n- `<object name='baz'><string name=\\\"foo\\\" format=\\\"capitalize two-words\\\" /><integer name=\\\"index\\\" format=\\\"1-indexed\\\" /></object>` => `{'baz': {'foo': 'Some String', 'index': 1}}`\\n\\n\\n\\nJson Output:\\n\\n\",\n",
    "    \"temperature\": 0\n",
    "}\n",
    "```\n",
    "\n",
    "Followed by another call with these arguments:\n",
    "\n",
    "```{.json .code-overflow-wrap}\n",
    "{\n",
    "    \"model\": \"gpt-3.5-turbo-instruct\",\n",
    "    \"prompt\": \"\\nI was given the following response, which was not parseable as JSON.\\n\\n\\\"{\\\\n    \\\\\\\"pet_type\\\\\\\": \\\\\\\"dog\\\\\\\",\\\\n    \\\\\\\"name\\\\\\\": \\\\\\\"Buddy\\\"\\n\\nHelp me correct this by making it valid JSON.\\n\\nGiven below is XML that describes the information to extract from this document and the tags to extract it into.\\n\\n<output>\\n    <string name=\\\"pet_type\\\" description=\\\"Species of pet\\\"/>\\n    <string name=\\\"name\\\" description=\\\"a unique pet name\\\"/>\\n</output>\\n\\n\\nONLY return a valid JSON object (no other text is necessary), where the key of the field in JSON is the `name` attribute of the corresponding XML, and the value is of the type specified by the corresponding XML's tag. The JSON MUST conform to the XML format, including any types and format requests e.g. requests for lists, objects and specific types. Be correct and concise. If you are unsure anywhere, enter `null`.\\n\\n\\nJson Output:\\n\\n\",\n",
    "    \"temperature\": 0\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d761ab6d-6210-4e96-b030-d51b530be265",
   "metadata": {},
   "source": [
    "Woof.  That's a whole lot of ceremony to get structured output!  We learned that this library's approach to structured output uses XML schemas (while others use function calling).  It's worth considering if you can fashion a better or simpler approach now that you the magic has been lifted.  Either way, we have immediate insight into how it works without dragging you into uncessary complexity, which is a win.\n",
    "\n",
    ":::{.callout-note}\n",
    "\n",
    "I'm not going to critique  any LLM frameworks in detail in this post.  That is a decision you must make depending on your use-case.  What's important is that you know how they work before you inject them into your workflow.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf336a7-b35b-4294-be25-0f321a39076b",
   "metadata": {},
   "source": [
    "### Guidance\n",
    "\n",
    "Guidance offers constrained generation and programming constructs for writing prompts. Let's dive into a hello world example from [`guidance-ai/guidance` README](https://github.com/guidance-ai/guidance):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe6882f-e22c-4666-9a66-6ce41262c6ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style='margin: 0px; padding: 0px; vertical-align: middle; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>system</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'>You are a cat expert.</div></div><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>user</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'>What are the smallest cats?</div></div><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>assistant</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>The</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> smallest</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> cat</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> breed</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> is</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> the</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'> Sing</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>apur</span><span style='background-color: rgba(0.0, 165.0, 0, 0.15); border-radius: 3px;' title='1.0'>a</span></div></div></pre>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from guidance import models, user, system, assistant, gen\n",
    "gpt = models.OpenAI(\"gpt-3.5-turbo\")\n",
    "\n",
    "with system(): lm = gpt + \"You are a cat expert.\"\n",
    "with user(): lm += \"What are the smallest cats?\"\n",
    "with assistant(): lm += gen(\"answer\", stop=\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64feb9d7-b3f9-46a4-ab92-0023121e8eca",
   "metadata": {},
   "source": [
    "This looks pretty neat!  But what is it doing exactly?  In particular I wonder if the `stop` parameter in `gen` is doing anything.  This is the call from the mitmproxy UI:\n",
    "\n",
    "```{.json .code-overflow-wrap}\n",
    "{\n",
    "    \"max_tokens\": 1000,\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"content\": \"You are a cat expert.\",\n",
    "            \"role\": \"system\"\n",
    "        },\n",
    "        {\n",
    "            \"content\": \"What are the smallest cats?\",\n",
    "            \"role\": \"user\"\n",
    "        }\n",
    "    ],\n",
    "    \"model\": \"gpt-3.5-turbo\",\n",
    "    \"n\": 1,\n",
    "    \"stream\": true,\n",
    "    \"temperature\": 0.0,\n",
    "    \"top_p\": 1.0\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b16b2f-4a06-4848-97e7-8bb56883c15a",
   "metadata": {},
   "source": [
    "Interestingly, this request sent by this library doesn't forward the [`stop` parameter](https://platform.openai.com/docs/api-reference/chat/create) when calling the API, even though it appears to be stopping generation in another way.  In this situation, I'm being charged extra for tokens that are generated by the underlying LLM, even though `guidance` is truncating the output - which is not ideal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a63654-f79d-4bbc-a5bc-24424edd8320",
   "metadata": {},
   "source": [
    "### Langchain\n",
    "\n",
    "Langchain is a multi-tool for all things LLM. Lots of people rely on Langchain when get started with LLMs.  Since Langchain has lots of surface area I'll go through two examples. \n",
    "\n",
    "#### LCEL Batching\n",
    "\n",
    "First, let's take a look at the [this example](https://python.langchain.com/docs/expression_language/why#batch) from their new `LCEL` (langchain expression language) guide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5c742c-885e-4691-a803-22c470072d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}\"\n",
    ")\n",
    "output_parser = StrOutputParser()\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "chain = (\n",
    "    {\"topic\": RunnablePassthrough()} \n",
    "    | prompt\n",
    "    | model\n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70028f3-3f9e-482c-bc63-05fbf0632c61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Why did the ice cream go to therapy?\\n\\nBecause it had too many toppings and couldn't find its flavor!\",\n",
       " 'Why did the tomato turn red?\\n\\nBecause it saw the spaghetti sauce!',\n",
       " 'Why did the dumpling go to the bakery?\\n\\nBecause it kneaded some company!',\n",
       " 'Why did the tofu go to the party?\\n\\nBecause it wanted to blend in with the crowd!',\n",
       " 'Why did the pizza go to the wedding?\\n\\nBecause it wanted to be a little cheesy!']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.batch([\"ice cream\", \"spaghetti\", \"dumplings\", \"tofu\", \"pizza\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87aac66e-8323-4633-b0ef-4a839bba8574",
   "metadata": {},
   "source": [
    "That's interesting!  So how does this actually work?  When looking at mitmproxy, I see _five separate_ API calls:\n",
    "\n",
    "```json\n",
    "{ \"messages\": [{\"content\": \"Tell me a short joke about spaghetti\", \"role\": \"user\"}],\n",
    "  \"model\": \"gpt-3.5-turbo\", \"n\": 1, \"stream\": false, \"temperature\": 0.7}\n",
    "```\n",
    "\n",
    "```json\n",
    "{ \"messages\": [{\"content\": \"Tell me a short joke about ice cream\", \"role\": \"user\"}],\n",
    "  \"model\": \"gpt-3.5-turbo\", \"n\": 1, \"stream\": false, \"temperature\": 0.7}\n",
    "```\n",
    "\n",
    "...and so on for each of the five items in the list.\n",
    "\n",
    "\n",
    "Three separate calls to OpenAI may be sub-optimal as they are not truly [batching requests as allowed by the OpenAI API](https://platform.openai.com/docs/guides/rate-limits/batching-requests).   I've personally hit rate limits when using LCEL in this way - its only until I looked at the API calls that I understood what was happening!  (It's easy to be mislead by the word \"batch\")."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40106cd1-f29b-40a4-a447-905e68113d1b",
   "metadata": {},
   "source": [
    "#### SmartLLMChain\n",
    "\n",
    "Next I'll focus on automation that writes prompts for you, particularly [SmartLLMChain](https://api.python.langchain.com/en/latest/smart_llm/langchain_experimental.smart_llm.base.SmartLLMChain.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600b1e60-6c01-429e-834b-470730b5ea26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_experimental.smart_llm import SmartLLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "hard_question = \"I have a 12 liter jug and a 6 liter jug.\\\n",
    "I want to measure 6 liters. How do I do it?\"\n",
    "prompt = PromptTemplate.from_template(hard_question)\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb002f1-5f1b-40a7-ad4f-c9052813ef1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SmartLLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mI have a 12 liter jug and a 6 liter jug.I want to measure 6 liters. How do I do it?\u001b[0m\n",
      "Idea 1:\n",
      "\u001b[36;1m\u001b[1;3m1. Fill the 12 liter jug completely.\n",
      "2. Pour the contents of the 12 liter jug into the 6 liter jug. This will leave you with 6 liters in the 12 liter jug.\n",
      "3. Empty the 6 liter jug.\n",
      "4. Pour the remaining 6 liters from the 12 liter jug into the now empty 6 liter jug.\n",
      "5. You now have 6 liters in the 6 liter jug.\u001b[0m\n",
      "Idea 2:\n",
      "\u001b[36;1m\u001b[1;3m1. Fill the 12 liter jug completely.\n",
      "2. Pour the contents of the 12 liter jug into the 6 liter jug. This will leave you with 6 liters in the 12 liter jug.\n",
      "3. Empty the 6 liter jug.\n",
      "4. Pour the remaining 6 liters from the 12 liter jug into the now empty 6 liter jug.\n",
      "5. You now have 6 liters in the 6 liter jug.\u001b[0m\n",
      "Critique:\n",
      "\u001b[33;1m\u001b[1;3mFlaws and faulty logic in Idea 1:\n",
      "1. The first step of filling the 12 liter jug completely assumes that the jug can hold exactly 12 liters without any overflow or measurement errors. This assumption may not be accurate in reality.\n",
      "2. Pouring the contents of the 12 liter jug into the 6 liter jug assumes that the 6 liter jug can hold all 12 liters without any spillage or measurement errors. Again, this assumption may not be accurate.\n",
      "3. Emptying the 6 liter jug assumes that it can be emptied completely without any residue or measurement errors. This assumption may not be accurate.\n",
      "4. Pouring the remaining 6 liters from the 12 liter jug into the now empty 6 liter jug assumes that the remaining 6 liters can be accurately measured and transferred without any spillage or measurement errors. This assumption may not be accurate.\n",
      "5. The conclusion that you now have 6 liters in the 6 liter jug assumes that all the measurements and transfers were accurate and error-free, which may not be the case.\n",
      "\n",
      "Flaws and faulty logic in Idea 2:\n",
      "1. The first step of filling the 12 liter jug completely assumes that the jug can hold exactly 12 liters without any overflow or measurement errors. This assumption may not be accurate in reality.\n",
      "2. Pouring the contents of the 12 liter jug into the 6 liter jug assumes that the 6 liter jug can hold all 12 liters without any spillage or measurement errors. Again, this assumption may not be accurate.\n",
      "3. Emptying the 6 liter jug assumes that it can be emptied completely without any residue or measurement errors. This assumption may not be accurate.\n",
      "4. Pouring the remaining 6 liters from the 12 liter jug into the now empty 6 liter jug assumes that the remaining 6 liters can be accurately measured and transferred without any spillage or measurement errors. This assumption may not be accurate.\n",
      "5. The conclusion that you now have 6 liters in the 6 liter jug assumes that all the measurements and transfers were accurate and error-free, which may not be the case.\n",
      "\n",
      "In both ideas, there are several assumptions made about the accuracy and precision of the measurements and transfers, which may not hold true in practice. Additionally, there is no consideration given to the possibility of spillage or measurement errors, which can significantly affect the final result.\u001b[0m\n",
      "Resolution:\n",
      "\u001b[32;1m\u001b[1;3mIdea 1: 1. Fill the 12 liter jug completely.\n",
      "2. Pour the contents of the 12 liter jug into the 6 liter jug. This will leave you with 6 liters in the 12 liter jug.\n",
      "3. Empty the 6 liter jug.\n",
      "4. Pour the remaining 6 liters from the 12 liter jug into the now empty 6 liter jug.\n",
      "5. You now have 6 liters in the 6 liter jug.\n",
      "\n",
      "Idea 2: 1. Fill the 12 liter jug completely.\n",
      "2. Pour the contents of the 12 liter jug into the 6 liter jug. This will leave you with 6 liters in the 12 liter jug.\n",
      "3. Empty the 6 liter jug.\n",
      "4. Pour the remaining 6 liters from the 12 liter jug into the now empty 6 liter jug.\n",
      "5. You now have 6 liters in the 6 liter jug.\n",
      "\n",
      "Improved Answer:\n",
      "1. Fill the 12 liter jug completely.\n",
      "2. Pour the contents of the 12 liter jug into the 6 liter jug until the 6 liter jug is full. This will leave you with 6 liters in the 12 liter jug and the 6 liter jug completely filled.\n",
      "3. Empty the 6 liter jug.\n",
      "4. Pour the remaining 6 liters from the 12 liter jug into the now empty 6 liter jug.\n",
      "5. You now have 6 liters in the 6 liter jug.\n",
      "\n",
      "Full Answer:\n",
      "To measure 6 liters using a 12 liter jug and a 6 liter jug, follow these steps:\n",
      "1. Fill the 12 liter jug completely.\n",
      "2. Pour the contents of the 12 liter jug into the 6 liter jug until the 6 liter jug is full. This will leave you with 6 liters in the 12 liter jug and the 6 liter jug completely filled.\n",
      "3. Empty the 6 liter jug.\n",
      "4. Pour the remaining 6 liters from the 12 liter jug into the now empty 6 liter jug.\n",
      "5. You now have 6 liters in the 6 liter jug.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#| warning: false\n",
    "#| output: false\n",
    "chain = SmartLLMChain(llm=llm, prompt=prompt, \n",
    "                      n_ideas=2, \n",
    "                      verbose=True)\n",
    "result = chain.run({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13be7600-e9eb-4ed2-91d9-a8182858ca83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Idea 1: 1. Fill the 12 liter jug completely.\n",
      "2. Pour the contents of the 12 liter jug into the 6 liter jug. This will leave you with 6 liters in the 12 liter jug.\n",
      "3. Empty the 6 liter jug.\n",
      "4. Pour the remaining 6 liters from the 12 liter jug into the now empty 6 liter jug.\n",
      "5. You now have 6 liters in the 6 liter jug.\n",
      "\n",
      "Idea 2: 1. Fill the 12 liter jug completely.\n",
      "2. Pour the contents of the 12 liter jug into the 6 liter jug. This will leave you with 6 liters in the 12 liter jug.\n",
      "3. Empty the 6 liter jug.\n",
      "4. Pour the remaining 6 liters from the 12 liter jug into the now empty 6 liter jug.\n",
      "5. You now have 6 liters in the 6 liter jug.\n",
      "\n",
      "Improved Answer:\n",
      "1. Fill the 12 liter jug completely.\n",
      "2. Pour the contents of the 12 liter jug into the 6 liter jug until the 6 liter jug is full. This will leave you with 6 liters in the 12 liter jug and the 6 liter jug completely filled.\n",
      "3. Empty the 6 liter jug.\n",
      "4. Pour the remaining 6 liters from the 12 liter jug into the now empty 6 liter jug.\n",
      "5. You now have 6 liters in the 6 liter jug.\n",
      "\n",
      "Full Answer:\n",
      "To measure 6 liters using a 12 liter jug and a 6 liter jug, follow these steps:\n",
      "1. Fill the 12 liter jug completely.\n",
      "2. Pour the contents of the 12 liter jug into the 6 liter jug until the 6 liter jug is full. This will leave you with 6 liters in the 12 liter jug and the 6 liter jug completely filled.\n",
      "3. Empty the 6 liter jug.\n",
      "4. Pour the remaining 6 liters from the 12 liter jug into the now empty 6 liter jug.\n",
      "5. You now have 6 liters in the 6 liter jug.\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4511fd01-3bec-45a4-aac1-e19373b83e09",
   "metadata": {},
   "source": [
    "Neat!  So what happened exactly?  While this API emits logs that show you a lot of information (which I've omitted for brevity), the API request pattern is interesting:\n",
    "\n",
    "1. Two _seperate_ api calls for each \"idea\".\n",
    "2. Another API call that incorporates the two ideas as context, with the prompt:\n",
    "   \n",
    "   > You are a researcher tasked with investigating the 2 response options provided. List the flaws and faulty logic of each answer options. Let'w work this out in a step by step way to be sure we have all the errors:\"\n",
    "\n",
    "4. A final API call that that takes the critique from step 2 and generates an answer.\n",
    "   \n",
    "Its not clear that this approach is optimal.  I am not sure it should take 4 separate API calls to accomplish this task.  Perhaps the critique and the final answer could be generated in one step?  Furthermore, the prompt has a spelling error (`Let'w`) but also overly focuses on the negative about identifying errors - which makes me skeptical that this prompt has been optimized or tested."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a9c56e-b033-43c0-9d23-1088bd3bda3f",
   "metadata": {},
   "source": [
    "### Instructor\n",
    "\n",
    "[Instructor](https://github.com/jxnl/instructor) is a framework for structured outputs.  \n",
    "\n",
    "#### Strucutred data extraction with Pydantic\n",
    "\n",
    "Here is a basic example from the project's [README](https://github.com/jxnl/instructor) that allows you to extract structured data by using Pydantic to define your schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8469f9f6-6998-4de7-8551-c073929e385c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import instructor\n",
    "from openai import OpenAI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "client = instructor.patch(OpenAI())\n",
    "\n",
    "class UserDetail(BaseModel):\n",
    "    name: str\n",
    "    age: int\n",
    "\n",
    "user = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    response_model=UserDetail,\n",
    "    messages=[{\"role\": \"user\", \"content\": \"Extract Jason is 25 years old\"}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b90a4e-7060-4676-b77a-757089e62855",
   "metadata": {},
   "source": [
    "We can see how this works by inspecting the API call logged to mitmproxy:\n",
    "\n",
    "```{.json .code-overflow-wrap}\n",
    "{\n",
    "    \"function_call\": {\n",
    "        \"name\": \"UserDetail\"\n",
    "    },\n",
    "    \"functions\": [\n",
    "        {\n",
    "            \"description\": \"Correctly extracted `UserDetail` with all the required parameters with correct types\",\n",
    "            \"name\": \"UserDetail\",\n",
    "            \"parameters\": {\n",
    "                \"properties\": {\n",
    "                    \"age\": {\n",
    "                        \"title\": \"Age\",\n",
    "                        \"type\": \"integer\"\n",
    "                    },\n",
    "                    \"name\": {\n",
    "                        \"title\": \"Name\",\n",
    "                        \"type\": \"string\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\n",
    "                    \"age\",\n",
    "                    \"name\"\n",
    "                ],\n",
    "                \"type\": \"object\"\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"content\": \"Extract Jason is 25 years old\",\n",
    "            \"role\": \"user\"\n",
    "        }\n",
    "    ],\n",
    "    \"model\": \"gpt-3.5-turbo\"\n",
    "}\n",
    "```\n",
    "\n",
    "This is great. For structured output - **It does exactly what I want, and it correctly uses the OpenAI API the way I would use it** if I were writing this manually (by defining a function schema).  I would consider this specific API a zero-cost abstraction, meaning it does exactly what I expect it to with a minimal surface area.[^1]\n",
    "\n",
    "[^1]: This is a subjective opinion and may be different for you.\n",
    "\n",
    "#### Validation\n",
    "\n",
    "However, instructor has other APIs that are more agressive and write prompts for you.  For example, consider this [validation example](https://jxnl.github.io/instructor/tutorials/4-validation/).  Running through that example should trigger similar questions to the exploration of [Langchain's SmartLLMChain](#SmartLLMChain) above.  In this example, you will observe 3 LLM API calls to get the right answer, with the final payload looking like this:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"function_call\": {\n",
    "        \"name\": \"Validator\"\n",
    "    },\n",
    "    \"functions\": [\n",
    "        {\n",
    "            \"description\": \"Validate if an attribute is correct and if not,\\nreturn a new value with an error message\",\n",
    "            \"name\": \"Validator\",\n",
    "            \"parameters\": {\n",
    "                \"properties\": {\n",
    "                    \"fixed_value\": {\n",
    "                        \"anyOf\": [\n",
    "                            {\n",
    "                                \"type\": \"string\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"type\": \"null\"\n",
    "                            }\n",
    "                        ],\n",
    "                        \"default\": null,\n",
    "                        \"description\": \"If the attribute is not valid, suggest a new value for the attribute\",\n",
    "                        \"title\": \"Fixed Value\"\n",
    "                    },\n",
    "                    \"is_valid\": {\n",
    "                        \"default\": true,\n",
    "                        \"description\": \"Whether the attribute is valid based on the requirements\",\n",
    "                        \"title\": \"Is Valid\",\n",
    "                        \"type\": \"boolean\"\n",
    "                    },\n",
    "                    \"reason\": {\n",
    "                        \"anyOf\": [\n",
    "                            {\n",
    "                                \"type\": \"string\"\n",
    "                            },\n",
    "                            {\n",
    "                                \"type\": \"null\"\n",
    "                            }\n",
    "                        ],\n",
    "                        \"default\": null,\n",
    "                        \"description\": \"The error message if the attribute is not valid, otherwise None\",\n",
    "                        \"title\": \"Reason\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [],\n",
    "                \"type\": \"object\"\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"content\": \"You are a world class validation model. Capable to determine if the following value is valid for the statement, if it is not, explain why and suggest a new value.\",\n",
    "            \"role\": \"system\"\n",
    "        },\n",
    "        {\n",
    "            \"content\": \"Does `According to some perspectives, the meaning of life is to find purpose, happiness, and fulfillment. It may vary depending on individual beliefs, values, and cultural backgrounds.` follow the rules: don't say objectionable things\",\n",
    "            \"role\": \"user\"\n",
    "        }\n",
    "    ],\n",
    "    \"model\": \"gpt-3.5-turbo\",\n",
    "    \"temperature\": 0\n",
    "}\n",
    "```\n",
    "\n",
    "Concretely, I'm curious if the validation and healing steps could be collapsed into one LLM call.  Furthermore, I wonder if generic validation functions (as supplied in the above payload) are the right way to critique output?  I don't know the answer, but this is an interesting design pattern that is worth poking at.\n",
    "\n",
    ":::{.callout-note}\n",
    "As far as LLM frameworks go, I really like this one.  The core functionality of defining schemas with Pydantic is very convenient.  The code is also very readable and easy to understand.  Despite this, I still found it helpful to intercept instructor's API calls to get another perspective.  \n",
    "\n",
    "There is a way to set a logging level in instructor to see the raw API calls, however, I like using a framework agnostic approach :)\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f418d1-dc1e-45b0-b3b8-1647f59658cf",
   "metadata": {},
   "source": [
    "### DSPy\n",
    "\n",
    "[DSPy](https://github.com/stanfordnlp/dspy) is the framework that helps you optimize your prompts to optimize any arbitrary metric.  There is a fairly steep learning curve to DSPy, partly because it introduces many new technical terms specific to its framework like compilers and teleprompters.  However, we can quickly peel back the complexity by looking at the API calls that it makes!\n",
    "\n",
    "Let's run the [minimal working example](https://dspy-docs.vercel.app/docs/quick-start/minimal-example):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e9870b-741b-4b48-8dd4-c7b619fe8693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████| 7473/7473 [00:00<00:00, 46440.90it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████| 1319/1319 [00:00<00:00, 13480.23it/s]\n"
     ]
    }
   ],
   "source": [
    "#|output: false\n",
    "import time\n",
    "import dspy\n",
    "from dspy.datasets.gsm8k import GSM8K, gsm8k_metric\n",
    "start_time = time.time()\n",
    "\n",
    "# Set up the LM\n",
    "turbo = dspy.OpenAI(model='gpt-3.5-turbo-instruct', max_tokens=250,\n",
    "                   api_key='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')\n",
    "dspy.settings.configure(lm=turbo)\n",
    "\n",
    "# Load math questions from the GSM8K dataset\n",
    "gms8k = GSM8K()\n",
    "trainset, devset = gms8k.train, gms8k.dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba9a01d-da36-43ba-a2be-2c6801f9154e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CoT(dspy.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.prog = dspy.ChainOfThought(\"question -> answer\")\n",
    "    \n",
    "    def forward(self, question):\n",
    "        return self.prog(question=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f7feaf-0ad0-4691-9488-a6214abc2bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Going to sample between 1 and 8 traces per predictor.\n",
      "Will attempt to train 10 candidate sets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 164 / 300  (54.7): 100%|██████████████████████████████████| 300/300 [00:00<00:00, 754.58it/s]\n",
      "/Users/hamel/mambaforge/lib/python3.10/site-packages/dspy/evaluate/evaluate.py:137: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(truncate_cell)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 164 / 300  (54.7%)\n",
      "Score: 54.67 for set: [0]\n",
      "New best score: 54.67 for seed -3\n",
      "Scores so far: [54.67]\n",
      "Best score: 54.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 203 / 300  (67.7): 100%|██████████████████████████████████| 300/300 [00:00<00:00, 743.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 203 / 300  (67.7%)\n",
      "Score: 67.67 for set: [8]\n",
      "New best score: 67.67 for seed -2\n",
      "Scores so far: [54.67, 67.67]\n",
      "Best score: 67.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▏                                                                   | 9/200 [00:00<00:00, 763.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 10 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 215 / 300  (71.7): 100%|██████████████████████████████████| 300/300 [00:00<00:00, 673.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 215 / 300  (71.7%)\n",
      "Score: 71.67 for set: [8]\n",
      "New best score: 71.67 for seed -1\n",
      "Scores so far: [54.67, 67.67, 71.67]\n",
      "Best score: 71.67\n",
      "Average of max per entry across top 1 scores: 0.7166666666666667\n",
      "Average of max per entry across top 2 scores: 0.8533333333333334\n",
      "Average of max per entry across top 3 scores: 0.9033333333333333\n",
      "Average of max per entry across top 5 scores: 0.9033333333333333\n",
      "Average of max per entry across top 8 scores: 0.9033333333333333\n",
      "Average of max per entry across top 9999 scores: 0.9033333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▏                                                                   | 9/200 [00:00<00:00, 722.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 7 full traces after 10 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 229 / 300  (76.3): 100%|██████████████████████████████████| 300/300 [00:00<00:00, 577.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 229 / 300  (76.3%)\n",
      "Score: 76.33 for set: [8]\n",
      "New best score: 76.33 for seed 0\n",
      "Scores so far: [54.67, 67.67, 71.67, 76.33]\n",
      "Best score: 76.33\n",
      "Average of max per entry across top 1 scores: 0.7633333333333333\n",
      "Average of max per entry across top 2 scores: 0.8433333333333334\n",
      "Average of max per entry across top 3 scores: 0.8966666666666666\n",
      "Average of max per entry across top 5 scores: 0.9266666666666666\n",
      "Average of max per entry across top 8 scores: 0.9266666666666666\n",
      "Average of max per entry across top 9999 scores: 0.9266666666666666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▍                                                                     | 4/200 [00:00<00:00, 595.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 3 full traces after 5 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 238 / 300  (79.3): 100%|██████████████████████████████████| 300/300 [00:00<00:00, 623.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 238 / 300  (79.3%)\n",
      "Score: 79.33 for set: [8]\n",
      "New best score: 79.33 for seed 1\n",
      "Scores so far: [54.67, 67.67, 71.67, 76.33, 79.33]\n",
      "Best score: 79.33\n",
      "Average of max per entry across top 1 scores: 0.7933333333333333\n",
      "Average of max per entry across top 2 scores: 0.8866666666666667\n",
      "Average of max per entry across top 3 scores: 0.9133333333333333\n",
      "Average of max per entry across top 5 scores: 0.95\n",
      "Average of max per entry across top 8 scores: 0.95\n",
      "Average of max per entry across top 9999 scores: 0.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▎                                                                      | 1/200 [00:00<00:00, 656.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 1 full traces after 2 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 218 / 300  (72.7): 100%|██████████████████████████████████| 300/300 [00:00<00:00, 639.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 218 / 300  (72.7%)\n",
      "Score: 72.67 for set: [8]\n",
      "Scores so far: [54.67, 67.67, 71.67, 76.33, 79.33, 72.67]\n",
      "Best score: 79.33\n",
      "Average of max per entry across top 1 scores: 0.7933333333333333\n",
      "Average of max per entry across top 2 scores: 0.8866666666666667\n",
      "Average of max per entry across top 3 scores: 0.9266666666666666\n",
      "Average of max per entry across top 5 scores: 0.9566666666666667\n",
      "Average of max per entry across top 8 scores: 0.9633333333333334\n",
      "Average of max per entry across top 9999 scores: 0.9633333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|██▊                                                                    | 8/200 [00:00<00:00, 477.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 9 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 234 / 300  (78.0): 100%|██████████████████████████████████| 300/300 [00:00<00:00, 576.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 234 / 300  (78.0%)\n",
      "Score: 78.0 for set: [8]\n",
      "Scores so far: [54.67, 67.67, 71.67, 76.33, 79.33, 72.67, 78.0]\n",
      "Best score: 79.33\n",
      "Average of max per entry across top 1 scores: 0.7933333333333333\n",
      "Average of max per entry across top 2 scores: 0.9033333333333333\n",
      "Average of max per entry across top 3 scores: 0.93\n",
      "Average of max per entry across top 5 scores: 0.9633333333333334\n",
      "Average of max per entry across top 8 scores: 0.97\n",
      "Average of max per entry across top 9999 scores: 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▊                                                                     | 5/200 [00:00<00:00, 560.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 6 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 228 / 300  (76.0): 100%|██████████████████████████████████| 300/300 [00:00<00:00, 814.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 228 / 300  (76.0%)\n",
      "Score: 76.0 for set: [8]\n",
      "Scores so far: [54.67, 67.67, 71.67, 76.33, 79.33, 72.67, 78.0, 76.0]\n",
      "Best score: 79.33\n",
      "Average of max per entry across top 1 scores: 0.7933333333333333\n",
      "Average of max per entry across top 2 scores: 0.9033333333333333\n",
      "Average of max per entry across top 3 scores: 0.93\n",
      "Average of max per entry across top 5 scores: 0.96\n",
      "Average of max per entry across top 8 scores: 0.97\n",
      "Average of max per entry across top 9999 scores: 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|██▊                                                                    | 8/200 [00:00<00:00, 535.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 5 full traces after 9 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 234 / 300  (78.0): 100%|██████████████████████████████████| 300/300 [00:00<00:00, 801.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 234 / 300  (78.0%)\n",
      "Score: 78.0 for set: [8]\n",
      "Scores so far: [54.67, 67.67, 71.67, 76.33, 79.33, 72.67, 78.0, 76.0, 78.0]\n",
      "Best score: 79.33\n",
      "Average of max per entry across top 1 scores: 0.7933333333333333\n",
      "Average of max per entry across top 2 scores: 0.9033333333333333\n",
      "Average of max per entry across top 3 scores: 0.9266666666666666\n",
      "Average of max per entry across top 5 scores: 0.95\n",
      "Average of max per entry across top 8 scores: 0.97\n",
      "Average of max per entry across top 9999 scores: 0.97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█▊                                                                     | 5/200 [00:00<00:00, 639.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 2 full traces after 6 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 222 / 300  (74.0): 100%|██████████████████████████████████| 300/300 [00:00<00:00, 798.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 222 / 300  (74.0%)\n",
      "Score: 74.0 for set: [8]\n",
      "Scores so far: [54.67, 67.67, 71.67, 76.33, 79.33, 72.67, 78.0, 76.0, 78.0, 74.0]\n",
      "Best score: 79.33\n",
      "Average of max per entry across top 1 scores: 0.7933333333333333\n",
      "Average of max per entry across top 2 scores: 0.9033333333333333\n",
      "Average of max per entry across top 3 scores: 0.9266666666666666\n",
      "Average of max per entry across top 5 scores: 0.95\n",
      "Average of max per entry across top 8 scores: 0.9733333333333334\n",
      "Average of max per entry across top 9999 scores: 0.9733333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|███▊                                                                  | 11/200 [00:00<00:00, 799.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 6 full traces after 12 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 228 / 300  (76.0): 100%|██████████████████████████████████| 300/300 [00:00<00:00, 489.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 228 / 300  (76.0%)\n",
      "Score: 76.0 for set: [8]\n",
      "Scores so far: [54.67, 67.67, 71.67, 76.33, 79.33, 72.67, 78.0, 76.0, 78.0, 74.0, 76.0]\n",
      "Best score: 79.33\n",
      "Average of max per entry across top 1 scores: 0.7933333333333333\n",
      "Average of max per entry across top 2 scores: 0.9033333333333333\n",
      "Average of max per entry across top 3 scores: 0.9266666666666666\n",
      "Average of max per entry across top 5 scores: 0.95\n",
      "Average of max per entry across top 8 scores: 0.9666666666666667\n",
      "Average of max per entry across top 9999 scores: 0.9733333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|██▍                                                                   | 7/200 [00:00<00:00, 1007.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 4 full traces after 8 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 240 / 300  (80.0): 100%|██████████████████████████████████| 300/300 [00:00<00:00, 732.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 240 / 300  (80.0%)\n",
      "Score: 80.0 for set: [8]\n",
      "New best score: 80.0 for seed 8\n",
      "Scores so far: [54.67, 67.67, 71.67, 76.33, 79.33, 72.67, 78.0, 76.0, 78.0, 74.0, 76.0, 80.0]\n",
      "Best score: 80.0\n",
      "Average of max per entry across top 1 scores: 0.8\n",
      "Average of max per entry across top 2 scores: 0.89\n",
      "Average of max per entry across top 3 scores: 0.93\n",
      "Average of max per entry across top 5 scores: 0.9533333333333334\n",
      "Average of max per entry across top 8 scores: 0.96\n",
      "Average of max per entry across top 9999 scores: 0.9733333333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|███▏                                                                   | 9/200 [00:00<00:00, 829.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bootstrapped 8 full traces after 10 examples in round 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Average Metric: 207 / 300  (69.0): 100%|███████████████████████████████████| 300/300 [01:03<00:00,  4.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Metric: 207 / 300  (69.0%)\n",
      "Score: 69.0 for set: [8]\n",
      "Scores so far: [54.67, 67.67, 71.67, 76.33, 79.33, 72.67, 78.0, 76.0, 78.0, 74.0, 76.0, 80.0, 69.0]\n",
      "Best score: 80.0\n",
      "Average of max per entry across top 1 scores: 0.8\n",
      "Average of max per entry across top 2 scores: 0.89\n",
      "Average of max per entry across top 3 scores: 0.93\n",
      "Average of max per entry across top 5 scores: 0.9533333333333334\n",
      "Average of max per entry across top 8 scores: 0.96\n",
      "Average of max per entry across top 9999 scores: 0.9766666666666667\n",
      "13 candidate programs found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/hamel/mambaforge/lib/python3.10/site-packages/dspy/evaluate/evaluate.py:137: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(truncate_cell)\n"
     ]
    }
   ],
   "source": [
    "#|output: false\n",
    "from dspy.teleprompt import BootstrapFewShotWithRandomSearch\n",
    "\n",
    "# Set up the optimizer: we want to \"bootstrap\" (i.e., self-generate) 8-shot examples of our CoT program.\n",
    "# The optimizer will repeat this 10 times (plus some initial attempts) before selecting its best attempt on the devset.\n",
    "config = dict(max_bootstrapped_demos=8, max_labeled_demos=8, num_candidate_programs=10, num_threads=4)\n",
    "\n",
    "# Optimize! Use the `gms8k_metric` here. In general, the metric is going to tell the optimizer how well it's doing.\n",
    "teleprompter = BootstrapFewShotWithRandomSearch(metric=gsm8k_metric, **config)\n",
    "optimized_cot = teleprompter.compile(CoT(), trainset=trainset, valset=devset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6736e94f-c8d7-4c11-b551-d83fbcb50f2b",
   "metadata": {},
   "source": [
    ":::{.callout-warning}\n",
    "\n",
    "# This was not very minimal\n",
    "\n",
    "Despite this being the official [quick-start/minimal working](https://dspy-docs.vercel.app/docs/quick-start/minimal-example) example, this code took **more than 30 minutes to run, and made hundreds of calls to OpenAI!**  This cost non-trivial time (and money), especially as an entry-point to the library for someone trying to take a look.  There was no prior warning that this would happen.\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193faa0c-c710-47ab-97f9-6b5999b3776b",
   "metadata": {},
   "source": [
    "DSPy made 100's of API calls because it was iteratively sampling examples for a few-shot prompt and selecting the best ones according to the `gsm8k_metric` on a validation set.  I was able to quickly understand this by scanning through the API requests logged to mitmproxy.\n",
    "\n",
    "DSPy offers an `inspect_history` method which allows you to see the the last `n` prompts and their completions:\n",
    "\n",
    "```python\n",
    "turbo.inspect_history(n=1)\n",
    "```\n",
    "\n",
    "I was able to verify that these prompts matched the last few API calls being made in mitmproxy.  Overall, I would be motivated to potentially keep the prompt and and jettison the library.  That being said, I think I am curious to try this out on industry examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c51d7b7f-dd87-4291-83a7-a67c583a4d14",
   "metadata": {},
   "source": [
    "## My Personal Experience\n",
    "\n",
    "Do I hate LLM libraries?  No!  I think many of the libraries in this blog post could be helpful if used thoughtfully in the right situations.  However, I've witnessed too many people fall into the trap of using these libraries without understanding what they are doing.\n",
    "\n",
    "One thing I focus on as an independent consultant is to make sure my clients don't take on accidental complexity. It's very tempting to adopt additional tools given all the excitement around LLMs.  Looking at prompts is one way to mitigate that temptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5319ef3b-16e5-4875-8535-022371c1dab0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
