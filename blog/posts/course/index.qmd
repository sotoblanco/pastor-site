---
title: "An Open Course on LLMs, Led by Practitioners"
description: A free survey course on LLMs, taught by practitioners.
categories: [llms, courses]
author: Hamel Husain
date: 2024-08-02
image: course.png
draft: true
margin-footer: <br>
---

Today, we are releasing [Mastering LLMs](https://parlance-labs.com/education/), a set of talks and workshops from practitioners on topics like: evals, RAG, fine-tuning and more.  We have meticulously organized and annotated the talks from our popular paid course[^1] and are making it <ins>**open and free to everyone.**</ins>

This is a survey course for engineers and who have some experience with LLMs and need guidance on techniques and approaches to improve AI products.  More importantly, the course is taught by veterans who have worked on LLMs in production, and often AI more generally for decades:

[![_Speakers include Jeremy Howard, Sophia Yang, Simon Willison, JJ Allaire, Wing Lian, Mark Saroufim, Jane Xu, Jason Liu, Emmanuel Ameisen, Hailey Schoelkopf, Johno Whitaker, John Berryman, Ben Clavié, Abhishek Thakur, Kyle Corbitt, Ankur Goyal, Freddy Boulton, Jo Bergum, Eugene Yan, Shreya Shankar, Charles Frye and more_](course.png)](https://parlance-labs.com/education/){target="_blank"}

## Getting The Most Value From The Course

### Prerequisites

The course assumes existing familiarity with LLMs.  If you do not have any experience, we recommend watching [A Hacker’s Guide to LLMs](https://www.youtube.com/watch?v=jkrNMKz9pWU).  We also recommend the tutorial [Instruction Tuning llama2](https://www.philschmid.de/instruction-tune-llama-2) to give you background if you are interested in fine-tuning [^2].

### Navigating The Material

The course has over 40 hours of content.  To help you navigate this, we provide:

- **Organization by subject area**: evals, RAG, fine-tuning, building applications and prompt engineering.
- **Chapter summaries:** quickly peruse topics in each talk and skip ahead
- **Notes, slides, and resources**: these are resources used in the talk, as well as resources to learn more.  Many times we have detailed notes as well!

To get started, [navigate to this page](https://parlance-labs.com/education) and peruse topics that you are interested in.  The course isn't meant to be watched linearly, so feel free to jump around and skip bits that aren't interesting or relevant.  However, we did our best to order the talks within each subject that will maximize learning.  We strongly encourage you to use chapter summaries, notes and resources and be selective with your time. Finally, this is a survey course, so it errs on the side of introducing you topics rather than going deeply into code. It's important to leverage the resources and notes to dive deeper into topics that interest you.

## Stay Connected

I'm continuously learning about LLMs, and enjoy sharing my findings and thoughts. If you're interested in this journey, consider subscribing to my updates.

What to expect:

- Occasional emails with my latest insights on LLMs
- Early access to new content I'm working on
- No spam, just honest thoughts and discoveries

<script async data-uid="6379a28bdb" src="https://hamel.ck.page/6379a28bdb/index.js"></script>

If you have useful notes or resources for a specific talk, please submit a pull request to [this repo](https://github.com/parlance-labs/website/tree/main/education).


[^1]: https://maven.com/parlance-labs/fine-tuning. I was inspired by [fastai](https://course.fast.ai/) and decided to follow their model of making the course material free and open to everyone. We had more than 2,000 students, with generous gifts from Modal, Replicate, HuggingFace, OpenAI, JarvisLabs, RunPod, Langsmith, OpenPipe, Predibase and more in the form of free compute or credits for students.
[^2]: We find that instruction tuning a model to be a very useful educational experience even if you never intend to fine-tune, because it familiarizes you with topics such as (1) working with open weights models (2) generating synthetic data  (3) managing prompts (4) fine-tuning (5)  and generating predictions.
[^3]: Eventually, tools become so good that they encapsulate good processes.  However, this is often not true for nascent or emerging technologies.  