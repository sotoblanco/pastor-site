[
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "üé§ Talks",
    "section": "",
    "text": "These are a list of talks I‚Äôve given:\n\nAutoML, Literate Programming, and Data Tooling Cargo Cults, Vanishing Gradients Podcast with Hugo Bowne Anderson, July 2022.\nHow to evaluate ML Tooling: Guest Lecutre for Stanford CS 329S ML Systems Design, Feb 2022. Slides, Video\nJupyterCon 2020: ‚Äúfastpages - A new, open source Jupyter notebook blogging system.‚Äù. Slides, Video.\nGradient Descent by Weights & Biases: A discussion on Automated Machine Learning, CodeSearchNet, GitHub Actions and MLOps: Video\nGitHub Universe 2019: ‚ÄúMachine Learning Ops With GitHub Actions & Kubernetes‚Äù. Video\nTensorFlow World, 2019: ‚ÄúAutomating your developer workflow on GitHub with Tensorflow‚Äù. Slides, Link\nData Skeptic Interview, Jan 2018: ‚ÄúSemantic Search at Github‚Äù.\nKubeCon 2018, ‚ÄúNatural Language Code Search With Kubeflow‚Äù. Slides, Video\nKDD, London August 2018: Hands on tutorial, ‚ÄúFeature Extraction and Summarization With Sequence to Sequence Learning‚Äù. Tutorial-site\nMl4all, Portland May 2018: ‚ÄúHow to Create Magical Data Products Using Sequence-to-Sequence Models‚Äù. Slides, Video\nODSC, San Francisco Nov 2017: ‚ÄúAdvice For New And Junior Data Scientists‚Äù Video"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hamel's Blog",
    "section": "",
    "text": "I‚Äôm Hamel Husain. I‚Äôm a machine learning engineer who loves building data-science and developer tools üë∑üèº‚Äç‚ôÇÔ∏è. I‚Äôve recently caught the entrepreneurship bug and am incubating something new. I have previously worked at Airbnb, DataRobot, and GitHub, as well as a long stint in management consulting.\nI am a core developer for fastai. I‚Äôve also contributed to many open source ML/data tools, which I‚Äôve listed here."
  },
  {
    "objectID": "index.html#get-in-touch",
    "href": "index.html#get-in-touch",
    "title": "Hamel's Blog",
    "section": "üíº Get In Touch",
    "text": "üíº Get In Touch\nDo you need help operationalizing ML, or with any of the open source projects I‚Äôm involved with? I‚Äôm open to consulting work and other forms of advisory.\nEmail me at hamel.husain@gmail.com, or DM me on  or  if you‚Äôd like to chat!"
  },
  {
    "objectID": "index.html#blog-posts",
    "href": "index.html#blog-posts",
    "title": "Hamel's Blog",
    "section": "üìÆ Blog Posts",
    "text": "üìÆ Blog Posts\nSubscribe via RSS: \n\n\n\n\n\n\nDate\n\n\nTitle\n\n\n\n\n\n\n7/28/22\n\n\nnbdev + Quarto: A new secret weapon for productivity\n\n\n\n\n2/9/22\n\n\nNotebooks in production with Metaflow\n\n\n\n\n2/5/20\n\n\nPython Concurrency: The Tricky Bits\n\n\n\n\n12/18/20\n\n\nghapi, a new third-party Python client for the GitHub API\n\n\n\n\n11/20/20\n\n\nNbdev: A literate programming environment that democratizes software engineering best practices\n\n\n\n\n9/1/20\n\n\nfastcore: An Underrated Python Library\n\n\n\n\n9/1/20\n\n\nData Science Meets Devops: MLOps with Jupyter, Git, & Kubernetes\n\n\n\n\n3/6/20\n\n\nGitHub Actions: Providing Data Scientists With New Superpowers.\n\n\n\n\n2/21/20\n\n\nIntroducing fastpages, An easy to use blogging platform with extra features for Jupyter Notebooks.\n\n\n\n\n9/20/19\n\n\nCodeSearchNet Challenge: Evaluating the State of Semantic Code Search\n\n\n\n\n5/29/18\n\n\nHow To Create Natural Language Semantic Search for Arbitrary Objects With Deep¬†Learning\n\n\n\n\n1/18/18\n\n\nHow To Create Magical Data Products Using Sequence-to-Sequence Models\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blog",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\n\n\n\n\n7/28/22\n\n\nnbdev + Quarto: A new secret weapon for productivity\n\n\n\n\n2/9/22\n\n\nNotebooks in production with Metaflow\n\n\n\n\n2/5/20\n\n\nPython Concurrency: The Tricky Bits\n\n\n\n\n12/18/20\n\n\nghapi, a new third-party Python client for the GitHub API\n\n\n\n\n11/20/20\n\n\nNbdev: A literate programming environment that democratizes software engineering best practices\n\n\n\n\n9/1/20\n\n\nfastcore: An Underrated Python Library\n\n\n\n\n9/1/20\n\n\nData Science Meets Devops: MLOps with Jupyter, Git, & Kubernetes\n\n\n\n\n3/6/20\n\n\nGitHub Actions: Providing Data Scientists With New Superpowers.\n\n\n\n\n2/21/20\n\n\nIntroducing fastpages, An easy to use blogging platform with extra features for Jupyter Notebooks.\n\n\n\n\n9/20/19\n\n\nCodeSearchNet Challenge: Evaluating the State of Semantic Code Search\n\n\n\n\n5/29/18\n\n\nHow To Create Natural Language Semantic Search for Arbitrary Objects With Deep¬†Learning\n\n\n\n\n1/18/18\n\n\nHow To Create Magical Data Products Using Sequence-to-Sequence Models\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "guest-blog.html",
    "href": "guest-blog.html",
    "title": "Guest Blogs",
    "section": "",
    "text": "nbdev + Quarto: A new secret weapon for productivity, the fastai blog, July 2022.\nNotebooks in production with Metaflow Introduces a new Metaflow feature that allows users to use notebooks in production ML workflows.\nPython Concurrency: The Tricky Bits: An exploration of threads, processes, and coroutines in Python, with interesting examples that illuminate the differences between each.\nghapi, a new third-party Python client for the GitHub API by Jeremy Howard & Hamel Husain, GitHub Repo.\nNbdev: A literate programming environment that democratizes software engineering best practices by Hamel Husain, Jeremy Howard, The GitHub Blog.\nfastcore: An Underrated Python Library by Hamel Husain, Jeremy Howard, GitHub Repo.\nData Science Meets Devops: MLOps with Jupyter, Git, & Kubernetes: An end-to-end example of deploying a machine learning product using Jupyter, Papermill, Tekton, GitOps and Kubeflow. by Jeremy Lewi, Hamel_Husain, The Kubeflow Blog.\nIntroducing fastpages, An easy to use blogging platform with extra features for Jupyter Notebooks. by Jeremy Howard & Hamel Husain, GitHub Repo\nGitHub Actions: Providing Data Scientists With New Superpowers by Jeremy Howard & Hamel Husain.\nCodeSearchNet Challenge: Evaluating the State of Semantic Code Search: by Miltiadis Allamanis, Marc Brockschmidt, Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit GitHub Repo\nHow To Create Natural Language Semantic Search for Arbitrary Objects With Deep¬†Learning. (Related: GitHub engineering blog article, Live demo)\nHow To Create Magical Data Products Using Sequence-to-Sequence Models\nHow to Automate Tasks on GitHub With Machine Learning for Fun and Profit\nHow Docker Can Make You A More Effective Data Scientist\nAutomated Machine Learning, A Paradigm Shift That Accelerates Data Scientst Productivity At Airbnb"
  },
  {
    "objectID": "notes/how-to-learn/lhtl.html",
    "href": "notes/how-to-learn/lhtl.html",
    "title": "How to learn",
    "section": "",
    "text": "I read the book Mindshift and it was unituitively so good that I decided to take this class. As a parent, I learned a bunch of things that I think will be beneficial to my children‚Äôs education.\nNotes from class Learning how to learn. These notes are for me and may not make sense for others."
  },
  {
    "objectID": "notes/how-to-learn/lhtl.html#focused-vs-diffused-mode",
    "href": "notes/how-to-learn/lhtl.html#focused-vs-diffused-mode",
    "title": "How to learn",
    "section": "Focused vs Diffused Mode",
    "text": "Focused vs Diffused Mode\nYou can not access focus and diffused mode simultaneously.\nPeople have tried to access diffuse mode of thinking by bringing themselves to the point of sleep and waking up just as they fall asleep. For example, Salvador Dali - holding keys in your hand, and let the sound of keys falling the ground wake you up.\nExercise, going for a walk good way to access diffuse thinking. You must take notes right away b/c diffuse thoughts may evaporate very fast."
  },
  {
    "objectID": "notes/how-to-learn/lhtl.html#procrastination-memory-and-sleep",
    "href": "notes/how-to-learn/lhtl.html#procrastination-memory-and-sleep",
    "title": "How to learn",
    "section": "Procrastination Memory and Sleep",
    "text": "Procrastination Memory and Sleep\nThey advocate the Pomodoro technique to combating procrastination. Its like HITT.\nPeriodic relaxation (every ~ 30 minutes) is important for accessing your diffuse mode. ‚ÄúIts important for the mortar to dry‚Äù.\nSpaced repetition (like Anki) is important for building memory. i\nGo over what you want to learn about right before you go to sleep, this will substantially improve the chances you will dream about it and form new connections about the subject.\nExercise can help create new neurons in your hippocampus (new neurons can be created there in adulthood) and help them survive longer."
  },
  {
    "objectID": "notes/how-to-learn/lhtl.html#writing-tips-diffuse-mode",
    "href": "notes/how-to-learn/lhtl.html#writing-tips-diffuse-mode",
    "title": "How to learn",
    "section": "Writing Tips Diffuse Mode",
    "text": "Writing Tips Diffuse Mode\nDiffuse mode is very important for writing. Editing is like focus mode and creating ideas is diffuse mode. Some rules of thumb: - Do not outline, make a mind map - Do not edit while you are writing (this is really hard to do -> turn off monitor and just write).https://writeordie.com - app that forces you to stay in diffuse mode. You really cannot look at the screen. - Repeating again, do not look at screen while you are writing! Only when editing."
  },
  {
    "objectID": "notes/how-to-learn/lhtl.html#chunking",
    "href": "notes/how-to-learn/lhtl.html#chunking",
    "title": "How to learn",
    "section": "Chunking",
    "text": "Chunking\n‚ÄúTying your shoes‚Äù. Best chunks are subconscious. Spoken language is the best example of chunking. You have to practice to build chunks, you cannot just observe. You have to perform the task yourself.\nYou should scan a chapter before you read it: section headings, pictures, etc. This can help you build chunks."
  },
  {
    "objectID": "notes/how-to-learn/lhtl.html#illusions-of-competence",
    "href": "notes/how-to-learn/lhtl.html#illusions-of-competence",
    "title": "How to learn",
    "section": "Illusions of competence",
    "text": "Illusions of competence\nRight after you read something, look away and repeat to yourself what you recall. You can also draw a concept map. The recall process actually improves memory.\nRecall is better than re-reading. Re-reading is effective when you let time pass so you get spaced repetition. You need to test yourself to make sure you are competent. Recall is a form of testing.\nRecall outside your place of study to strengthen your memory. This is because you can get queues from where you are studying."
  },
  {
    "objectID": "notes/how-to-learn/lhtl.html#deliberate-practice",
    "href": "notes/how-to-learn/lhtl.html#deliberate-practice",
    "title": "How to learn",
    "section": "Deliberate Practice",
    "text": "Deliberate Practice\nFocus on the bits that you find difficult. Interleaving is important, meaning learning different subjects or even sections within one subject at once. Thomas S. Khun discovered that two types of people tend to make scientific breakthroughs: (1) young people (2) those who are trained in another discipline."
  },
  {
    "objectID": "notes/how-to-learn/lhtl.html#procrastination-and-memory",
    "href": "notes/how-to-learn/lhtl.html#procrastination-and-memory",
    "title": "How to learn",
    "section": "Procrastination and Memory",
    "text": "Procrastination and Memory\nYou have already learned about the Pomodoro technique. There are other techniques.\nFocus on the process, not the product. Don‚Äôt focus on completing the homework, focus on the process that leads you to complete the homework. Process is the small chunks of time to chip away at the task. This is the idea behind the Pomodoro. Your only goal is to finish the Pomodoro, for example."
  },
  {
    "objectID": "notes/how-to-learn/lhtl.html#juggling-life-and-learning",
    "href": "notes/how-to-learn/lhtl.html#juggling-life-and-learning",
    "title": "How to learn",
    "section": "Juggling Life and Learning",
    "text": "Juggling Life and Learning\nYou should make to-do list the night before for the next day and write it down. This will allow your subconscious to work on how it will conquer that task. Furthermore, writing it down will allow you to free it from working memory.\nPlan your quitting time is important."
  },
  {
    "objectID": "notes/concurrency.html",
    "href": "notes/concurrency.html",
    "title": "Python Concurrency",
    "section": "",
    "text": "Understand the world of Python concurrency: threads, processes, coroutines and asynchronous programming with a realistic examples."
  },
  {
    "objectID": "notes/concurrency.html#see-this-blog-article.",
    "href": "notes/concurrency.html#see-this-blog-article.",
    "title": "Python Concurrency",
    "section": "See this blog article.",
    "text": "See this blog article."
  },
  {
    "objectID": "notes/pandoc/filters.html",
    "href": "notes/pandoc/filters.html",
    "title": "pandoc filters",
    "section": "",
    "text": "Two python packages\nThe tutorial on pandoc filters can help you get oriented to the general idea. If rolling your own filters, you probably want to use the JSON filters. Furthermore you can understand the pandoc AST by using the -t native flag (examples of this are shown later)."
  },
  {
    "objectID": "notes/pandoc/filters.html#the-minimal-notebook",
    "href": "notes/pandoc/filters.html#the-minimal-notebook",
    "title": "pandoc filters",
    "section": "The minimal notebook",
    "text": "The minimal notebook\nHere is minimal notebook we are working with:\njson title=\"minimal.ipynb\" {  \"cells\": [   {    \"cell_type\": \"markdown\",    \"metadata\": {},    \"source\": [     \"## A minimal notebook\"    ]   },   {    \"cell_type\": \"markdown\",    \"metadata\": {},    \"source\": [     \"<MyTag></MyTag>\"    ]   },   {    \"cell_type\": \"code\",    \"execution_count\": 1,    \"metadata\": {},    \"outputs\": [     {      \"name\": \"stdout\",      \"output_type\": \"stream\",      \"text\": [       \"2\\n\"      ]     }    ],    \"source\": [     \"# Do some arithmetic\\n\",     \"print(1+1)\"    ]   }  ],  \"metadata\": {   \"interpreter\": {    \"hash\": \"42fd40e048e0585f88ec242f050f7ef0895cf845a8dd1159352394e5826cd102\"   },   \"kernelspec\": {    \"display_name\": \"Python 3.9.7 ('base')\",    \"language\": \"python\",    \"name\": \"python3\"   },   \"language_info\": {    \"codemirror_mode\": {     \"name\": \"ipython\",     \"version\": 3    },    \"file_extension\": \".py\",    \"mimetype\": \"text/x-python\",    \"name\": \"python\",    \"nbconvert_exporter\": \"python\",    \"pygments_lexer\": \"ipython3\",    \"version\": \"3.9.7\"   }  },  \"nbformat\": 4,  \"nbformat_minor\": 4 }"
  },
  {
    "objectID": "notes/pandoc/filters.html#minimal-ipynb-to-md-converstion-with-pandoc",
    "href": "notes/pandoc/filters.html#minimal-ipynb-to-md-converstion-with-pandoc",
    "title": "pandoc filters",
    "section": "Minimal ipynb to md converstion with pandoc",
    "text": "Minimal ipynb to md converstion with pandoc\n$ pandoc --to gfm minimal.ipynb\n<div class=\"cell markdown\">\n\n## A minimal notebook\n\n</div>\n\n<div class=\"cell markdown\">\n\n<MyTag></MyTag>\n\n</div>\n\n<div class=\"cell code\" execution_count=\"1\">\n\n``` python\n# Do some arithmetic\nprint(1+1)\n```\n\n<div class=\"output stream stdout\">\n\n    2\n\n</div>\n\n</div>"
  },
  {
    "objectID": "notes/pandoc/filters.html#minimal-ipynb-to-md-converstion-with-quarto",
    "href": "notes/pandoc/filters.html#minimal-ipynb-to-md-converstion-with-quarto",
    "title": "pandoc filters",
    "section": "Minimal ipynb to md converstion with quarto",
    "text": "Minimal ipynb to md converstion with quarto\n$ quarto render minimal.ipynb --to gfm\npandoc\n  to: gfm+footnotes+tex_math_dollars-yaml_metadata_block\n  output-file: minimal.md\n  standalone: true\n  default-image-extension: png\n  filters:\n    - crossref\n\nOutput created: minimal.md\nThis creates\n\n## A minimal notebook\n\n<MyTag></MyTag>\n\n``` python\n# Do some arithmetic\nprint(1+1)\n```\n\n    2\nRunning Pandoc With those Extensions\nrunning pandoc with --standalone --to gfm+footnotes+tex_math_dollars-yaml_metadata_block still adds the divs and looks different than quarto. Somewhere, maybe quarto is removing the divs. We can see the Div elements in the AST when we explore panflute in the sections below."
  },
  {
    "objectID": "notes/pandoc/filters.html#how-to-use-panflute",
    "href": "notes/pandoc/filters.html#how-to-use-panflute",
    "title": "pandoc filters",
    "section": "How to use panflute",
    "text": "How to use panflute\nThe examples are helpful.\nThis filter places CodeOutput blocks around code as well as changes the codefence to have file=script.py in order to hack the code fence.\n#!/Users/hamel/opt/anaconda3/bin/python\n#flute.py\nfrom typing import Text\nfrom panflute import *\nfrom logging import warning\n\n\ndef increase_header_level(elem, doc):\n    if type(elem) == CodeBlock and type(elem.parent.prev) == CodeBlock:\n        return ([RawBlock(\"<CodeOutput>\"), elem, RawBlock(\"</CodeOutput>\")])\n    elif type(elem) == CodeBlock:\n        elem.classes = ['file=script.py']\n\n\ndef main(doc=None):\n    return run_filter(increase_header_level, doc=doc)\n\n\nif __name__ == \"__main__\":\n    main()\nThis is how we can use this filter and see the rendered output:\n$ pandoc --to gfm minimal.ipynb --filter \"flute.py\"\n<div class=\"cell markdown\">\n\n## A minimal notebook\n\n</div>\n\n<div class=\"cell markdown\">\n\n<MyTag></MyTag>\n\n</div>\n\n<div class=\"cell code\" execution_count=\"1\">\n\n``` file=script.py\n# Do some arithmetic\nprint(1+1)\n```\n\n<div class=\"output stream stdout\">\n\n<CodeOutput>\n\n    2\n\n</CodeOutput>\n\n</div>\n\n</div>\nNote: we could probably replace the inner div with the output class with <CodeOutput> tag\nJust for completeness, this is the schema of the minimal notebook using the --to native flag prior to applying the filter:\n$pandoc --to native minimal.ipynb\n[ Div\n    ( \"\" , [ \"cell\" , \"markdown\" ] , [] )\n    [ Header\n        2\n        ( \"a-minimal-notebook\" , [] , [] )\n        [ Str \"A\" , Space , Str \"minimal\" , Space , Str \"notebook\" ]\n    ]\n, Div\n    ( \"\" , [ \"cell\" , \"markdown\" ] , [] )\n    [ Para\n        [ RawInline (Format \"html\") \"<MyTag>\"\n        , RawInline (Format \"html\") \"</MyTag>\"\n        ]\n    ]\n, Div\n    ( \"\"\n    , [ \"cell\" , \"code\" ]\n    , [ ( \"execution_count\" , \"1\" ) ]\n    )\n    [ CodeBlock\n        ( \"\" , [ \"python\" ] , [] )\n        \"# Do some arithmetic\\nprint(1+1)\"\n    , Div\n        ( \"\" , [ \"output\" , \"stream\" , \"stdout\" ] , [] )\n        [ CodeBlock ( \"\" , [] , [] ) \"2\\n\" ]\n    ]\n]\nAnd after applying the filter:\n$pandoc --to native minimal.ipynb --filter flute.py\n[ Div\n    ( \"\" , [ \"cell\" , \"markdown\" ] , [] )\n    [ Header\n        2\n        ( \"a-minimal-notebook\" , [] , [] )\n        [ Str \"A\" , Space , Str \"minimal\" , Space , Str \"notebook\" ]\n    ]\n, Div\n    ( \"\" , [ \"cell\" , \"markdown\" ] , [] )\n    [ Para\n        [ RawInline (Format \"html\") \"<MyTag>\"\n        , RawInline (Format \"html\") \"</MyTag>\"\n        ]\n    ]\n, Div\n    ( \"\"\n    , [ \"cell\" , \"code\" ]\n    , [ ( \"execution_count\" , \"1\" ) ]\n    )\n    [ CodeBlock\n        ( \"\" , [ \"file=script.py\" ] , [] )\n        \"# Do some arithmetic\\nprint(1+1)\"\n    , Div\n        ( \"\" , [ \"output\" , \"stream\" , \"stdout\" ] , [] )\n        [ RawBlock (Format \"html\") \"<CodeOutput>\"\n        , CodeBlock ( \"\" , [] , [] ) \"2\\n\"\n        , RawBlock (Format \"html\") \"</CodeOutput>\"\n        ]\n    ]\n]"
  },
  {
    "objectID": "notes/docker/index.html",
    "href": "notes/docker/index.html",
    "title": "Docker",
    "section": "",
    "text": "Notes from the book Docker In Action\nimport TOCInline from ‚Äò@theme/TOCInline‚Äô;\n;\n\nChapter 1\n\nDocker containers are faster than VMs to start, partly because they do NOT offer any hardware virtualization.\n\nVMs provide hardware abstractions so you can run operating systems.\n\nDocker uses Linux namespaces and cgropus\n\nHamel: I don‚Äôt know what this is\n\n\n\n\nChapter 2\n\nGetting help:\n\ndocker help cp\ndocker help run\n\nLinking containers: docker run --link\n\nthis is apparently deprecated per the docs\nOpens a secure tunnel between two containers automatically\nAlso exposes environment variables and other things (see the docs)\n\ndocker cp copy files from a container to local filesystem\nDetach an interactive container:\n\nHold down Control and press P then Q\n\nGet logs docker logs <container name>\n\nHamel: This is like kubectl logs\n\nRun a new command in a running container docker exec\n\ndocker exec <container_name> ps will run the ps command and emit that to stdout\n\nRename a container with docker rename <current_name> <new_name>\ndocker exec run additional processes in an already running container\ndocker create is the same as docker run except that the container is created in a stopped state.\ndocker run --read-only allows you to run a container in a read only state, which you only need to do in special circumstances (you probably never need to use this). You can make exceptions to the read only constraint with the -v flag:\n\n\n\nOverride the entrypoint using the --entrypoint flag (this is discussed in part 2 of the book).\n\n\n\nInjecting environment variables\nWith the --env or -e flags.\nA nice trick to see all the environment variables in a docker container is to use the Unix command env\n\nSetting multiple environment variables: use \\ for multiline like this:\ndocker create \\\n  --env WORDPRESS_DB_HOST=<my database hostname> \\\n  --env WORDPRESS_DB_USER=site_admin \\\n  --env WORDPRESS_DB_PASSWORD=MeowMix42 \\ \nwordpress:4\n\n\nAutomatically restarting containers\nDocker uses an exponential backoff strategy - double the previous time waiting until restarting.\ndocker run -d --restart always ...\nSee these restart policies\n\nno\non-failure[:max-retries]\nalways\nunless-stopped\n\n\n\nRemoving containers vs.¬†images\nContainers are the actual instantiation of an image, just like how an object is an instantion of an instance of a class.\ndocker rm: remove a container docker rmi: remove an image\n\n\n\nChapter 3\n\nTwo ways to publish an image\n\nBuild locally, push image to registry\nMake a Dockerfile and use DockerHub‚Äôs build system. This is preferred and considered to be safer, and DockerHub will mark your image as trusted if you do this because it is the only way to provide transparency to what is in your image.\n\nSearch dockerhub by keyword , sorted descending by stars\n\ndocker search <keyword>\nexample: docker search postgres\n\nUsing Alternative registries\n\ndocker pull quay.io/dockerinaction/ch3_hello_registry:latest\n\n\n\nImages as files\nYou can transport, save and load images as files! (You don‚Äôt have to push them to a registry).\n\nYou can then load the image:\ndocker load -i myfile.tar\n\n\n\nChapter 4 Persistent Storage &. Shared State with Volumes\n-v and --volume are aliases\n--volumes-from=\"<container-name>\" Mount all volumes from the given container\n\nDifferent kind of Volumes\n\nBind mount - this is what you always use\nDocker managed volume (2 kinds)\n\nAnonymous\nNamed volume (a special case of Anonymous)\n\n\nUse volumes | Docker Documentation - Named vs.¬†Anonymous volumes: article - Hamel: maybe? You might use named volumes to persist data between containers.\n\n\nTo persist data with named volumes\nNamed volume is a kind of anonymous volume where the mount point is managed by Docker. Example of how you used a named volume:\n\nStart container with a named volume: docker run --name myDatabaseWithVolume -v appdata:/var/lib/mysql  mysql save a table in the mysql database\nStart a new container with the same named volume docker run --name myDatabaseWithVolume2 -v appdata:/var/lib/mysql mysql You should be able to see the same table you created in the last container b/c data has been persisted.\n\n\n\nSee where Docker anonymous volumes store information\nUnlike a bind mount, where you explicitly name the host location, docker will manage the storage location of anonymous volumes. But how do you know where the files are stored on the host?\nYou can use docker inspect command filtered for the Volumes key to find the storage location on the host.\nCreate a container with an anonymous volume. docker run -v /some/location --name cass-shared alpine\ndocker inspect -f \"{{json .Volumes}}\" cass-shared\nThis will output a json blob which will show the mount points.\n\n\nOther things you didn‚Äôt know about volumes\n\nwhen you mount a volume, it overrides any files already at that location\n\nYou can mount specific files which avoid this\nif you specify a host directory that doesn‚Äôt exist Docker will create it for you\n\nexception: If you are mounting a file instead of a directory and it doesn‚Äôt exist on the host, Docker will throw an error\n\n\nyou can mount a volume as read only -v /source:/destination:ro\n\nsee docs (there is this optional third argument for volumes)\n\n\n\n\nThe volumes-from flag\nAllows you to share volumes with another container. When you use this flag, the same volumes are mounted into your container at the same location.\n\nVolumes are copied transitively, so this will automatically mount volumes that are also mounted this way from another container.\nCaveats - You cannot mount a shared volume to a different location within a container. This is a limitation of --volumes-from - If you have a collision in the destination mount point among several volumes-from only one volume will survive, which you can ascertain from docker inpsect - see above for how to use docker inspect - You cannot change the write permission of the volume, you inherit whatever the permission is in the source container.\n\n\nCleaning up volumes\n-v flag\ndocker rm -v will delete any managed volumes referenced by the target container\nHowever, if you delete all containers but forget a -v flag you will be left with an orphaned volume. This is bad b/c it takes up disk space until cleaned up. You have to run complicated cleanup steps to get rid of orphans.\nSolution: There is none, its a good habit to use -v anytime you call docker rm\nHamel: this means that- - Don‚Äôt use managed volumes unless you really need it - If you do use them, try to include makefiles that include -v as a part of things\n\n\nAdvanced Volume Stuff\n\nYou can have a volume container p.¬†72 so that you can reference --volume-from from all your containers.\n\nData-paced volume containers, you can pre-load volume containers with data p.¬†73\nYou can change the behavior of currently running containers by mounting configuration files and application in volumes. In a way, Hamel\n\n\n\n\nChapter 5 Single Host Networking\n\nTerminology:\n\nprotocols: tcp, http\ninterfaces: IP addresses\nports: you know what this means\n\nCustomary ports:\n\nHTTP: 80\nMySQL: 3306\nMemcached: 11211\n\n\n\n\nDiscuss advanced networking and creating a network using the docker network command. Hamel: I don‚Äôt see an immediate use for this.\n\nSpecial container networks:\n\nhost\n\ndocker run --network host allows you to pretend like the host is your local machine, and you can expose any port and that will bind to the host.\n\nnone\n\ndocker run --network none closes all connection to the outside world. This is useful for security.\n\n\n\n\nexposing ports\n-p 8080 This binds port 8080 to a random port on the host! you can find the port that the container is bound to by docker port <image name> example: docker run -p 8080 --name listener alpine docker port listener\nThis will give you output that looks like container --> host (which is reverse the other nomenclature of host:container\n-p 8080:8080 this binds the container‚Äôs port to the host‚Äôs port 8080\n-p 0.0.0.0:8080:8080/tcp same as above but specifies the interface and the tcp protocol.\nSyntax is -p <host-interface>:<host-port>:<target-port>/<protocol>\n\n\n\nChapter 6 Isolation\n\nLimit resources: Memory, CPU,\n\n-m or --memory\n\nnumber, where unit = b, k, m or g\nmemory limits are not reservations, just caps\n\n--cpu-shares\n\nis a weight you set that is used to calculate % of CPU usage allowed\n% is calculated as weight / (sum of all weights)\nonly enforced when there is contention for a CPU\n\n--cpuset-cpus : limits process to a specific CPU\n\ndocker run -d --cpuset-cpus 0 Restricts to CPU number 0\nCan specify a list or 0,1,2 or a range 0-2\n\n--device\n\nmount your webcam: docker run --device /dev/video0:/dev/video0\n\nShared memory : Hamel this was too advanced for me\n\n\n\nRunning as a user\n\nYou can only inspect the default run-as User by creating or pulling the image\n\nsee p.¬†113\n\nChange run-as user\n\ndocker run --user nobody\nThe user has to exist in the image when doing this or you will get an error. The user will not be created automatically for you.\nSee available users:\n\ndocker run --rm busybox:latest awk -F: '$0=$1' /etc/passwd\n\n\n\n\n\nPrivileged Containers: TRY NOT TO DO THIS\n\nThis is how you run Docker-in-Docker\nPriviliged containers have root privileges on the host.\n\n--privilged on docker create or docker run\n\n\n\n\nChapter 7 packaging software\nAside: cleaning up your docker environment\ndocker image prune -a and docker container prune\n\nRecovering changes to a stopped container\nI always thought you have to commit changes in order to preserve changes to an image you made in a container. This is not true (although committing changes is a good idea).\nAny changes you make to a container is saved even if the container is exited\nTo recover changes to a container\n\nFind the container (if you didn‚Äôt name it with docker run --name it will be named for you), using docker ps -a\nStart the container using docker start -ai <container_name> the -ai flags mean to attach and run interactively\nNow you are in the container you can verify that everything you installed is still there!\n\nNote: if you run your container initially with docker run --rm this automatically removes your container upon exit, so this might not be recommended as your changes are not recoverable if you forget to commit\n\n\n\nSeeing changes to a container from the base image\ndocker diff <container name> will output a long list of of file changes: - A: file added - D: file deleted - C: file changed\n\n\nOther tricks\nYou can override the entry point to the container permanently by using the --entrypoint flag: docker run --entrypoint\n\n\nUnderstanding Images & Layers\n\nfiles are stored in a Union file system, so they are stored in specific layers. The file system you are seeing as an end user are a union of all the layers. Each time a change is made to a union file system, that change is recorded on a new layer on top of all of the others. The ‚Äúunion‚Äù of all of those layers, or top-down view, is what the container (and user) sees when accessing the file system.\n\nThis means if you are not careful you can bloat the file system by making a bunch of unnecessary changes to add/delete files.\n\ndocker commit commits the top-layer changes to an image, meaning all the files changes are saved.\n\nSee image size with\ndocker images. Even though you remove a file, the image size will increase! This is because of the Union File System\nSee size of all layers\ndocker history <image name>\nflatten an image This is kind of complicated, you can do this by exporting and importing the filesystem into a base image See pg. 140. BUT there is an experimental feature called docker build --squash -t <image> .You can enable experimental features by following these instructions: dockerd Docker Documentation. For Mac, you can turn on experimental features by setting experimental: true in `settings> Command Line > enable experimental\n\n\n\nChapter 8 Build Automation\n\nuse .dockerignore to prevent certain files from being copied\nYou can set multiple environment variables at once in Dockerfile\nYou can use environment variables in the LABEL command\n\nThe metadata makes it clear that the environment variable substitution works. You can use this form of substitution in the ENV, ADD, COPY, WORKDIR, VOLUME, EXPOSE, and USER instructions.\n\n\nENV APPROOT \"/app\" APP \"mailer.sh\" VERSION \"0.6\"\nLABEL base.name \"Mailer Archetype\" base.version \"${VERSION}\"\n\nview metadata using the command docker inspect <image name>\n\n\nENTRYPOINT something arugment vs.¬†ENTRYPOINT [‚Äúsomething‚Äù, ‚Äúargument‚Äù]\nTLDR; use the ugly list approach\nThere are two instruction forms shell form and exec form docker - Dockerfile CMD shell versus exec form - Stack Overflow\nThe ENTRYPOINT instruction has two forms: the shell form and an exec form. The shell form looks like a shell command with whitespace-delimited arguments. The exec form is a string array where the first value is the command to execute and the remaining values are arguments. .\nMost importantly, if the shell form is used for ENTRYPOINT, then all other arguments provided by the CMD instruction or at runtime as extra arguments to docker run will be ignored. This makes the shell form of ENTRYPOINT less flexible.\nOther commands can use the exec form too! You must use the exec form when any of the arguments contain a whitespace:\nFROM dockerinaction/mailer-base:0.6 \nCOPY [\"./log-impl\", \"${APPROOT}\"] \nRUN chmod a+x ${APPROOT}/${APP} && \\ chown example:example /var/log \nUSER example:example \nVOLUME [\"/var/log\"]  # each value in this array will be created as a new volume definition\nCMD [\"/var/log/mailer.log\"]\nNote: you usually don‚Äôt want to specify a volume at build time.\n\n\nCMD vs.¬†ENTRYPOINT (You should really try to always use both!)\nCMD is actually an argument list for the ENTRYPOINT.\n\nLogically when you run a container it runs as <default shell program> ENTRYPOINT CMD\nYou can override the ENTRYPOINT with docker run --entrypoint, and you can override commands by just passing commands to docker run : docker run <image name> <command>\n\nFROM ubuntu\n\nENTRYPOINT [ \"ls\" ]\nCMD [\"-lah\"]\nAs you can see using ENTRYPOINT as well as CMD separately provides your downstream users with the most flexibility.\n\n\nCOPY vs ADD\nUse COPY. ADD has additional functionality like ability to download from urls and decompress files, which proved opaque over time and you shouldn‚Äôt use it.\n\n\nONBUILD\nThe ONBUILD instruction defines instructions to execute if the resulting image is used as a base for another build. those ONBUILD instructions are executed after the FROM instruction and before the next instruction in a Dockerfile.\nFROM busybox:latest \nWORKDIR /app RUN touch /app/base-evidence \nONBUILD RUN ls -al /app\n\n\nOther Stuff\n\nYou should always validate the presence of required environment variables in a startup shell script like entrypoint.sh\n\n\n\nDocker Digests\nReference the exact SHA of a Container which is the only way to guarantee the image you are referencing has not changed. @ symbol followed by the digest.\nHamel: doesn‚Äôt look like a good way to find history of digests, but you can see the current SHA when you use docker pull , you can see the SHA as well if you call docker images --digests\nFROM debian@sha256:d5e87cfcb730...\n\n\n\nChapter 10 (skipped Ch 9)\n\nYou can run your own customized registry. Simplest version can be hosted from a Docker Container!\n\n# start a local registry on port 5000\ndocker run -d --name personal_registry\n \\ -p 5000:5000 --restart=always \n \\ registry:2\n\n# push an image to the registry (using the same image that created the registry for convenience)\ndocker tag registry:2 localhost:5000/distribution:2 \ndocker push localhost:5000/distribution:2\nNote that docker push syntax is actually docker push <registry url>/org/repo\nThis chapter discusses many more things which are skipped: - Centralized registries - Enhancements - Durable blog storage - Integrating through notifications\n\n\nChapter 11 Docker Compose\nDocker compose for fastpages:\nversion: \"3\"\nservices:\n  fastpages: &fastpages\n    working_dir: /data\n    environment:\n        - INPUT_BOOL_SAVE_MARKDOWN=false\n    build:\n      context: ./_action_files\n      dockerfile: ./Dockerfile\n    image: fastpages-dev\n    logging:\n      driver: json-file\n      options:\n        max-size: 50m\n    stdin_open: true\n    tty: true\n    volumes:\n      - .:/data/\n\n  converter:\n    <<: *fastpages\n    command: /fastpages/action_entrypoint.sh\n\n  watcher:\n    <<: *fastpages\n    command: watchmedo shell-command --command /fastpages/action_entrypoint.sh --pattern *.ipynb --recursive --drop\n\n  jekyll:\n    working_dir: /data\n    image: hamelsmu/fastpages-jekyll\n    restart: unless-stopped\n    ports:\n      - \"4000:4000\"\n    volumes:\n      - .:/data/\n    command: >\n     bash -c \"gem install bundler\n     && jekyll serve --trace --strict_front_matter\"\nThe above uses YAML anchors: YAML anchors - Atlassian Documentation\nStart a particular service: docker-compose up <service name> Rebuild a service docker-compose build <service name>\nYou can express dependencies with depends_on which is useful for compose to know which services to restart or start in a specified order.\nSee examples of Docker Compose files on p 243\n\nScaling Up w/Docker Compose\nThat‚Äôs right you don‚Äôt need docker swarm. This example uses ch11_coffee_api/docker-compose.yml at master ¬∑ dockerinaction/ch11_coffee_api ¬∑ GitHub\n\nGet list of containers that are currently providing the service.\n\ndocker-compose ps coffee\n          Name                 Command       State            Ports\n----------------------------------------------------------------------------\nch11_coffee_api_coffee_1   ./entrypoint.sh   Up      0.0.0.0:32768->3000/tcp\n\nScale it up with docker-compose up --scale\n\ndocker-compose up --scale coffee=5\nWhen you run docker-compose ps coffee:\ndocker-compose ps coffee                                                                                                                        ÓÇ≤ ‚úî\n          Name                 Command       State            Ports\n----------------------------------------------------------------------------\nch11_coffee_api_coffee_1   ./entrypoint.sh   Up      0.0.0.0:32768->3000/tcp\nch11_coffee_api_coffee_2   ./entrypoint.sh   Up      0.0.0.0:32769->3000/tcp\nch11_coffee_api_coffee_3   ./entrypoint.sh   Up      0.0.0.0:32771->3000/tcp\nch11_coffee_api_coffee_4   ./entrypoint.sh   Up      0.0.0.0:32770->3000/tcp\nch11_coffee_api_coffee_5   ./entrypoint.sh   Up      0.0.0.0:32772->3000/tcp\nNote that the coffee service binds to port 0 on your host, which is an ephemeral port, which just means that your host machine assigns the service to a random port. This is required if you plan on using docker compose up --scale\nThe service was bound to port 0 on the host with\ncoffee:\n  build: ./coffee\n  user: 777:777\n  restart: always\n  expose:\n    - 3000\n  ports:\n    - \"0:3000\"\n...\n\nLoad balancer\n\nProblem with this kind of scaling is you don‚Äôt know the ports in advance , and you don‚Äôt want to hit these individual endpoints, you need a load balancer. This blog post shows you how to luse NGINX as a load balancer.\nYou will need something like this in your compose file\n  nginx:\n    image: nginx:latest\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf:ro\n    depends_on:\n      - pspdfkit\n    ports:\n      - \"4000:4000\"\n\n\nTemplating Docker Compose Files\nYou can read about this here: Share Compose configurations between files and projects | Docker Documentation, allows you to override certain things from a base compose file.\n\n\n\nChapter 12 Clusters w/Machine & Swarm\nHamel: I skipped this completely"
  },
  {
    "objectID": "notes/k8s/14-RolloutsRollbacks.html",
    "href": "notes/k8s/14-RolloutsRollbacks.html",
    "title": "Hamel's Blog",
    "section": "",
    "text": "Rollouts happen when you create a deployment or update a podspec.\nOnly triggered by change to the podspec, not other changes to a Deployment You can see rollout history like this\nkl rollout history deploy/vweb\nYou can get details of a revision with the revision flag:\n% kl rollout history deploy/vweb                                                                                             \ndeployment.apps/vweb\nREVISION  CHANGE-CAUSE\n1         <none>\n2         kubectl apply --filename=vweb/update/vweb-v11.yaml --record=true\n\n% kl rollout history deploy/vweb  --revision=1                                                                               \ndeployment.apps/vweb with revision #1\nPod Template:\n  Labels:   app=vweb\n    pod-template-hash=6ddb844d69\n    version=v1\n  Containers:\n   web:\n    Image:  kiamol/ch09-vweb:v1\n    Port:   80/TCP\n    Host Port:  0/TCP\n    Environment:    <none>\n    Mounts: <none>\n  Volumes:  <none>\nIt‚Äôs helpful to include informational labels with version numbers."
  },
  {
    "objectID": "notes/k8s/14-RolloutsRollbacks.html#to-a-specific-version",
    "href": "notes/k8s/14-RolloutsRollbacks.html#to-a-specific-version",
    "title": "Hamel's Blog",
    "section": "To a specific version",
    "text": "To a specific version\nYou can rollback to a specific revision\nkubectl rollout undo deploy/vweb --to-revision=2"
  },
  {
    "objectID": "notes/k8s/18-Developer.html",
    "href": "notes/k8s/18-Developer.html",
    "title": "Developer tips",
    "section": "",
    "text": "These notes provide tips on the developer workflow while using K8s. Some people use Docker compose to work with things locally, however you can also run a Kubernetes cluster locally."
  },
  {
    "objectID": "notes/k8s/18-Developer.html#use-the-ifnotpresent-imagepullpolicy",
    "href": "notes/k8s/18-Developer.html#use-the-ifnotpresent-imagepullpolicy",
    "title": "Developer tips",
    "section": "Use the IfNotPresent imagePullPolicy",
    "text": "Use the IfNotPresent imagePullPolicy\nK8s have tricky rules for which container images are used (local vs from repo).\nIf the image doesn‚Äôt have an explict tag in the name (and therefore uses the implicit :latest tag), then K8s will always pull the image first. Otherwise, K8s will use the local image if it exists in the image cache on the node.\nYou can override this behavior by specifying an image pull policy. When developing locally, you want to use the IfNotPresent policy.\nspec:                         # This is the Pod spec within the Deployment.\n containers:\n   - name: bulletin-board\n     image: kiamol/ch11-bulletin-board:dev \n     imagePullPolicy: IfNotPresent   # Prefer the local image if it exists\nIf you forget to do this, it can be very confusing as to why your image doesn‚Äôt seem to be updated!"
  },
  {
    "objectID": "notes/k8s/18-Developer.html#use-namespaces",
    "href": "notes/k8s/18-Developer.html#use-namespaces",
    "title": "Developer tips",
    "section": "Use namespaces",
    "text": "Use namespaces\nYou can use namespaces to test apps on the cluster. For example, a production and a test namespace.\nDeploy with a namespace using the --namespace flag:\n# create a new namespace:\nkubectl create namespace kiamol-ch11-test\n\n# deploy a sleep Pod in the new namespace:\nkubectl apply -f sleep.yaml --namespace kiamol-ch11-test\n\n# list sleep Pods--this won‚Äôt return anything:\nkubectl get pods -l app=sleep\n\n# now list the Pods in the namespace:\nkubectl get pods -l app=sleep -n kiamol-ch11-test\nObjects within a namespace are isolated, so you can deploy the same apps with the same object names in different namespaces.\n\nSetting the namespace in YAML\nFirst create the namespace and then assign the deployment to a namespace.\napiVersion: v1\nkind: Namespace      # Namespace specs need only a name.\nmetadata:\n name: kiamol-ch11-uat\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:                       # The target namespace is part of the \n name: sleep                   # object metadata. The namespace needs\n namespace: kiamol-ch11-uat    # to exist, or the deployment fails.    \n\n  # The Pod spec follows.\n\n\nSee resources in all namespaces with --all namespaces\n# create the namespace and Deployment:\nkubectl apply -f sleep-uat.yaml\n\n# list the sleep Deployments in all namespaces:\nkubectl get deploy -l app=sleep --all-namespaces\n\n# delete the new UAT namespace:\nkubectl delete namespace kiamol-ch11-uat\n\n# list Deployments again:\nkubectl get deploy -l app=sleep --all-namespaces\n\n\nDeleting namespace deletes all resources\nWhen you delete everything in a namespace, like with the above example, you also delete all the resources in the namespace.\nOften people will delete a namespace and re-create it, this will delete everything in the namespace. For example:\nkl delete namespace {namespace}\nkubectl create namespace {namespace}"
  },
  {
    "objectID": "notes/k8s/18-Developer.html#change-the-default-namespace",
    "href": "notes/k8s/18-Developer.html#change-the-default-namespace",
    "title": "Developer tips",
    "section": "Change the default namespace",
    "text": "Change the default namespace\nConstantly passing the --namespace flag is tedious. You can set the default namespace with kl config set-context:\n# list all contexts:\nkubectl config get-contexts\n\n# update the default namespace for the current context:\nkubectl config set-context --current --namespace=kiamol-ch11-test\n\n# list the Pods in the default namespace:\nkubectl get pods\nYou can also get the current context with:\nkl config current-context"
  },
  {
    "objectID": "notes/k8s/18-Developer.html#switching-between-clusters",
    "href": "notes/k8s/18-Developer.html#switching-between-clusters",
    "title": "Developer tips",
    "section": "Switching Between Clusters",
    "text": "Switching Between Clusters\nUse contexts to switch b/w clusters. Config files with contexts live at ~/.kube.\n\nReset the default namespace\nBelow shows you how to reset the default namespace. You can also set another context to a different namespace.\nIt‚Äôs always a good idea to check your config as well.\n# setting the namespace to blank resets the default:\nkubectl config set-context --current --namespace=\n\n# printing out the config file shows your cluster connection:\nkubectl config view\nWhat does Michal do to manage different clusters?"
  },
  {
    "objectID": "notes/k8s/18-Developer.html#private-images",
    "href": "notes/k8s/18-Developer.html#private-images",
    "title": "Developer tips",
    "section": "Private Images",
    "text": "Private Images\nKubernetes supports pulling private images by storing registry credentials in a special type of Secret object named docker-registry.\n % kl create secret --help                                                                             \nCreate a secret using specified subcommand.\n\nAvailable Commands:\n  docker-registry   Create a secret for use with a Docker registry\n  generic           Create a secret from a local file, directory, or literal value\n  tls               Create a TLS secret\nYou can set the secret like this, where we create a docker-registry secret called registry-creds\n# create the Secret using the details from the script:\nkubectl create secret docker-registry registry-creds \n   --docker-server=$REGISTRY_SERVER\n   --docker-username=$REGISTRY_USER\n   --docker-password=$REGISTRY_PASSWORD\n\n# show the Secret details:\nkubectl get secret registry-creds\nThis docker secret is mounted into the container like so:\nyaml title=\"bb-deployment.yaml\"     spec:       containers:         - name: bulletin-board           image: {{ .Values.registryServer }}/{{ .Values.registryUser }}/bulletin-board:{{ .Values.imageBuildNumber }}-kiamol            imagePullPolicy: Always               ports:             - name: http               containerPort: 8080         imagePullSecrets:       - name: {{ .Values.registrySecretName }}\nWhere the Helm values are configured like so:\nyaml title=\"values.yaml\" # port for the Service to listen on servicePort: 8012 # type of the Service: serviceType: LoadBalancer # domain of the registry server - e.g docker.io for Docker Hub registryServer: docker.io # user portion of the image repostory: registryUser: kiamol # build number portion of the image tag: imageBuildNumber: dev # name of the Secret containing registry credentials: registrySecretName: registry-creds"
  },
  {
    "objectID": "notes/k8s/18-Developer.html#local-setup",
    "href": "notes/k8s/18-Developer.html#local-setup",
    "title": "Developer tips",
    "section": "Local Setup",
    "text": "Local Setup\nTry to encapsulate the CI process into a script that you run locally, that also includes a local version of K8s if possible, where you:\n\nBuild container images\nSpin everything up in a local K8s cluster\nRun/test the app\n\nThis won‚Äôt work all the time. You can also develop without containers, and setup GitHub Actions to do the container builds, tests, and deploy K8s in a test namespace."
  },
  {
    "objectID": "notes/k8s/19-Pod-Lifecycle.html",
    "href": "notes/k8s/19-Pod-Lifecycle.html",
    "title": "Pod restart vs.¬†replacement",
    "section": "",
    "text": "If you google Pod restart vs replacement, virutally every article conflates the two, but the distinction is very important!\nA good way to test if some event causes a restart vs a replacment is to see if the UID for the pod remains the same or not before vs.¬†after the event:\nA pod with the same UID is guaranteed to be running on the same node, since it has only been restarted.\nThis article on Pod lifecycle is helpful."
  },
  {
    "objectID": "notes/k8s/19-Pod-Lifecycle.html#what-causes-a-restart-vs-replacement",
    "href": "notes/k8s/19-Pod-Lifecycle.html#what-causes-a-restart-vs-replacement",
    "title": "Pod restart vs.¬†replacement",
    "section": "What causes a restart vs replacement",
    "text": "What causes a restart vs replacement\n\nrestart:\n\nfailed liveliness probe (I confirmed with the UID that this restarts the Pod).\nWhen a container exits the pod will be restarted according to the restartPolicy in the podspec.\n\nreplacement:\n\nkubectl rollout restart Yes! It replaces the pod, I checked and the UID changes! Don‚Äôt get foooled by the word ‚Äúrestart‚Äù\ndeleting the resource (ex: kubectl delete deploy/...)\nscaling the resource to zero (ex: kubectl scale deployment ...)\nIf you change the podspec.\n\n\nIf unsure do some experiments!"
  },
  {
    "objectID": "notes/k8s/19-Pod-Lifecycle.html#forcing-a-container-to-exit",
    "href": "notes/k8s/19-Pod-Lifecycle.html#forcing-a-container-to-exit",
    "title": "Pod restart vs.¬†replacement",
    "section": "Forcing a container to exit",
    "text": "Forcing a container to exit\nYou can force a container to exit with the following command. This might be useful for testing:\nkl exec -it {pod name} -- killall5\nThis will cause the pod to restart the container, not replace it."
  },
  {
    "objectID": "notes/k8s/19-Pod-Lifecycle.html#storage-implications",
    "href": "notes/k8s/19-Pod-Lifecycle.html#storage-implications",
    "title": "Pod restart vs.¬†replacement",
    "section": "Storage Implications",
    "text": "Storage Implications\nStorage that exists at the Pod-level, like emptyDir will survive a Pod restart, but NOT a pod replacement:\n...\nspec:\n containers:\n   - name: myimage\n     image: repo/image\n     volumeMounts:\n      - name: data                 # Mounts a volume called data\n         mountPath: /data          # into the /data directory\n volumes:\n   - name: data                    # This is the data volume spec,\n     emptyDir: {}                  # which is the EmptyDir type.\nAny data stored in an EmptyDir volume remains in the Pod between restarts, so Pod‚Äôs that are restarted can access data written by their predecessors. An EmptyDir volume can be a reasonable source for a local cache because if the app crashes, then the replacement container will still have the cached files."
  },
  {
    "objectID": "notes/k8s/99-Random.html",
    "href": "notes/k8s/99-Random.html",
    "title": "Random TILs",
    "section": "",
    "text": "--show labels will show you all the labels!"
  },
  {
    "objectID": "notes/k8s/13-JobsCron.html",
    "href": "notes/k8s/13-JobsCron.html",
    "title": "Hamel's Blog",
    "section": "",
    "text": "Thes are useful! You can run ad-hoc jobs to completion, or schedule something to run! Lots of DS workloads are like this.\n\nJobs aren‚Äôt just for stateful apps; they‚Äôre a great way to bring a standard approach to any batch-processing problems, where you can hand off all the scheduling and monitoring and retry logic to the cluster. You can run any container image in the Pod for a Job, but it should start a process that ends; otherwise, your jobs will keep running forever.\n\napiVersion: batch/v1\nkind: Job                           # Job is the object type.\nmetadata:\n name: pi-job\nspec:\n template:\n   spec:                           # The standard Pod spec\n     containers:\n       - name: pi                  # The container should run and exit.\n         image: kiamol/ch05-pi     \n         command: [\"dotnet\", \"Pi.Web.dll\", \"-m\", \"console\", \"-dp\", \"50\"]\n     restartPolicy: Never          # If the container fails, replace the Pod.\n\n\nRun the above and get logs\nkl apply -f pi/pi-job.yaml  # this is the filename  for the above\nkl logs jobs/pi-job\n\n\n\nits like a Pod standard spec, but there is an additional required field restartPolicy.\n\n\n\ncompletions: how many times should the job run.\nparallelism: How many Pods to run in parallel with multiple completions set.\n\n\n\n\n\nParallel Jobs with a work queue: - do not specify .spec.completions, default to .spec.parallelism. - the Pods must coordinate amongst themselves or an external service to determine what each should work on. For example, a Pod might fetch a batch of up to N items from the work queue. - each Pod is independently capable of determining whether or not all its peers are done, and thus that the entire Job is done. - when any Pod from the Job terminates with success, no new Pods are created. - once at least one Pod has terminated with success and all Pods are terminated, then the Job is completed with success. - once any Pod has exited with success, no other Pod should still be doing any work for this task or writing any output. They should all be in the process of exiting.\n\nhttps://kubernetes.io/docs/concepts/workloads/controllers/job/\nArgo is basically a wrapper on Jobs."
  },
  {
    "objectID": "notes/k8s/13-JobsCron.html#cronjob",
    "href": "notes/k8s/13-JobsCron.html#cronjob",
    "title": "Hamel's Blog",
    "section": "CronJob",
    "text": "CronJob\nJust adds a few lines to the Job YAML:\napiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n name: todo-db-backup\nspec:\n schedule: \"*/2 * * * *\"          # Creates a Job every 2 minutes\n concurrencyPolicy: Forbid        # Prevents overlap so a new Job won‚Äôt be\n jobTemplate:                     # created if the previous one is running\n   spec:\n     # job template...\nhttps://kubernetes.io/docs/reference/generated/kubernetes-api/v1.22/#cronjobspec-v1-batch\n\nCronJob Cleanup\n\nCronJobs don‚Äôt perform an automatic cleanup for Pods and Jobs.\n\nCronJobs don‚Äôt follow the standard controller model, with a label selector to identify the Jobs it owns. You can add your own labels in the Job template for the CronJob, but if you don‚Äôt do that, you need to identify Jobs where the owner reference is the CronJob\n\n\nTLDR; Clean up lingering pods afer you are done, and organize everything with labels!\n\n\nPausing CronJobs\n\nYou can also move CronJobs to a suspended state, which means the object spec still exists in the cluster, but it doesn‚Äôt run until the CronJob is activated again"
  },
  {
    "objectID": "notes/k8s/Open Questions.html",
    "href": "notes/k8s/Open Questions.html",
    "title": "Hamel's Blog",
    "section": "",
    "text": "What is kl wait --for=condition=Ready pod -l app=something ? Is it possible to customize the Ready condition?\n\nUnderstand how nginx works\n\nWTF is going on in Chapter 5 w/proxy setup and caching\n\nUnderstand how caddy works\n\nWhen you are consuming a queue with a Job and completions, can you end the job when the queue is exhausted?"
  },
  {
    "objectID": "notes/k8s/20-health-checks.html",
    "href": "notes/k8s/20-health-checks.html",
    "title": "Probes",
    "section": "",
    "text": "This is Chapter 12."
  },
  {
    "objectID": "notes/k8s/20-health-checks.html#testing-liveness-probe",
    "href": "notes/k8s/20-health-checks.html#testing-liveness-probe",
    "title": "Probes",
    "section": "Testing Liveness Probe",
    "text": "Testing Liveness Probe\nThis is a clever way of testing the livenessProbe:\nspec:\n  containers:\n  - name: liveness\n    image: repo/name\n    args:\n    - /bin/sh\n    - -c\n    - touch /tmp/healthy; sleep 30; rm -f /tmp/healthy; sleep 600\n    livenessProbe:\n      exec:\n        command:\n        - cat\n        - /tmp/healthy\n      initialDelaySeconds: 5\n      periodSeconds: 5\nSource\nFailed liveness checks will cause a pod to restart, not to be replaced."
  },
  {
    "objectID": "notes/k8s/20-health-checks.html#forcing-a-container-to-exit",
    "href": "notes/k8s/20-health-checks.html#forcing-a-container-to-exit",
    "title": "Probes",
    "section": "Forcing a container to exit",
    "text": "Forcing a container to exit\nYou can force a container to exit with the following command. This might be useful for testing:\nkl exec -it {pod name} -- killall5\nThis will cause the pod to restart the container, not replace it."
  },
  {
    "objectID": "notes/k8s/DBs on K8s.html",
    "href": "notes/k8s/DBs on K8s.html",
    "title": "Hamel's Blog",
    "section": "",
    "text": "Don‚Äôt do it. Use a managed DB from your cloud provider instead."
  },
  {
    "objectID": "notes/k8s/12-StatefulSet.html",
    "href": "notes/k8s/12-StatefulSet.html",
    "title": "StatefulSet",
    "section": "",
    "text": "Hamel: you probably don‚Äôt need this. JUST SKIP THESE NOTES\n\n[[StatefulSet]] is a Pod controller, just like [[5. ReplicaSets]] or [[DaemonSets]]\nWhen you deploy a StatefulSet, it creates Pods with predictable names, which can be individually accessed over DNS, and starts them in order; the first Pod needs to be up and running before the second Pod is created.\nIf you are trying to model database on K8s, you might use StatefulSet. However, don‚Äôt put DBs on K8s - use a managed service for that instead. StatefulSet just gives you determinstic Pod names and networking, you have to take care of synching your apps yourself. That would be outside the scope of what DS should do IMO.\nHere is kind: StatefulSet\n % cat todo-list/db/todo-db.yaml                                                                                                      \napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: todo-db\n  labels:\n    kiamol: ch08\nspec:\n  selector:\n    matchLabels:\n      app: todo-db\n  serviceName: todo-db\n  replicas: 2\n  template:\n    metadata:\n      labels:\n        app: todo-db\nWhen you get pods, they will be incremented from 0, this allows you network/communicate with them deterministically.\n% kl get po                                                                                                                         \nNAME        READY   STATUS    RESTARTS   AGE\ntodo-db-0   1/1     Running   0          23s\ntodo-db-1   1/1     Running   0          21s\nStatefulSet is a controller, so if you delete a pod the StatefulSet will recreate it.\n\nInitContainers\nYou can bootstrap pods with initcontainers and stateful sets. For example.\napiVersion: apps/v1\nkind: StatefulSet\n...\n      initContainers:\n        - name: wait-service\n          image: kiamol/ch03-sleep\n          envFrom:\n          - configMapRef:\n              name: todo-db-env\n          command: ['/scripts/wait-service.sh']\nThe script says if its running in Pod 0 do nothing, but if its running in Pod 1 then replicate the master. This is just an idea, you probably should never do this yourself.\n\n\nNetworking In StatefulSets\nYou need to have a special configuration ‚Äúheadless Service‚Äù to setup newtworking for StatefulSets, by setting ClusterIP: None\n%cat todo-list/db/todo-db-service.yaml                                                                                              \napiVersion: v1\nkind: Service\nmetadata:\n  name: todo-db\n  labels:\n    kiamol: ch08\nspec:\n  ports:                  # 5432 is the port Postgres uses\n    - port: 5432\n      targetPort: 5432 \n      name: postgres\n  selector:\n    app: todo-db\n  clusterIP: None             # Note how this is None\nThe pod will be reachable at pod-name.service-name.cluster-domain-suffix. for example:\ntodo-db-0.todo-db.default.svc.cluster.local\nSee: https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/#stable-network-id\n\nA StatefulSet can use a Headless Service to control the domain of its Pods. The domain managed by this Service takes the form: \\((service name).\\)(namespace).svc.cluster.local, where ‚Äúcluster.local‚Äù is the cluster domain. As each Pod is created, it gets a matching DNS subdomain, taking the form: \\((podname).\\)(governing service domain), where the governing service is defined by the serviceName field on the StatefulSet.\n\nThis is also related to the serviceName field on the StatefulSet\nYou can lookup the cluster-domain-suffix like this :\nkl exec deploy/sleep -- sh -c 'nslookup todo-db'`\nThe headless service still does load balancing, but lets you access the Pod\n\n\nStorage\nFor DBs you want each pod to have its own persistent disk, there is a shortcut: using volumeClaimTemplates which makes sure each Pod in the stateful set always gets its own persistent volume.\n% cat sleep/sleep-with-pvc.yaml                                             \n...\nkind: StatefulSet\n  template:\n    ...\n  volumeClaimTemplates:\n  - metadata:\n      name: data\n      labels:\n        kiamol: ch08\n    spec:\n      accessModes:\n       - ReadWriteOnce\n      resources:\n        requests:\n          storage: 5Mi\nEach pod will get a PVC created dynamically, which will create a Persistent Volume using the default storage class (or the requested storage class if included in the spec).\nThe link b/w each pod and its PVC is maintained when pods are replaced. For example:\n# create a file\nkl exec sleep-with-pvc-0 -- sh -c 'echo pod 0 > /data/pod.txt'\n\n#delete the pod\nkl delete po sleep-with-pvc-0\n\n# this will show the right contents\nkl exec sleep-with-pvc-0 -- cat /data/pod.txt"
  },
  {
    "objectID": "notes/k8s/multi_container_pods/09-Ambassador Sidecars.html",
    "href": "notes/k8s/multi_container_pods/09-Ambassador Sidecars.html",
    "title": "Ambassador Sidecars",
    "section": "",
    "text": "[[Ambassador]]\nThese are [[Sidecar]] containers that act as proxys. You can do this for reliability or security. Proxy containers can do load balancing, retries, or encrypt items. [[service mesh]] uses patterns like this.\nFor example you may want to restrict what web requests or URLs your app is allowed to talk to. With an ambassador sidecar, you can block all traffic besides the allowed one. Here is an example:\n      containers:\n        - name: web\n          image: kiamol/ch03-numbers-web \n          env:\n          - name: http_proxy\n            value: http://localhost:1080\n          - name: RngApi__Url\n            value: http://localhost/api\n        - name: proxy                         # this is a basic proxy\n          image: kiamol/ch07-simple-proxy          \n          env:\n          - name: Proxy__Port                 #Routes network requets given \n            value: \"1080\"                     # the below mapping\n          - name: Proxy__Request__UriMap__Source\n            value: http://localhost/api\n          - name: Proxy__Request__UriMap__Target\n            value: http://numbers-api/sixeyed/kiamol/master/ch03/numbers/rng\nIn the above example, anything that is not in the mapping is blocked. Now the web app is restricted to a single address for outgoing requests, which are logged by the proxy.\nthe app container uses localhost addresses for any services it consumes, and it‚Äôs configured to route all network calls through the proxy container. The proxy is a custom app that logs network calls, maps localhost addresses to real addresses, and blocks any addresses that are not listed in the map. All that becomes functionality in the Pod, but it‚Äôs transparent to the application container.\nYou can also use Ambassador‚Äôs for database connections, to query read-only copies when there are no db updates/writes:"
  },
  {
    "objectID": "notes/k8s/multi_container_pods/08-Multi-Container-Pods.html",
    "href": "notes/k8s/multi_container_pods/08-Multi-Container-Pods.html",
    "title": "Multi-Container Pods",
    "section": "",
    "text": "Pods can run more than one container. Pods in a container share the same network and same IP address, so they must listen on different ports. Containers in a pod can communicate over local host. Each container has its own file system, but can mount from the Pod and can share info that way."
  },
  {
    "objectID": "notes/k8s/multi_container_pods/08-Multi-Container-Pods.html#accessing-containers-in-multi-container-pods",
    "href": "notes/k8s/multi_container_pods/08-Multi-Container-Pods.html#accessing-containers-in-multi-container-pods",
    "title": "Multi-Container Pods",
    "section": "Accessing containers in multi-container Pods",
    "text": "Accessing containers in multi-container Pods\nYou can use the -c flag, to narrow down the container\n% kl logs deploy/sleep -c file-reader\nSame thing is necessary for kl exec deploy/sleep ..., you would also add -c file-reader onto that."
  },
  {
    "objectID": "notes/k8s/multi_container_pods/08-Multi-Container-Pods.html#networking-sharing",
    "href": "notes/k8s/multi_container_pods/08-Multi-Container-Pods.html#networking-sharing",
    "title": "Multi-Container Pods",
    "section": "Networking Sharing",
    "text": "Networking Sharing\nTo demonstrate network sharing:\n% cat sleep/sleep-with-server.yaml                                                                                                   \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sleep\n  labels:\n    kiamol: ch07\nspec:\n  selector:\n    matchLabels:\n      app: sleep\n  template:\n    metadata:\n      labels:\n        app: sleep\n    spec:\n      containers:\n        - name: sleep\n          image: kiamol/ch03-sleep\n        - name: server\n          image: kiamol/ch03-sleep\n          command: ['sh', '-c', \"while true; do echo -e 'HTTP/1.1 200 OK\\nContent-Type: text/plain\\nContent-Length: 7\\n\\nkiamol' | nc -l -p 8080; done\"]\n          ports:\n            - containerPort: 8080    # this exposes a port to this container.\n              name: http\nWe can access the server container on local host from the sleep container:\nkl apply -f sleep/sleep-with-server.yaml\nkl exec deploy/sleep -c sleep -- wget -q -O - localhost:8080\n\nCreating A Serivce to Multi Container Pod\nYou just have to make sure that the port is routing to the correct place.\napiVersion: v1\nkind: Service\nmetadata:\n  name: sleep\nspec:\n  ports:\n    - port: 8020\n      targetPort: 8080\n  selector:\n    app: sleep\n  type: LoadBalancer\nNow from my lapto I can do this, which will allow me to access the container listening in the pod on port 8080\nwget -q -O - localhost:8020"
  },
  {
    "objectID": "notes/k8s/multi_container_pods/08-Multi-Container-Pods.html#when-to-use-multi-cotainer-pods",
    "href": "notes/k8s/multi_container_pods/08-Multi-Container-Pods.html#when-to-use-multi-cotainer-pods",
    "title": "Multi-Container Pods",
    "section": "When to use multi cotainer pods",
    "text": "When to use multi cotainer pods\nYou don‚Äôt want to usually shove different components of an application into a Pod together! Doing so will limit you, as you want to be able to scale/upgrade etc these different components independently.\nThere are two patterns:\n\n[[Sidecar]] runs alongside; pod isn‚Äôt considered ready until all the containers are ready. This is what is shown above.\n[Init containers] you can have multiple init containers, they run in sequence, in order they are specified. Each must complete sucessfully before next one starts, and all must complete sucessfully before the Pod containers start (if mulitple they are sidecars)\n\nInit containers are often used to generate data for container Pods (which is written to a shared mounted directory as previously shown). An example is an init container w/ the git command line installed that clones a repo to a shared file system. Another example is to write configuration files in a specific format that your app expects from env variables and config maps.\nThe below YAML shows the initContainer craeating the index.html file so the next imge can serve it.\n% cat sleep/sleep-with-html-server.yaml                                                                                              \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n...\nspec:\n  selector:\n    matchLabels:\n      app: sleep\n  template:\n...\n    spec:\n      initContainers:\n        - name: init-html\n          image: kiamol/ch03-sleep\n          command: ['sh', '-c', \"echo '<!DOCTYPE html><html><body><h1>KIAMOL Ch07</h1></body></html>' > /data/index.html\"]\n          volumeMounts:\n            - name: data\n              mountPath: /data\n      containers:\n        - name: sleep\n          image: kiamol/ch03-sleep\n        - name: server\n          image: kiamol/ch03-sleep\n          command: ['sh', '-c', 'while true; do echo -e \"HTTP/1.1 200 OK\\nContent-Type: text/html\\nContent-Length: 62\\n\\n$(cat /data-ro/index.html)\" | nc -l -p 8080; done']\n          ports:\n            - containerPort: 8080\n              name: http\n          volumeMounts:\n            - name: data\n              mountPath: /data-ro\n              readOnly: true\nHere is an example that writes a config file callled appsettings.json:\n...\n    spec:\n      initContainers:\n        - name: init-config\n          image: kiamol/ch03-sleep\n          command: ['sh', '-c', \"cat /config-in/appsettings.json | jq --arg APP_ENV \\\"$APP_ENVIRONMENT\\\" '.Application.Environment=$APP_ENV' > /config-out/appsettings.json\"]\n          env:\n          - name: APP_ENVIRONMENT\n            value: TEST\n          volumeMounts:\n            - name: config-map\n              mountPath: /config-in\n            - name: config-dir\n              mountPath: /config-out\n...\n      volumes:\n        - name: config-map     # this is a volume that is mounted as input\n          configMap:\n            name: timecheck-config\n        - name: config-dir     # files are written out here\n          emptyDir: {}"
  },
  {
    "objectID": "notes/k8s/multi_container_pods/10-Downsides of MC Pods.html",
    "href": "notes/k8s/multi_container_pods/10-Downsides of MC Pods.html",
    "title": "Hamel's Blog",
    "section": "",
    "text": "MC = multi container\nAdding sidecars and init containers adds to the failure modes for your application.\nYou might see ready = 0 if there is a container in a multi-container pod that is failing!\n\nRestart Conditions\n\nIf a Pod with init containers is replaced, then the new Pod runs all the init containers again. You must ensure your init logic can be run repeatedly.\nIf you deploy a change to the init container image(s) for a Pod, that restarts the Pod. Init containers all execute again, and app containers are replaced.\nIf you deploy a Pod spec change to the app container image(s), the app containers are replaced, but the init containers are not executed again.\nIf an application container exits, then the Pod re-creates it. Until the container is replaced, the Pod is not fully running and won‚Äôt receive Service traffic."
  },
  {
    "objectID": "notes/k8s/multi_container_pods/11-Sharing Processes in MC Pods.html",
    "href": "notes/k8s/multi_container_pods/11-Sharing Processes in MC Pods.html",
    "title": "Sharing Processes in MC Pods",
    "section": "",
    "text": "MC = multi container\nContainers isolate proceses, so containers cannot see eachothers processes. You can set Namespace: true to make processes visible amongst all containers in a pod:\n...\nspec:\n  ...\n  template:\n    ...\n    spec:\n      shareProcessNamespace: true\n      containers:\n      ...\nIf you enable this: - containers can kill eachother‚Äôs processes - enable interproces communication - fetch metrics about the app process"
  },
  {
    "objectID": "notes/k8s/index.html",
    "href": "notes/k8s/index.html",
    "title": "Why Should Data Scientists Learn Kubernetes?",
    "section": "",
    "text": "For the uninitiated Kubernetes, also known as K8s, is ‚Äúan open-source system for automating deployment, scaling, and management of containerized applications.‚Äù\nI agree with Chip Huyen that Data Scientists shouldn‚Äôt need to learn K8s. However, the cold truth is: Even though you shouldn‚Äôt have to, you should anyway! Even Vicki Boykis seems to agree1:\nBelow, I outline reasons why I think learning K8s is a good idea for data scientists2. But if you are already convinced, feel free to jump ahead and read about my free course: K8s For Data Scientists."
  },
  {
    "objectID": "notes/k8s/index.html#managed-services-are-not-always-an-option",
    "href": "notes/k8s/index.html#managed-services-are-not-always-an-option",
    "title": "Why Should Data Scientists Learn Kubernetes?",
    "section": "Managed services are not always an option",
    "text": "Managed services are not always an option\n\n\n\nA robot concierge helping a data scientist\n\n\nMajor clouds offer some data science infrastructure as managed services, but it usually doesn‚Äôt cover everything you need. There are many popular open source infrastructure tools which data scienists consider to be staples or must-haves in their workflow. For example, many clouds offer only limited or no managed services for things like:\n\nAirflow\nJupyterHub\nML workflow and experiment tracking systems. 3\nDask\netc.\n\nThere is often a gap between cloud offerings and the needs of data science teams, which are often filled by open-source tools. When open source isn‚Äôt enough, third party vendors are happy to install their software on your cloud. However, you often need basic infrastructure skills to enable either of these things. These skills often intersect with Kubernetes."
  },
  {
    "objectID": "notes/k8s/index.html#nobody-is-coming-to-save-you",
    "href": "notes/k8s/index.html#nobody-is-coming-to-save-you",
    "title": "Why Should Data Scientists Learn Kubernetes?",
    "section": "Nobody is coming to save you",
    "text": "Nobody is coming to save you\n\n\n\nA scientist dressed as a super hero\n\n\nA typical first experience as a data scientist or machine learning professional is that you don‚Äôt have the necessary tools to get started. This is incredibly frustrating, as making progress without the proper tools can be hard. This experience usually culminates in a conversation like this:\n\nData Scientist: I‚Äôm excited to join ACME company! You‚Äôve hired me to optimize marketing spend with predictive models. The issue is that we don‚Äôt have the basic infrastructure or tools are necessary for me to work efficiently.\nManager: I‚Äôm confused. Can‚Äôt you install the tools you need? Isn‚Äôt that what you are for? I was expecting that you would figure it out.\nData Scientist: No, I don‚Äôt know how to setup and deploy infrastructure. We need a special infrastructure or DevOps person for that.\nManager: It will be hard to ask for more resources if we don‚Äôt know the expected return on investment. Can you do the ML project first, demonstrate some value, and then we can invest in infrastructure?\nData Scientist: I need some minimum tools to allow me to more quickly experiment and develop proof of concepts. Also, I need tools that might help me collaborate better with my team‚Ä¶\n‚Ä¶ At this point, the Manager and the Data Scientist are stuck debating the chicken and the egg problem. This impasse leads to career dissatisfaction and stagnation.\n\nBy learning basic infra skills, you can unblock yourself and your team in many cases!"
  },
  {
    "objectID": "notes/k8s/index.html#helm-charts-are-like-an-oss-app-store",
    "href": "notes/k8s/index.html#helm-charts-are-like-an-oss-app-store",
    "title": "Why Should Data Scientists Learn Kubernetes?",
    "section": "Helm Charts are like an OSS app store",
    "text": "Helm Charts are like an OSS app store\n\n\n\nApp store, abstract\n\n\nHelm is a package mansger for K8s. It allows you to install a wide variety of software in the cloud quickly. Sure, you can install software on your laptop, but the whole point of many data science tools is to enable better collaboration with your team. To properly evaluate these tools, you will want to install them in an environment that your whole team can observe and interact with.\nYou can install many data science tools with Helm:\n\nAirflow: https://airflow.apache.org/docs/helm-chart/stable/index.html\nDask: https://docs.dask.org/en/stable/deploying-kubernetes-helm.html\nJuptyerHub: https://z2jh.jupyter.org/en/stable/jupyterhub/installation.html\nMetaflow: https://github.com/outerbounds/metaflow-tools#metaflow-services-on-kubernetes-k8s\nPrefect: https://github.com/PrefectHQ/prefect-helm\nElasaticSearch: https://github.com/elastic/helm-charts\n\nYou don‚Äôt need to become an expert in K8s to deploy and perform basic configuration of these tools. You might need deeper exercise to maintain them over a long period; however, you can get quite far with basic knowledge of K8s."
  },
  {
    "objectID": "notes/k8s/index.html#ml-research-is-crowded.-compete-on-swe-skills.",
    "href": "notes/k8s/index.html#ml-research-is-crowded.-compete-on-swe-skills.",
    "title": "Why Should Data Scientists Learn Kubernetes?",
    "section": "ML research is crowded. Compete on SWE skills.",
    "text": "ML research is crowded. Compete on SWE skills.\n\n\n\nA crowded space of scientists frantically trying to do work\n\n\nOne of the best ways to set yourself apart as a data scientist is through your skills. Traditional education often emphasizes learning the latest ML techniques. However, cutting-edge ML research is very competitive. It also moves incredibly fast! In the last six months, we have seen Stable Diffusion and ChatGPT, with more to come. It‚Äôs also an extremely crowded space.\nIn my experience, the bottleneck many teams face is not a lack of knowledge of cutting-edge ML techniques, but software engineering skills and partners to help operationalize models. If you take some time to learn how to stand up tools and infrastructure, you will be invaluable to your team."
  },
  {
    "objectID": "notes/k8s/index.html#your-company-likely-already-runs-k8s",
    "href": "notes/k8s/index.html#your-company-likely-already-runs-k8s",
    "title": "Why Should Data Scientists Learn Kubernetes?",
    "section": "Your company likely already runs K8s",
    "text": "Your company likely already runs K8s\n\n\n\nA scientist shaking hands with someone who runs infrastructure\n\n\nI believe that data scientists should have tools that meet them where they are. An example of an anti-pattern of failing to meet data scientists where they are is not allowing development in Jupyter Notebooks.\nBy deploying and exploring tools in K8s you increase the likelihood that:\n\nYour DevOps counterparts will feel comfortable with the tools you want to deploy\nYou will have a shared language in which to talk to your application administrators\nYou will be more likely to attract people to help you with infra\nYou will look smart for leveraging technology that‚Äôs (likely) already adopted\n\nThese factors make it much more likely that you will get the tools that meet you where you are as opposed to something a software engineer without any data science experience thinks is a good idea (which I‚Äôve seen happen a lot!)\nEven if your company doesn‚Äôt run K8s, you can recruit generalists that can administer K8s for you, as opposed to trying to find a unicorn that specializes in ML Infrastructure."
  },
  {
    "objectID": "notes/k8s/index.html#but-isnt-it-overkill",
    "href": "notes/k8s/index.html#but-isnt-it-overkill",
    "title": "Why Should Data Scientists Learn Kubernetes?",
    "section": "But isn‚Äôt it overkill?",
    "text": "But isn‚Äôt it overkill?\n\n\n\nCutting oranges with a chainsaw\n\n\nFor simple apps that you want to stand up quickly or prototype, K8s is likely overkill. Instead, I‚Äôm advocating knowledge of K8s as useful when working within the typical constraints and environments in many companies. For example, if you want to deploy production software, hosting your data product on a single VM is often insufficient. Many companies even have infrastructure that may block you from doing this with paved paths that only include Kubernetes.\nEven if you are not deploying any production software, K8s can be invaluable in allowing you to deploy the tools you need."
  },
  {
    "objectID": "notes/k8s/index.html#you-dont-need-to-become-an-expert",
    "href": "notes/k8s/index.html#you-dont-need-to-become-an-expert",
    "title": "Why Should Data Scientists Learn Kubernetes?",
    "section": "You don‚Äôt need to become an expert",
    "text": "You don‚Äôt need to become an expert\n\n\n\nA student sitting at a desk in a library\n\n\nK8s are complicated, but you don‚Äôt need to become an expert to unlock tons of value as a Data Scientist. I would focus on three capabilities:\n\nDeploying tools/infra you need (with the help of managed cloud services)\nBasic debugging\nFamiliarity with high-level concepts\n\nFurthermore, I‚Äôm not suggesting that data scientists become K8s administrators. That is a very involved task and worthy of its own role. However, we can use managed cloud services and some basic knowledge to get far. That‚Äôs why I‚Äôm teaching a course on this topic: Kubernetes for Data Scientists."
  },
  {
    "objectID": "notes/k8s/index.html#course-k8s-for-data-scientists",
    "href": "notes/k8s/index.html#course-k8s-for-data-scientists",
    "title": "Why Should Data Scientists Learn Kubernetes?",
    "section": "Course: K8s for Data Scientists",
    "text": "Course: K8s for Data Scientists\nTODO"
  },
  {
    "objectID": "notes/k8s/storage/05-Dynamic Provisioning.html",
    "href": "notes/k8s/storage/05-Dynamic Provisioning.html",
    "title": "Hamel's Blog",
    "section": "",
    "text": "[[k8s]]\nIn [[3. Storage - Basics]], you were shown how to setup a PV, and a PVC that would bind to the PV, and finally how to create a deployment that would reference the PVC\nHowever we can have K8s dynamically provision the PV. So you just create the PVC, and K8s creates the PV for you! You can have clusters configured with different storage classes. You can also use the default class:\nSo if we deploy ths, let‚Äôs see what will happen! kl apply -f todo-list/postgres-persistentVolumeClaim-dynamic.yaml\nIf you do kl get pv you will see a PV has been automatically created."
  },
  {
    "objectID": "notes/k8s/storage/05-Dynamic Provisioning.html#storage-classes",
    "href": "notes/k8s/storage/05-Dynamic Provisioning.html#storage-classes",
    "title": "Hamel's Blog",
    "section": "Storage Classes",
    "text": "Storage Classes\nkind: StorageClass\nYou can create a storage class and reference it from the PVC. Three fields:\n\nprovisioner: How to create PV on demand\nreclaimPolicy what happens to dynamically created volumes when PVC is deleted\nvolumeBindingMode if pv is created now or when the related Pod is created\n\nExample of [[StorageClass]]\napiVersion: storage.k8s.io/v1\nkind: StorageClass\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"storage.k8s.io/v1\",\"kind\":\"StorageClass\",\"metadata\":{\"annotations\":{},\"name\":\"kiamol\"},\"provisioner\":\"docker.io/hostpath\",\"reclaimPolicy\":\"Delete\",\"volumeBindingMode\":\"Immediate\"}\n  creationTimestamp: \"2022-12-06T00:45:35Z\"\n  name: kiamol\n  resourceVersion: \"819084\"\n  uid: 79a1b70e-6ebe-4aa8-92ce-595220fc6b14\nprovisioner: docker.io/hostpath\nreclaimPolicy: Delete\nvolumeBindingMode: Immediate\nYou can see all of the storage classes in your cluster with kl get sc\n% cat sktorageClass/postgres-persistentVolumeClaim-storageClass.yaml                                                                   \napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: postgres-pvc-kiamol\nspec:\n  accessModes:\n    - ReadWriteOnce\n  storageClassName: kiamol\n  resources:\n    requests:\n      storage: 100Mi\nYou would use the above storage class in a deployment like this:\nkind: Deployment\n...\n    spec:\n      containers:\n        - name: db\n          image: postgres:11.6-alpine\n...\n          volumeMounts:\n            - name: secret\n              mountPath: \"/secrets\"\n            - name: data\n              mountPath: /var/lib/postgresql/data\n      volumes:\n        - name: secret\n          secret:\n            secretName: todo-db-secret\n            defaultMode: 0400\n            items:\n            - key: POSTGRES_PASSWORD\n              path: postgres_password\n        - name: data\n          persistentVolumeClaim:\n            claimName: postgres-pvc-kiamol"
  },
  {
    "objectID": "notes/k8s/storage/04-Basics.html",
    "href": "notes/k8s/storage/04-Basics.html",
    "title": "Hamel's Blog",
    "section": "",
    "text": "[[k8s]]\nThis is Chapter 5 in KIAMOL\nUnlike compute, storage is more complicated because you don‚Äôt want your data to get lost on pod restarts.\nSolution: you want to mount external file systems that will survive a container restart.\nConfigMaps and Secrets are mounted, but those are read aonly."
  },
  {
    "objectID": "notes/k8s/storage/04-Basics.html#pod-storage",
    "href": "notes/k8s/storage/04-Basics.html#pod-storage",
    "title": "Hamel's Blog",
    "section": "Pod Storage",
    "text": "Pod Storage\nThis kind of storage lives outside the container but on the Pod. It will survive container restarts, but not a Pod restart.\n%cat sleep/sleep-with-emptyDir.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sleep\nspec:\n  selector:\n    matchLabels:\n      app: sleep\n  template:\n    metadata:\n      labels:\n        app: sleep\n    spec:\n      containers:\n        - name: sleep\n          image: kiamol/ch03-sleep\n          volumeMounts:             # Mounts a volume call data\n            - name: data\n              mountPath: /data      # into the /data directory\n      volumes:\n        - name: data           # this is the data volume spec\n          emptyDir: {}         # this is the EmptyDir type\nIf you want your data to persist across pod restarts, you have to mount a different type of storage."
  },
  {
    "objectID": "notes/k8s/storage/04-Basics.html#hostpath",
    "href": "notes/k8s/storage/04-Basics.html#hostpath",
    "title": "Hamel's Blog",
    "section": "HostPath",
    "text": "HostPath\nWrites files to a disk on a node. So it will survive pod replacements. However, it is only on that Node and K8s doesn‚Äôt replicate files to other nodes for you. Assumes that the replacement pod will always run on the same node :/\n% cat pi/nginx-with-hostPath.yaml                                                                                                     \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pi-proxy\n  labels:\n    app: pi-proxy\n...\n          volumeMounts:\n            - name: config\n              mountPath: \"/etc/nginx/\"\n              readOnly: true\n            - name: cache-volume\n              mountPath: /data/nginx/cache\n      volumes:\n        - name: config\n          configMap:\n            name: pi-proxy-configmap\n        - name: cache-volume\n          hostPath:\n            path: /volumes/nginx/cache  #uses a directory non the node\n            type: DirectoryOrCreate #creates a path if it doesn't exist\n[HostPath] is only a good idea when your app needs temporary storage, because it can dissapear with a node. You could use Pod Storage for this, too so its not clear when this is useful."
  },
  {
    "objectID": "notes/k8s/storage/04-Basics.html#persistent-volumes-and-claims",
    "href": "notes/k8s/storage/04-Basics.html#persistent-volumes-and-claims",
    "title": "Hamel's Blog",
    "section": "Persistent Volumes and Claims",
    "text": "Persistent Volumes and Claims\n\nThis section is largely pedagoical, you will want to use Dynamic volume provisioning in most cases.\n\nYou have to configure shared storage on your cloud provider. For example, if you had a NFS server with the domain name nfs.my.network your PV resource would look like this:\n% cat todo-list/persistentVolume-nfs.yaml                                                                                            \napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: pv01\nspec:\n  capacity:\n    storage: 50Mi\n  accessModes:\n    - ReadWriteOnce\n  nfs:\n    server: nfs.my.network\n    path: \"/kubernetes-volumes\n\nNode Labeling\nIf you can use a local storage for a PV like this:\n1st make sure your node is labeled: kl label node docker-desktop kiamol=ch05\n% cat todo-list/persistentVolume.yaml                                                                                                 \napiVersion: v1\nkind: PersistentVolume\nmetadata:\n  name: pv01\nspec:\n  capacity:\n    storage: 50Mi\n  accessModes:\n    - ReadWriteOnce   # Means that we can only mount this to ONLY ONE POD\n  local:\n    path: /volumes/pv01  # this path must be present on the node\n  nodeAffinity:\n    required:\n      nodeSelectorTerms:\n        - matchExpressions:\n          - key: kiamol\n            operator: In\n            values:\n              - ch05\nPods cannot use this directly, they need to use a [[PersistenVolumeClaim]] or PVC. The PVC gets matched to a PV by K8s which leaves the underling volume details to the Pv.\n%cat todo-list/postgres-persistentVolumeClaim.yaml                                                                                   \napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: postgres-pvc\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 40Mi                  # 40 MB\n  storageClassName: \"\"               # A blank name means a PV needs to exist\nPV is like creating storage PVC is requesting storage that Pods use\n\n\nManual Provisioning\nWe have been manually provisioning PV + PVCs\nWhen you kl apply the PVC, it will find unbound PVs and then bind them.\nwhen you run kl get pv you will see if the PV is unclaimed yet or not\nif you create a PVC that requests more than any PV, it will show a pending status instead of Bound.\n\nIf you try to deploy a pod that uses an unbound PVC, the Pod will stay in a Pending state until the PVC gets bound\n\n\n\nBinding To the PVC\nThe deployment references the PVC like so:\n% cat todo-list/postgres/todo-db.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: todo-db\nspec:\n  selector:\n    matchLabels:\n      app: todo-db\n  template:\n    metadata:\n      labels:\n        app: todo-db\n    spec:\n      containers:\n        - name: db\n          image: postgres:11.6-alpine\n          env:\n          - name: POSTGRES_PASSWORD_FILE\n            value: /secrets/postgres_password\n          volumeMounts:\n            - name: secret\n              mountPath: \"/secrets\"\n            - name: data\n              mountPath: /var/lib/postgresql/data\n      volumes:\n        - name: secret\n          secret:\n            secretName: todo-db-secret\n            defaultMode: 0400\n            items:\n            - key: POSTGRES_PASSWORD\n              path: postgres_password\n        - name: data\n          persistentVolumeClaim:\n            claimName: postgres-pvc\n\nIn production, you want to replace the local volume PV with a distributed volume supported by your cloud provider or cluster.\nThe PVC doesn‚Äôt care about the implementation so you will just have to swap out the PV"
  },
  {
    "objectID": "notes/k8s/02-Basics.html",
    "href": "notes/k8s/02-Basics.html",
    "title": "Basics",
    "section": "",
    "text": "These are rough notes and not meant for consumption by others! Any course material will be seperate and may or may not be related to these notes!\nDo stuff: kubectl apply -f\nMultiple resources in one yaml with ---"
  },
  {
    "objectID": "notes/k8s/02-Basics.html#deployments",
    "href": "notes/k8s/02-Basics.html#deployments",
    "title": "Basics",
    "section": "Deployments",
    "text": "Deployments\nkind: deployment\n[[Pods]] can have more than 1 container but usually contain just one\n[Deployments] control pods and will restart Pods if they fail. Deployments are a type of [[controller]]. You usually deploy pods via a deployment. kubectl create deployment. Deployments keep track of pods via labels and a label selector. If you change the pod‚Äôs labels the deployment might lose track of the pods.\nA [[controller]] is a K8s resource that manages other resources.\n-o yaml is great for seeing labels, you can swithc to - json and pipe to jq\nExecute a command in a container by doing kubectl exec -it <pod name> -- sh"
  },
  {
    "objectID": "notes/k8s/02-Basics.html#services",
    "href": "notes/k8s/02-Basics.html#services",
    "title": "Basics",
    "section": "Services",
    "text": "Services\nkind: service\n\nYou can‚Äôt switch a Service from one type to another in every version of Kubernetes, so you‚Äôll need to delete the original ClusterIP Service for the API before you can deploy the ExternalName Service.\n\n\nLabels\nIf you have overlapping labels for a particular deployment, the service will route to all deployments that match that label. If you want to control for this, add additional unique labels. Just having one label like ‚Äúmyapp‚Äù can be dangerous for this reason.\nServices deal with networking. These use labels, too via a selector.\n\n\nRouting internal traffic ClusterIP\nClusterIP: default service that is internal DNS. type: ClusterIP\nForward port 8080 on your local computer to port 80 in container: kubectl port-forward deploy/numbers-web 8080:80\n\n\nRouting external traffic: LoadBalancer\ntype: LoadBalancer Uses labels too\n\n\nRouting Traffic Outside K8s ExternalNameService\ntype: ExternalName\nYou have to watch out when making HTTP requests through ENS, b/c the header wil still contain the original hostname, which will probably get rejected. It‚Äôs fine for things like TCP etc for databases.\n\n\nNamespaces\nThis is relevant to networking b/c resources outside the default namespace will have a different network address\nkubectl get svc -n default kubectl get svc -n kube-system\nFor example, the internal kube-dns service:\n kl get svc -n kube-system                                                                                                                                                                            (master)kiamol\nNAME       TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE\nkube-dns   ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   9d\nCan be accessed like this kl exec deploy/sleep-1 -- sh - c'nslookup kube-dns.kube-system.svc.cluster.local"
  },
  {
    "objectID": "notes/k8s/02-Basics.html#configuring-applications",
    "href": "notes/k8s/02-Basics.html#configuring-applications",
    "title": "Basics",
    "section": "Configuring Applications",
    "text": "Configuring Applications\nYou can environment variables to Pod specs\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sleep\nspec:\n  selector:\n    matchLabels:\n      app: sleep\n  template:\n    metadata:\n      labels:\n        app: sleep\n    spec:\n      containers:\n        - name: sleep\n          image: kiamol/ch03-sleep\n          env:\n          - name: KIAMOL_CHAPTER\n            value: \"04\"\nYou usually don‚Äôt set configs in pod specs. You ususally use [[ConfigMap]]\nHow to reference a configmap instead/in addition to of an env variable:\n% cat sleep/sleep-with-configMap-env-file.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sleep\nspec:\n  selector:\n    matchLabels:\n      app: sleep\n  template:\n    metadata:\n      labels:\n        app: sleep\n    spec:\n      containers:\n        - name: sleep\n          image: kiamol/ch03-sleep\n          envFrom: # This section will bring in all env variables from the config map `sleep-config-env-file` which we create below.  This can be thought of as the \"baseline\" config.\n          - configMapRef:\n              name: sleep-config-env-file\n          env: # This section can override any environment variables from the config, including any other configs that are elswhere.  So this will override other things\n          - name: KIAMOL_CHAPTER\n            value: \"04\"\n          - name: KIAMOL_SECTION\n            valueFrom:\n              configMapKeyRef: # this came from another configMap\n                name: sleep-config-literal\n                key: kiamol.section\n\nCreating a [[ConfigMap]]\n\nMethod 1: from env file\nThis way is not recommended b/c you have to use kl create rather than kl apply , and you want to use kl apply for everything\nStart with an env file, like this:\n% cat sleep/ch04.env                                                                                                                                                                                   \nKIAMOL_CHAPTER=ch04\nKIAMOL_SECTION=ch04-4.1\nKIAMOL_EXERCISE=try it now\nCreate a config file from an env file\n% kl create configmap sleep-config-env-file --from-env-file=sleep/ch04.env                                                                                                                             \nconfigmap/sleep-config-env-file created\nUpdate your deployment by making changes to add the reference to the config file (see previous section)\nkl apply -f sleep/sleep-with-configMap-env-file.yaml\n\n\nMethod 2: from ConfigMap spec\nThis is more flexible and powerful, you can embed arbitrary files like json files that can be read by your app.\nCreate a spec:\n% cat todo-list/configMaps/todo-web-config-dev.yaml                                                                                                                                                    \napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: todo-web-config-dev\ndata: # we are going to mount this json file into the container so the app can use it\n  config.json: |-\n    {\n      \"ConfigController\": {\n        \"Enabled\" : true\n      }\n    }\nApply this spec: kl apply -f todo-list/configMaps/todo-web-config-dev.yaml\nP.S. You could have also seen the yaml file for the other configmap we created earlier with kl get cm/sleep-config-env-file -o yaml and used that yaml file\nUse the config map in the deployment spec, and additionally mount a volume containing the config:\n% cat todo-list/todo-web-dev.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: todo-web\nspec:\n  selector:\n    matchLabels:\n      app: todo-web\n  template:\n    metadata:\n      labels:\n        app: todo-web\n    spec:\n      containers:\n        - name: web\n          image: kiamol/ch04-todo-list\n          volumeMounts: # This will load the config json file into `/app/config` in your container\n            - name: config\n              mountPath: \"/app/config\" #directory path to mount the volume **BE CAREFUL** if you mounted this to `/app`, then it would have wiped out all the files!\n              readOnly: true\n      volumes: # volumes are defined at pod level\n        - name: config  # Name matches the volume mount\n          configMap: # volume source is the Config Map\n            name: todo-web-config-dev  #ConfigMap name\nBe careful when specifying the mount path, lots of people make mistakes here and overwrite existing data. K8s will not merge directories for you!\nIf you change the config map, it will refresh the files in the directory. You have to make sure your app is watching that directory though.\nInstead of loading the whole config map, you can selectively mount files in the config map like this:\n% cat todo-list/todo-web-dev-no-logging.yaml                                                                                                                                                                     \napiVersion: apps/v1\n...\n      volumes:\n        - name: config\n          configMap:\n            name: todo-web-config-dev\n            items:\n            - key: config.json\n              path: config.json"
  },
  {
    "objectID": "notes/k8s/scaling/07- Scaling.html",
    "href": "notes/k8s/scaling/07- Scaling.html",
    "title": "Scaling",
    "section": "",
    "text": "replicas can be used for scaling. You must also think about storage."
  },
  {
    "objectID": "notes/k8s/scaling/07- Scaling.html#daemonsets",
    "href": "notes/k8s/scaling/07- Scaling.html#daemonsets",
    "title": "Scaling",
    "section": "DaemonSets",
    "text": "DaemonSets\n[DaemonSets] allow you to run a service on each node. You can do this for node specific things like collecting logs on each node. DaemonSets are yet another kind of controller for Pods beyond [[Deployments]]\nIf you switch from a Deployment to a DaemonSet you should delete the Deployment first. You can‚Äôt automatically change from one kind of controller to another.\nA DaemonSet runs a control loop that will watch for any new nodes and start a pod on that node.\n\nUse cases for DaemonSets:\n\nWant to run a pod on every node\nyou have only a subset of nodes that can receive traffic from the internet -> use labels to achieve this.\n\n\n\nLabeling A Node For DaemonSets\nThis allows you to select which nodes the Daemonset runs on:\n% cat pi/proxy/daemonset/nginx-ds-nodeSelector.yaml\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: pi-proxy\n  labels:\n    kiamol: ch06\nspec:\n  selector:\n    matchLabels:\n      app: pi-proxy\n  template:\n    metadata:\n      labels:\n        app: pi-proxy\n    spec:\n...\n      nodeSelector:\n        kiamol: ch06\nTo use thie above yaml, you have to label your node like this:\nkl label node $(kl get nodes...) kiamol=ch06 --overwrite\n\n\nCascade Delete\nTLDR; you probably don‚Äôt need this\nYou can set cascade=False to delete a controller without deleting its managed objects. This is how you can change a controller but still keep pods alive.\nkl delete ds pi-proxy --cascade=orphan  # deletes the daemonset pi-proxy\nControllers use a label selector to find objects they manage, so you just have to make sure the new controller you define has the right label. Hamel: it‚Äôs not clear how to switch from a Daemonset to a deployment."
  },
  {
    "objectID": "notes/k8s/scaling/06-ReplicaSets.html",
    "href": "notes/k8s/scaling/06-ReplicaSets.html",
    "title": "ReplicaSets",
    "section": "",
    "text": "Hierachy is Deployments ->  ReplicaSets -> Pods -> Containers\nYou probably never need to fiddle with ReplicSets directly and will mostly operate at the Deployment abstraction level that‚Äôs mentioned in [[2a. Basics]].\nThe reason the ReplicaSet abstraction is used is that the Deployment turns the replicas count to 0 when you update the metadata of podspec in a Deployment, as a way of gracefully winding down old pods in favor of new pods.\nThis is why sometimes you might see a ReplicaSet with a desired pod count of zero.\nThe way replicas are controled via deployments are through the spec.replicas field:\n%cat  pi/proxy/nginx.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: pi-proxy\n  labels:\n    kiamol: ch06\nspec:\n  replicas: 2  # Two replicas for nginx\n  selector:\n    matchLabels:\n      app: pi-proxy\n  template:\n    metadata:\n      labels:\n        app: pi-proxy\n    spec:\n      containers:\n        - image: nginx:1.17-alpine\n          name: nginx\n          ports:\n            - containerPort: 80\n              name: http\n          volumeMounts:\n            - name: config\n              mountPath: \"/etc/nginx/\"\n              readOnly: true\n            - name: cache-volume\n              mountPath: /data/nginx/cache\n      volumes:\n        - name: config\n          configMap:\n            name: pi-proxy-configmap\n        - name: cache-volume\n          emptyDir: {}"
  },
  {
    "objectID": "notes/k8s/03-Secrets.html",
    "href": "notes/k8s/03-Secrets.html",
    "title": "Secrets",
    "section": "",
    "text": "Unlike [[ConfigMap]], K8s doesn‚Äôt like to show you plain text version of your secret. You must decode it with base64 -d, this is not encrypted, just obfuscated.\nThe container still sees the original plain text data. Let‚Äôs manually create a secret like this:\nkl create secret generic sleep-secret-literal --from-literal=secret=shh...\nThen, we reference this seret in a deployment like this:\n% cat sleep/sleep-with-secret.yaml                                                                                                    \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sleep\nspec:\n  selector:\n    matchLabels:\n      app: sleep\n  template:\n    metadata:\n      labels:\n        app: sleep\n    spec:\n      containers:\n        - name: sleep\n          image: kiamol/ch03-sleep\n          env:\n          - name: KIAMOL_SECRET\n            valueFrom:\n              secretKeyRef:\n                name: sleep-secret-literal\n                key: secret\nIf we apply this kl apply -f sleep/sleep-with-secret.yaml , we can see the secret lke this:\n% kl exec -it deploy/sleep -- printenv | grep KIAMOL_SECRET                                                                           \nKIAMOL_SECRET=shh...\n\nYou shouldn‚Äôt expose secrets as environment variables as that is not very secure. You should store secrets in files that have restricted premissions.\n\nYou can also store your secrets in plain text in a YAML file like so:\n%cat todo-list/secrets/todo-db-secret-test.yaml                                                                                      \napiVersion: v1\nkind: Secret\nmetadata:\n  name: todo-db-secret-test\ntype: Opaque\nstringData:                          # use this field when using plain text stuff\n  POSTGRES_PASSWORD: \"kiamol-2*2*\"   # this is the plain text password\nInterestingly, you will be able to see the plain text password if you do this! See the metadata.annotations field:\n%kl get secret/sleep-secret-literal -o yaml\napiVersion: v1\ndata:\n  POSTGRES_PASSWORD: a2lhbW9sLTIqMio=\nkind: Secret\nmetadata:\n  annotations:\n    kubectl.kubernetes.io/last-applied-configuration: |\n      {\"apiVersion\":\"v1\",\"kind\":\"Secret\",\"metadata\":{\"annotations\":{},\"name\":\"todo-db-secret-test\",\"namespace\":\"default\"},\"stringData\":{\"POSTGRES_PASSWORD\":\"kiamol-2*2*\"},\"type\":\"Opaque\"}\n  creationTimestamp: \"2022-12-01T18:22:56Z\"\n  name: todo-db-secret-test\n  namespace: default\n  resourceVersion: \"629050\"\n  uid: 35b42a79-a8dd-447d-a191-a295ca1e4d66\ntype: Opaque"
  },
  {
    "objectID": "notes/k8s/03-Secrets.html#mounting-secrets-as-files",
    "href": "notes/k8s/03-Secrets.html#mounting-secrets-as-files",
    "title": "Secrets",
    "section": "Mounting Secrets as Files",
    "text": "Mounting Secrets as Files\nThis is recommended over env variables\n% cat todo-list/todo-db-test.yaml                                                                                                     \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: todo-db\nspec:\n  selector:\n    matchLabels:\n      app: todo-db\n  template:\n    metadata:\n      labels:\n        app: todo-db\n    spec:\n      containers:\n        - name: db\n          image: postgres:11.6-alpine\n          env:\n          - name: POSTGRES_PASSWORD_FILE\n            value: /secrets/postgres_password\n          volumeMounts:\n            - name: secret                           # Mounts a secret volume\n              mountPath: \"/secrets\"                  # The location\n      volumes:\n        - name: secret\n          secret:                                     # Volumen loaded\n            secretName: todo-db-secret-test           #Name of secret\n            defaultMode: 0400                         #Permissions set for files\n            items:\n            - key: POSTGRES_PASSWORD\n              path: postgres_password\nYou can see that this secret is now mounted as a file\n%kl exec deploy/todo-db -- ls /secrets                                                                                               \npostgres_password\n\n% kl exec deploy/todo-db -- cat /secrets/postgres_password                                                                            \nkiamol-2*2*"
  },
  {
    "objectID": "notes/k8s/03-Secrets.html#bringing-together-config-secrets-deployments-and-services",
    "href": "notes/k8s/03-Secrets.html#bringing-together-config-secrets-deployments-and-services",
    "title": "Secrets",
    "section": "Bringing together config, secrets, deployments and services",
    "text": "Bringing together config, secrets, deployments and services\nHere is an example file that uses both ConfigMaps and Secrets in a deployment\n% cat todo-list/todo-web-test.yaml | pbcopy\napiVersion: v1\nkind: Service\nmetadata:\n  name: todo-web-test\nspec:\n  ports:\n    - port: 8081\n      targetPort: 80\n  selector:\n    app: todo-web\n    environment: test\n  type: LoadBalancer\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: todo-web-test\nspec:\n  selector:\n    matchLabels:\n      app: todo-web\n      environment: test\n  template:\n    metadata:\n      labels:\n        app: todo-web\n        environment: test\n    spec:\n      containers:\n        - name: web\n          image: kiamol/ch04-todo-list\n          env:\n          - name: ASPNETCORE_ENVIRONMENT\n            value: Test\n          volumeMounts:\n            - name: config\n              mountPath: \"/app/config\"\n              readOnly: true\n            - name: secret\n              mountPath: \"/app/secrets\"\n              readOnly: true\n      volumes:\n        - name: config\n          configMap:\n            name: todo-web-config-test\n            items:\n            - key: config.json\n              path: config.json\n        - name: secret\n          secret:\n            secretName: todo-web-secret-test\n            defaultMode: 0400\n            items:\n            - key: secrets.json\n              path: secrets.json\nThat json secret is stored like this:\n% cat todo-list/secrets/todo-web-secret-test.yaml                                                                                     \napiVersion: v1\nkind: Secret\nmetadata:\n  name: todo-web-secret-test\ntype: Opaque\nstringData:\n  secrets.json: |-\n    {\n      \"ConnectionStrings\": {\n        \"ToDoDb\": \"Server=todo-db;Database=todo;User Id=postgres;Password=kiamol-2*2*;\"\n      }\n    }\nYou can see that these files exist now\n% kl exec deploy/todo-web-test -- ls /app/                                                                                            \nconfig\nsecrets\n..."
  },
  {
    "objectID": "notes/k8s/03-Secrets.html#updating-configurations",
    "href": "notes/k8s/03-Secrets.html#updating-configurations",
    "title": "Secrets",
    "section": "Updating Configurations",
    "text": "Updating Configurations\nYour app may only read config when it starts, so if you change configuration settings via [[ConfigMap]] or [[Secrets]] then you would have to restart your app. Two ways to do this:\n\nkl rollout restart deploy/.... (preferred method)\nDelete all pods related to that deployment using a label selector or something similar and let the deployment restart them.\n\nContext From Discord chat with Michal https://discord.com/channels/1043031122721914940/1045846418331537459/1047961668426158192"
  },
  {
    "objectID": "notes/k8s/helm/16-Creating Your Own Helm Charts.html",
    "href": "notes/k8s/helm/16-Creating Your Own Helm Charts.html",
    "title": "Creating Helm Charts",
    "section": "",
    "text": "This is going to be really light, as we don‚Äôt want to get too deep into this. You can really just skip this if you like.\nYou can reference a directory, vs a zip archive when developing locally."
  },
  {
    "objectID": "notes/k8s/helm/16-Creating Your Own Helm Charts.html#validate-with-helm-lint",
    "href": "notes/k8s/helm/16-Creating Your Own Helm Charts.html#validate-with-helm-lint",
    "title": "Creating Helm Charts",
    "section": "Validate with helm lint",
    "text": "Validate with helm lint\nhelm lint directory/"
  },
  {
    "objectID": "notes/k8s/helm/16-Creating Your Own Helm Charts.html#install",
    "href": "notes/k8s/helm/16-Creating Your Own Helm Charts.html#install",
    "title": "Creating Helm Charts",
    "section": "Install",
    "text": "Install\nhelm install directory/"
  },
  {
    "objectID": "notes/k8s/helm/17-Helm Upgrade & Rollbacks.html",
    "href": "notes/k8s/helm/17-Helm Upgrade & Rollbacks.html",
    "title": "Helm Upgrades & Rollbacks",
    "section": "",
    "text": "Recommended pattern:"
  },
  {
    "objectID": "notes/k8s/helm/17-Helm Upgrade & Rollbacks.html#install-test-the-new-version",
    "href": "notes/k8s/helm/17-Helm Upgrade & Rollbacks.html#install-test-the-new-version",
    "title": "Helm Upgrades & Rollbacks",
    "section": "1. Install & test the new version",
    "text": "1. Install & test the new version\nlist all installed releases:\n% helm ls -q \nch10-vweb\nsee which versions are available\n% helm search repo vweb --versions  \nNAME        CHART VERSION   APP VERSION DESCRIPTION\nkiamol/vweb 2.0.0           2.0.0       Simple versioned web app\nkiamol/vweb 1.0.0           1.0.0       Simple versioned web app\nInstall the new version of the app\n# check the values for the new chart version:\nhelm show values kiamol/vweb --version 2.0.0\n\nhelm install --set servicePort=8020 --set replicaCount=1 --set serviceType=ClusterIP ch10-vweb-v2 kiamol/vweb --version 2.0.0\nAfter you test the app,"
  },
  {
    "objectID": "notes/k8s/helm/17-Helm Upgrade & Rollbacks.html#uninstall-the-test-release",
    "href": "notes/k8s/helm/17-Helm Upgrade & Rollbacks.html#uninstall-the-test-release",
    "title": "Helm Upgrades & Rollbacks",
    "section": "2. Uninstall the test release",
    "text": "2. Uninstall the test release\nYou can see a list of all releases with helm list\n# see a list of releases\nhelm list\n....\n\nhelm uninstall ch10-vweb-v2"
  },
  {
    "objectID": "notes/k8s/helm/17-Helm Upgrade & Rollbacks.html#perform-an-upgrade",
    "href": "notes/k8s/helm/17-Helm Upgrade & Rollbacks.html#perform-an-upgrade",
    "title": "Helm Upgrades & Rollbacks",
    "section": "3. Perform an upgrade",
    "text": "3. Perform an upgrade\nYou can upgrade like this, optionally using the --reuse-values and --atomic flags:\nThe --atomic flag is important, always use this!\n\nwith the atomic flag. It waits for all the resource updates to complete, and if any of them fails, it rolls back every other resource to the previous state.\n\nhelm upgrade --reuse-values --atomic ch10-vweb kiamol/vweb --version 2.0.0\n\nAlways use the --atomic flag!\n\n\nWhen you upgrade, the --reuse-values flag will often be handy. However, this can cause things to break if the api changes between versions. So use with extreme care!"
  },
  {
    "objectID": "notes/k8s/helm/15-Helm.html",
    "href": "notes/k8s/helm/15-Helm.html",
    "title": "Hamel's Blog",
    "section": "",
    "text": "It‚Äôs a client-side tool\nUses kubectl to connect to your cluster\nAdd repos with a URL helm repo add https://...\nUpdate repos with helm repo update\nIt‚Äôs basically parametrized YAML\n\nHelm templates are not valid YAML, so you can‚Äôt use kubectl\nJeremy Lewi: Use Kustomize, not Helm, if you can."
  },
  {
    "objectID": "notes/k8s/helm/15-Helm.html#add-a-repo",
    "href": "notes/k8s/helm/15-Helm.html#add-a-repo",
    "title": "Hamel's Blog",
    "section": "Add a repo",
    "text": "Add a repo\n helm repo add kiamol https://kiamol.net"
  },
  {
    "objectID": "notes/k8s/helm/15-Helm.html#inspect-default-values-in-chart",
    "href": "notes/k8s/helm/15-Helm.html#inspect-default-values-in-chart",
    "title": "Hamel's Blog",
    "section": "Inspect default values in chart",
    "text": "Inspect default values in chart\nSee what versions are available\n% helm search repo vweb --versions  \nNAME        CHART VERSION   APP VERSION DESCRIPTION\nkiamol/vweb 2.0.0           2.0.0       Simple versioned web app\nkiamol/vweb 1.0.0           1.0.0       Simple versioned web app\nSee the default values:\n% helm show values kiamol/vweb --version 1.0.0                                                                            \nservicePort: 8090\nreplicaCount: 2"
  },
  {
    "objectID": "notes/k8s/helm/15-Helm.html#install-the-chart",
    "href": "notes/k8s/helm/15-Helm.html#install-the-chart",
    "title": "Hamel's Blog",
    "section": "Install the chart",
    "text": "Install the chart\nOverride default values, and name the release ch10-vweb:\n helm install --set servicePort=8010 --set replicaCount=1 ch10-vweb kiamol/vweb --version 1.0.0\nSee the deployment (labels omitted in below output for brevity)\n% kl get deploy --show-labels                                                                                                              \nNAME        READY   UP-TO-DATE   AVAILABLE   AGE\nch10-vweb   1/1     1            1           39s\n\nDry runs\nThere is also a --dry-run flag that will generate the YAML for you."
  },
  {
    "objectID": "notes/k8s/helm/15-Helm.html#update-the-release",
    "href": "notes/k8s/helm/15-Helm.html#update-the-release",
    "title": "Hamel's Blog",
    "section": "Update the release",
    "text": "Update the release\nUse helm upgrade :\nIn this case we are going to increase the replica count:\n% helm upgrade --set servicePort=8010 --set replicaCount=3 ch10-vweb kiamol/vweb --version 1.0.0\nRelease \"ch10-vweb\" has been upgraded. Happy Helming!\nNAME: ch10-vweb\nLAST DEPLOYED: Tue Dec 13 11:10:04 2022\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 2\nTEST SUITE: None"
  },
  {
    "objectID": "notes/index.html",
    "href": "notes/index.html",
    "title": "üìö Notes",
    "section": "",
    "text": "See The Listing Menu\n\n\n\nClick on ‚Äúüìö Notes‚Äù above to expand the listing menu.\n\n\nThis is a place where I share some of my notes. If you find an error with any of these notes, please submit a pull request. You can also get in touch with me here."
  },
  {
    "objectID": "notes/programming-languages/pl.html",
    "href": "notes/programming-languages/pl.html",
    "title": "programming languages",
    "section": "",
    "text": "High level takeaways after completing the 3-Part Coursera class Programming Languages with Dan Grossman.\nYour GitHub repo for this class (private) is here."
  },
  {
    "objectID": "notes/programming-languages/pl.html#sml-standard-ml-part-a",
    "href": "notes/programming-languages/pl.html#sml-standard-ml-part-a",
    "title": "programming languages",
    "section": "SML (Standard ML) Part A",
    "text": "SML (Standard ML) Part A\n\nYou setup vim to have an IDE for this. See notes in the VIM section below.\nML is a statically typed language with magical type inference that works really well. It automatically determines the types and is very intuitive and helpful.\nLearned how to use recursion everywhere instead of loops, particularly with hd, tl and cons.\nLocal variable binding with let is very important (which also allows you to bind local/private functions as well)\ncons allows you to append to the beginning of a list\nThere is an option type that is NONE or SOME v\nThis language doesn‚Äôt encourage mutation, which is a feature. Otherwise, you can use a reference which is like a pointer to mutate a variable.\npattern matching with a case expression: This is one of the coolest things that I learned, and something similar is coming to Python v 3.10.\n\nYou can have nested patterns\nYou can pattern match against function arguments which allow for really nice syntax for achieving multiple dispatch type of functionality.. (not sure about python)\nYou can pattern match against types as well as data structures.\nYou can have constants in there as well.\n\ncase name \n     NameType name => ...\n   | (first, \"MyLastName\") => ...\n   | (first, last) => ...\n   | name => ...\n   | _ => ...\nTail recursion with accumulators. Ex- factorial\nThe fn keyword is used to define anonymous functions.\nML uses lexical scope which means function is evaluated in the environment where the function was defined. dynamic scope, which is usually not desired, is the alternative where the function is evaluated in the in the environment it is called.\nClosure - the call stack has a ‚Äúpair‚Äù that is the (function, environment when the function was defined). This pair is called the closure. The call stack has a snapshot of what the environment looked like at the time the function was defined.\nfold is like reduce.\nML supports function composition like this with the keyword o: f1 o f2 o f3\n\nbest to do a val binding to avoid unnecessary wrapping: val newfunc = f1 o f2\nwith o you apply functions from right to left so f1 o f2 x is the same as f1(f2(x)) there is an alternative that is left to right called the pipeline operator.\n\nCurrying and partial application\n\nUniversal way to make a func curryable: ml       fun myfunc x           let fun f2 (z) = z               fun f1 (y) = f2(y)           begin               f1           end\nML has first class support for currying so you don‚Äôt have to do the above hack.\n\nML supports mutual recursion just like let-rec in racket."
  },
  {
    "objectID": "notes/programming-languages/pl.html#racket-part-b",
    "href": "notes/programming-languages/pl.html#racket-part-b",
    "title": "programming languages",
    "section": "Racket (Part B)",
    "text": "Racket (Part B)\nRacket is related to Lisp and Scheme. Everything is a function. Parenthesis for everything. The position of parenthesis changes the meaning of the code.\n\nRacket has dynamic typing, unlike SML.\nThunks: Wrap a function in a zero argument function to delay evaluation. Applications:\n\nStreams: the function will return a tuple of (value, func), and when you call func it will return (value, func) so you get one value at a time. This is not specific to Racket.\nLazy evaluation: You can use thunks to delay execution like a promise to a later time. This is an example of lazy evalution that doesn‚Äôt actually evaluate anything until being forced to:\n\n\n(define (my-delay f) (mcons #f f))\n\n(define (my-force th)\n\n(if (mcar th) (mcdr th) (begin (set-mcar! th #t) (set-mcdr! th ((mcdr th))) (mcdr th))))\nRacket allows you use macros that will evaluate before the code is run and that will ‚Äúexpand‚Äù into valid racket syntax.\nYou implemented your own small programming language. This used recursive calls to evluate expressions with the base case being the values (Integer, strings, etc). - Interperter: write a program in another language A that takes programs in B and produces answers directly. A better term would be ‚Äúevaluator‚Äù. - Compiler: write program in another language A that takes programs in B and produces an equivalent program in langauage C. A better term here would be ‚Äútranslator‚Äù.\nClosures: for lexical scope, the interpreter has a stack of tuples. The tuples are (1) the function to be called (2) the environment, which contains the value of all variables at the time the function was defined. You also have to track the arguments for the function seperately, so you can evaluate the arguments in the environment the function was run in."
  },
  {
    "objectID": "notes/programming-languages/pl.html#ruby-part-c",
    "href": "notes/programming-languages/pl.html#ruby-part-c",
    "title": "programming languages",
    "section": "Ruby (Part C)",
    "text": "Ruby (Part C)\nI didn‚Äôt spend too much time some concepts I was mostly familiar with this.\n\nRuby is OOP, dynamically typed.\nRuby is pure OOP, even top level functions and variables are part of the built-in Object class.\nThey have fastcore like shortcuts for getters and setters:\n\nattr_reader :y, :z # defines getters \nattr_accessor :x # defines getters and setters\nnewlines are important. The syntax can change without them.\nDynamic class definitions. The following code will result in Class with the methods foo and bar! The second one doesn‚Äôt override the first one!\nclass Class\n    def foo\n        ...\n    end\nend\n\nclass Class\n    def bar\n        ...\n    end\nend\n\nBlocks\nThey also have a very convenient lambda like thing called Blocks:\nsum = 0 \n[4,6,8].each { |x| sum += x \n               puts sum }\nYou can use Blocks to make accumulators too, and even use inject to initialize the accumulator:\nsum = [4,6,8].inject(0) { |acc,elt| acc + elt }\nTo use blocks in a method, you will have to look that up in the docs. This involves the yield keyword. For example, this code will print ‚Äúhi‚Äù 3 times:\ndef foo x \n  if x \n    yield \n   else \n    yield \n    yield \n   end \nend \n\nfoo (true) { puts \"hi\" } \nfoo (false) { puts \"hi\" }\nBlocks are not first class functions even though they kind of look like lambdas. Lets say you wanted to map over an array but wanted to return an array of functions instead of values. The way to do this is to use the keyword lambda:\nc = a.map {|x| {|y| x >= y} } # wrong, a syntax error\n\nc = a.map {|x| lambda {|y| x >= y} } # this will work\n\nSubclassing\n\nsuper calls the same method in the parent class. You dont have to do super.method_name(), just super.\nInstance variables are preceeded with @\n\nChild classes are defined like this:\nclass Child < Parent\n ...\nend\n\n\n\nTyping\nThey discussed the various ways different type systems are constructed. The interface idiom, that is familar to you from Golang (but not specific to Golang) was introduced."
  },
  {
    "objectID": "notes/programming-languages/pl.html#vim",
    "href": "notes/programming-languages/pl.html#vim",
    "title": "programming languages",
    "section": "VIM",
    "text": "VIM\nFor the Standard ML programming language I decided to force myself to use vim. I added the following things to my .vimrc to make it manageable. Note the plugin jez/vim-better-sml\n\" from https://github.com/jez/vim-as-an-ide\nset nocompatible\n\ninoremap <C-e> <C-o>A\n\n\nfiletype off\n\nset rtp+=~/.vim/bundle/Vundle.vim\ncall vundle#begin()\n\nPlugin 'VundleVim/Vundle.vim'\n\n\" ----- Making Vim look good ------------------------------------------\nPlugin 'altercation/vim-colors-solarized'\nPlugin 'tomasr/molokai'\nPlugin 'vim-airline/vim-airline'\nPlugin 'vim-airline/vim-airline-themes'\n\n\" ----- Vim as a programmer's text editor -----------------------------\nPlugin 'scrooloose/nerdtree'\nPlugin 'jistr/vim-nerdtree-tabs'\nPlugin 'vim-syntastic/syntastic'\nPlugin 'xolox/vim-misc'\nPlugin 'xolox/vim-easytags'\nPlugin 'majutsushi/tagbar'\nPlugin 'ctrlpvim/ctrlp.vim'\n\" ----- Working with Git ----------------------------------------------\nPlugin 'airblade/vim-gitgutter'\nPlugin 'tpope/vim-fugitive'\nPlugin 'Raimondi/delimitMate'\nPlugin 'jez/vim-better-sml'\nPlugin 'christoomey/vim-tmux-navigator'\nPlugin 'benmills/vimux'\ncall vundle#end()\n\nfiletype plugin indent on\n\nset number\nset ruler\nset showcmd\nset incsearch\nset hlsearch\nset backspace=indent,eol,start\n\nsyntax on\nset mouse=a"
  },
  {
    "objectID": "notes/fastai/02_cv.html",
    "href": "notes/fastai/02_cv.html",
    "title": "Computer Vision",
    "section": "",
    "text": "import TOCInline from ‚Äò@theme/TOCInline‚Äô;\n\n\n\nRemember, Datablock helps create DataLoaders.\nfrom fastai.vision.all import *\npath = untar_data(URLs.PETS)\n\npets = DataBlock(blocks = (ImageBlock, CategoryBlock),\n                 get_items=get_image_files, \n                 splitter=RandomSplitter(seed=42),\n                 get_y=using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'), 'name'),\n                 item_tfms=Resize(460),\n                 batch_tfms=aug_transforms(size=224, min_scale=0.75))\n\n\nYou can debug the Datablock by calling .summary(), which will show you if you have any errors.\npets.summary(path/\"images\")\nIf everything looks good, you can use the DataBlock to create a DataLoaders instance:\ndls = pets.dataloaders(path/\"images\")\nOnce you have a DataLoaders instance, it is a good idea to call show_batch to spot check that things look reasonable:\nYou can debug this by using show_batch:\n>>> dls.show_batch(nrows=1, ncols=3)\n... [shows images]\nFinally, you can see what a batch looks like by calling dls.one_batch()\nx,y = dls.one_batch()\nYou always want to train a model ASAP as your final debugging step. If you wait too long, you will not discover problems\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(2)\n\n\n\nA common error is forgetting to use Resize in your DataBlock as an item transform. For example, the below code will cause an error:\npets1 = DataBlock(blocks = (ImageBlock, CategoryBlock),\n                 get_items=get_image_files, \n                 splitter=RandomSplitter(seed=42),\n                 #forgot to pass `item_tfms=Resize(...),`\n                 get_y=using_attr(RegexLabeller(r'(.+)_\\d+.jpg$'), 'name'))\npets1.summary(path/\"images\")\nThis will complain that it is not able to collate the images because they are of different sizes.\n\n\n\n\nYou can get diagnostics for your model using this:\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(12,12), dpi=60)\nWhich will return a confusion matrix. You can see the ‚Äúmost confused‚Äù items by doing this:\n>>> interp.most_confused(min_val=5)\n[('Bengal', 'Egyptian_Mau', 10),\n ('american_pit_bull_terrier', 'staffordshire_bull_terrier', 8),\n ('Ragdoll', 'Birman', 7),\n ('staffordshire_bull_terrier', 'american_pit_bull_terrier', 6),\n ('american_pit_bull_terrier', 'american_bulldog', 5)]\n\n\n\n\n\nStart with a very, very small learning rate, something so small that we would never expect it to be too big to handle. We use that for one mini-batch, find what the losses are afterwards, and then increase the learning rate by some percentage (e.g., doubling it each time). Then we do another mini-batch, track the loss, and double the learning rate again. We keep doing this until the loss gets worse, instead of better. This is the point where we know we have gone too far. We then select a learning rate a bit lower than this point. Our advice is to pick either:\n\nOne order of magnitude less than where the minimum loss was achieved (i.e., the minimum divided by 10)\nThe last point where the loss was clearly decreasing\n\nThe learning rate finder computes those points and more on the curve to help you. Additional learning rate suggestion algorithms can be passed into the function, by default only the valley paradigm is used. The learning rate finder can be called with learn.lr_find:\n>>> learn = cnn_learner(dls, resnet34, metrics=error_rate)\n>>> lr_min, lr_steep, lr_valley, lr_slide = learn.lr_find(suggest_funcs=(minimum, steep, valley, slide))\n\nThe default valley hueristic works just fine. Note, you will want to re-run this anytime you change your model such as unfreeze layers. You might want to run this periodically if you are checkpointing during training.\n\n\n\nWhen we create a model from a pretrained network fastai automatically freezes all of the pretrained layers for us. When we call the fine_tune method fastai does two things:\n\nTrains the randomly added layers for one epoch, with all other layers frozen\nUnfreezes all of the layers, and trains them all for the number of epochs requested\n\nAlthough this is a reasonable default approach, it is likely that for your particular dataset you may get better results by doing things slightly differently. The fine_tune method has a number of parameters you can use to change its behavior, but it might be easiest for you to just call the underlying methods directly if you want to get some custom behavior.\nfit_one_cycle is the suggested way to train models without using fine_tune. We‚Äôll see why later in the book; in short, what fit_one_cycle does is to start training at a low learning rate, gradually increase it for the first section of training, and then gradually decrease it again for the last section of training.\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fit_one_cycle(3, 3e-3) # train the head\nlearn.unfreeze() # unfreeze everything\nlearn.lr_find() # find new lr after unfreezing\nlearn.fit_one_cycle(6, lr_max=1e-5) #fine tune it all\n\n\n\nOne important aspect of fine tuning is discriminative learning rates: use a lower learning rate for the early layers of the neural network, and a higher learning rate for the later layers (and especially the randomly added layers).\nfastai lets you pass a Python slice object anywhere that a learning rate is expected. The first value passed will be the learning rate in the earliest layer of the neural network, and the second value will be the learning rate in the final layer. The layers in between will have learning rates that are multiplicatively equidistant throughout that range. Let‚Äôs use this approach to replicate the previous training, but this time we‚Äôll only set the lowest layer of our net to a learning rate of 1e-6; the other layers will scale up to 1e-4. Let‚Äôs train for a while and see what happens:\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fit_one_cycle(3, 3e-3)\nlearn.unfreeze()\nlearn.fit_one_cycle(12, lr_max=slice(1e-6,1e-4))\nWe can accomplish everything we did above by calling fine_tune instead. fine_tune will automatically apply discriminative learning rates for you:\n>>> learn.fine_tune??\nSignature:\nlearn.fine_tune(\n    epochs,\n    base_lr=0.002,\n    freeze_epochs=1,\n    lr_mult=100,\n    pct_start=0.3,\n    div=5.0,\n    lr_max=None,\n    div_final=100000.0,\n    wd=None,\n    moms=None,\n    cbs=None,\n    reset_opt=False,\n)\nSource:   \n@patch\n@delegates(Learner.fit_one_cycle)\ndef fine_tune(self:Learner, epochs, base_lr=2e-3, freeze_epochs=1, lr_mult=100,\n              pct_start=0.3, div=5.0, **kwargs):\n    \"Fine tune with `Learner.freeze` for `freeze_epochs`, then with `Learner.unfreeze` for `epochs`, using discriminative LR.\"\n    self.freeze()\n    self.fit_one_cycle(freeze_epochs, slice(base_lr), pct_start=0.99, **kwargs)\n    base_lr /= 2\n    self.unfreeze()\n    self.fit_one_cycle(epochs, slice(base_lr/lr_mult, base_lr), pct_start=pct_start, div=div, **kwargs)\nFile:      ~/anaconda3/lib/python3.9/site-packages/fastai/callback/schedule.py\nType:      method\n\n\n\n\nYou can achieve mixed precision training to speed up training and give you more memory headroom for bigger models with to_fp16()\nfrom fastai.callback.fp16 import *\nlearn = cnn_learner(dls, resnet50, metrics=error_rate).to_fp16()\nlearn.fine_tune(6, freeze_epochs=3)\nNote how you can use the freeze_epochs parameter to keep the base frozen for longer.\n\n\n\nLet‚Äôs say you have a Dataframe with filenames and multiple labels per filename. The best way to get started in to use the DataBlock api to construct Datasets and DataLoaders. A review of terminology:\n\nDataset: collection that returns a tuple of (x,y) for single item. Can do this with list(zip(x,y))\nDataLoader: an iterator that provides a stream of minibatches of (x,y) instead of a single item.\nDatasets: object that contains a training Dataset and a Validation dataset.\nDataLoaders: object that contains a training DataLoader and a validation DataLoader.\n\n\n\nYou can use a DataBlock:\n>>> from fastbook import *\n>>> from fastai.vision.all import *\n\n>>> path = untar_data(URLs.PASCAL_2007)\n>>> df = pd.read_csv(path/'train.csv')\n>>> def get_x(r): return path/'train'/r['fname']\n>>> def get_y(r): return r['labels'].split(' ')\n\n>>> dblock = DataBlock(get_x = get_x, get_y = get_y)\n>>> dsets = dblock.datasets(df)\n>>> dsets.train[0]\n\n(Path('/home/hamel/.fastai/data/pascal_2007/train/006162.jpg'), ['aeroplane'])\nNext we need to convert our images into tensors. We can do this by using the ImageBlock and MultiCategoryBlock:\n\n\n\n>>> dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                       get_x = get_x, get_y = get_y)\n>>> dsets = dblock.datasets(df)\n>>> dsets.train[0]\n\n(PILImage mode=RGB size=500x333,\n TensorMultiCategory([0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]))\n\n\n\nYou can inspect the vocabulary with the vocab attribute:\ndsets.train.vocab\n\n\n\nThe dataframe has a column called is_valid, we can use that do a train validation split. By default, the DataBlock uses a RandomSplitter. By default, RandomSplitter uses 20% of the data for the validation set.\ndef splitter(df):\n    train = df.index[~df['is_valid']].tolist()\n    valid = df.index[df['is_valid']].tolist()\n    return train,valid\n\ndblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                   splitter=splitter,\n                   get_x=get_x, \n                   get_y=get_y)\n\ndsets = dblock.datasets(df)\n\n\n\nDataLoaders build upon Datasets by streaming mini-batches instead of one example at a time. One prerequisite to making DataLoaders is that all the images are the same size. To do this you can use RandomResizedCrop:\ndblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock),\n                   splitter=splitter,\n                   get_x=get_x, \n                   get_y=get_y,\n                   item_tfms = RandomResizedCrop(128, min_scale=0.35))\ndls = dblock.dataloaders(df)\nWhen you are done with this, you want to debug things by calling show_batch:\ndls.show_batch(nrows=1, ncols=3)\n\n\n\n\nYou can create a learner like so:\nlearn = cnn_learner(dls, resnet18)\nOne useful thing is to debug / verify that the output shape conforms to what you are expecting. You can do this by running a tensor through your model and inspecting it‚Äôs output:\nx,y = to_cpu(dls.train.one_batch())\nactivs = learn.model(x)\nactivs.shape\n\nThis is what you would use to extract embeddings / activations out of another model\n\nIt‚Äôs a good idea to see what the activations look like:\n>>> activs[0]\nTensorBase([ 2.0858,  2.8195,  0.0460,  1.7563,  3.3371,  2.4251,  2.3295, -2.8101,  3.3967, -3.2120,  3.3452, -2.3762, -0.3137, -4.6004,  0.7441, -2.6875,  0.0873, -0.2247, -3.1242,  3.6477],\n       grad_fn=<AliasBackward0>)\nWe can see these are not b/w 0 and 1, because the sigmoid has not been applied yet.\n\n\nPyTorch already provides this function for us. In fact, it provides a number of versions, with rather confusing names!\nF.binary_cross_entropy and its module equivalent nn.BCELoss calculate cross-entropy on a one-hot-encoded target, but do not include the initial sigmoid. Normally for one-hot-encoded targets you‚Äôll want F.binary_cross_entropy_with_logits (or nn.BCEWithLogitsLoss), which do both sigmoid and binary cross-entropy in a single function, as in the preceding example.\nThe equivalent for single-label datasets (like MNIST or the Pet dataset), where the target is encoded as a single integer, is F.nll_loss or nn.NLLLoss for the version without the initial softmax, and F.cross_entropy or nn.CrossEntropyLoss for the version with the initial softmax.\nSince we have a one-hot-encoded target, we will use BCEWithLogitsLoss:\nloss_func = nn.BCEWithLogitsLoss()\nloss = loss_func(activs, y)\nWe don‚Äôt actually need to tell fastai to use this loss function (although we can if we want) since it will be automatically chosen for us. fastai knows that the DataLoaders has multiple category labels, so it will use nn.BCEWithLogitsLoss by default.\n\n\n\nWe need to make sure we have a metric that works for multi-label classfication:\ndef accuracy_multi(inp, targ, thresh=0.5, sigmoid=True):\n    \"Compute accuracy when `inp` and `targ` are the same size.\"\n    if sigmoid: inp = inp.sigmoid()\n    return ((inp>thresh)==targ.bool()).float().mean()\nWe can use partial to set the parameters we want in the metrics function and pass it like this:\nlearn = cnn_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=0.2))\nlearn.fine_tune(3, base_lr=3e-3, freeze_epochs=4)\nYou can change your metrics anytime and recalculate things. validate() will return the validation loss and metrics.\n>>> learn.metrics = partial(accuracy_multi, thresh=0.1)\n>>> learn.validate() # returns validation loss and metrics\n(#2) [0.10417556017637253,0.9376891851425171]\nYou can debug metrics by getting the predictions on the validation set with get_preds:\npreds,targs = learn.get_preds()\nassert preds.shape[0] == dls.valid.n\nOnce you have the predictions, you can run the metric function seperately:\naccuracy_multi(preds, targs, thresh=0.9, sigmoid=False)\n\n\n\nxs = torch.linspace(0.05,0.95,29)\naccs = [accuracy_multi(preds, targs, thresh=i, sigmoid=False) for i in xs]\nplt.plot(xs,accs);\n\nIn this case, we‚Äôre using the validation set to pick a hyperparameter (the threshold), which is the purpose of the validation set. Sometimes students have expressed their concern that we might be overfitting to the validation set, since we‚Äôre trying lots of values to see which is the best. However, as you see in the plot, changing the threshold in this case results in a smooth curve, so we‚Äôre clearly not picking some inappropriate outlier. This is a good example of where you have to be careful of the difference between theory (don‚Äôt try lots of hyperparameter values or you might overfit the validation set) versus practice (if the relationship is smooth, then it‚Äôs fine to do this).\n\n\n\n\nYes, X is images and y are floats. Ex: key point model -> predicting location of something like the center of someone‚Äôs face.\n\n\nFirst step is to get data with get_image_files\n# view the data and it's structure\npath = untar_data(URLs.BIWI_HEAD_POSE)\nPath.BASE_PATH = path\npath.ls().sorted()\n(path/'01').ls().sorted()\n\n# next get all the data systematically\nimg_files = get_image_files(path)\ndef img2pose(x): return Path(f'{str(x)[:-7]}pose.txt')\nimg2pose(img_files[0])\nIt is a good idea to see what you are working with as a general rule.\n\nYou can inspect images with the following code\nim = PILImage.create(img_files[0])\nim.to_thumb(160)\n\nDefine the functions to extract the data you need from the files. You can ignore what this does and treat it as a helper function, b/c your problem is likely to be specific.\ncal = np.genfromtxt(path/'01'/'rgb.cal', skip_footer=6)\ndef get_ctr(f):\n    ctr = np.genfromtxt(img2pose(f), skip_header=3)\n    c1 = ctr[0] * cal[0][0]/ctr[2] + cal[0][2]\n    c2 = ctr[1] * cal[1][1]/ctr[2] + cal[1][2]\n    return tensor([c1,c2])\n\n\n\nbiwi = DataBlock(\n    blocks=(ImageBlock, PointBlock),\n    get_items=get_image_files,\n    get_y=get_ctr,\n    splitter=FuncSplitter(lambda o: o.parent.name=='13'),\n    batch_tfms=aug_transforms(size=(240,320)),  \n)\nNote the splitter function: we want to ensure that our model can generalize to people that it hasn‚Äôt seen yet. Each folder in the dataset contains the images for one person. Therefore, we can create a splitter function that returns true for just one person, resulting in a validation set containing just that person‚Äôs images.\n\nPoints and Data Augmentation: We‚Äôre not aware of other libraries (except for fastai) that automatically and correctly apply data augmentation to coordinates. So, if you‚Äôre working with another library, you may need to disable data augmentation for these kinds of problems.\n\nThe only other difference from the previous data block examples is that the second block is a PointBlock. This is necessary so that fastai knows that the labels represent coordinates; that way, it knows that when doing data augmentation, it should do the same augmentation to these coordinates as it does to the images\n\n\n\nUsing showbatch:\ndls = biwi.dataloaders(path)\ndls.show_batch(max_n=9, figsize=(8,6))\nInspect the shape:\nxb,yb = dls.one_batch()\nxb.shape,yb.shape\n\n\n\nAs usual, we can use cnn_learner to create our Learner. Remember way back in <> how we used y_range to tell fastai the range of our targets? We‚Äôll do the same here - coordinates in fastai and PyTorch are always rescaled between -1 and +1 by the PointBlock, which is why you pass (-1, 1) to y_range.\n\n\n# Always use y_range when predicting a continous target\nlearn = cnn_learner(dls, resnet18, y_range=(-1,1))\nWe didn‚Äôt specify a loss function, which means we‚Äôre getting whatever fastai chooses as the default.\n>>> dls.loss_func\nFlattenedLoss of MSELoss()\nNote also that we didn‚Äôt specify any metrics. That‚Äôs because the MSE is already a useful metric for this task (although it‚Äôs probably more interpretable after we take the square root).\n\nYou should always set y_range when predicting continuous targets. y_range is implemented in fastai using sigmoid_range, which is defined as:\ndef sigmoid_range(x, lo, hi): return torch.sigmoid(x) * (hi-lo) + lo\nThis is set as the final layer of the model, if y_range is defined. Take a moment to think about what this function does, and why it forces the model to output activations in the range (lo,hi).\nHere‚Äôs what it looks like:\nplot_function(partial(sigmoid_range,lo=-1,hi=1), min=-4, max=4)\n\n\n\n\n\nlearn.lr_find()\nlr = 1e-2\nlearn.fine_tune(3, lr)\n\n\n\nlearn.show_results(ds_idx=1, nrows=3, figsize=(6,8))\n\n\n\n\n\nfastai will automatically try to pick the right one from the data you built, but if you are using pure PyTorch to build your DataLoaders, make sure you think hard when you have to decide on your choice of loss function, and remember that you most probably want:\n\nnn.CrossEntropyLoss for single-label classification\nnn.BCEWithLogitsLoss for multi-label classification\nnn.MSELoss for regression"
  },
  {
    "objectID": "notes/fastai/03_data.html",
    "href": "notes/fastai/03_data.html",
    "title": "Data",
    "section": "",
    "text": "from fastbook import *"
  },
  {
    "objectID": "notes/fastai/03_data.html#hello-world-datablock",
    "href": "notes/fastai/03_data.html#hello-world-datablock",
    "title": "Data",
    "section": "Hello World DataBlock",
    "text": "Hello World DataBlock\nThe argument get_x and get_y operate on an iterable. Let‚Äôs define an interable as our data:\n\ndata = list(range(100))\n\n\ndef get_x(r): return r\ndef get_y(r): return r + 10\ndblock = DataBlock(get_x=get_x, get_y = get_y)\ndsets = dblock.datasets(data)\n\nYou can see a dataset like so:\n\ndsets.train[0]\n\n(89, 99)\n\n\nYou can also see a DataLoader like so:\n\ndls = dblock.dataloaders(data, bs=5)\n\n\nnext(iter(dls.train))\n\n(tensor([57, 66, 73, 30, 14]), tensor([67, 76, 83, 40, 24]))\n\n\n\nWith A DataFrame\nSimilarly, you can operate on one row at a time:\n\nimport pandas as pd\ndf = pd.DataFrame({'x': range(100), 'y': range(100) })\ndf.head()\n\n\n\n\n\n  \n    \n      \n      x\n      y\n    \n  \n  \n    \n      0\n      0\n      0\n    \n    \n      1\n      1\n      1\n    \n    \n      2\n      2\n      2\n    \n    \n      3\n      3\n      3\n    \n    \n      4\n      4\n      4\n    \n  \n\n\n\n\n\ndef get_x(r): return r.x\ndef get_y(r): return r.y + 10\ndblock = DataBlock(get_x=get_x, get_y=get_y)\ndsets = dblock.datasets(df)\n\n\ndsets.train[0]\n\n(78, 88)\n\n\n\ndls = dblock.dataloaders(df, bs=3)\nnext(iter(dls.train))\n\n(tensor([90, 55, 11]), tensor([100,  65,  21]))\n\n\n\ndef tracer(nm):\n    def f(x, nm):\n        # print(f'{nm}:')\n        # print(f'\\tinput: {x}')\n        # import ipdb; ipdb.set_trace()\n        return str(x)\n    return partial(f, nm=nm)\n\n\ndef mult_0(x): return x * 0\ndef add_1(x): return x +1 \ntb = TransformBlock(item_tfms=[tracer('item_tfms')])\n# def get_y(l): return sum(l)\ndb = DataBlock(blocks=(TransformBlock, TransformBlock),\n               get_x=mult_0,\n               get_y=add_1,\n               item_tfms=lambda x: str(x))\n\n\ndata = L(range(10))\nresult = db.datasets(data)\n\n\ndb.summary(data)\n\nSetting-up type transforms pipelines\nCollecting items from [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nFound 10 items\n2 datasets of sizes 8,2\nSetting up Pipeline: mult_0\nSetting up Pipeline: add_1\n\nBuilding one sample\n  Pipeline: mult_0\n    starting from\n      1\n    applying mult_0 gives\n      0\n  Pipeline: add_1\n    starting from\n      1\n    applying add_1 gives\n      2\n\nFinal sample: (0, 2)\n\n\nCollecting items from [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nFound 10 items\n2 datasets of sizes 8,2\nSetting up Pipeline: mult_0\nSetting up Pipeline: add_1\nSetting up after_item: Pipeline: <lambda> -> ToTensor\nSetting up before_batch: Pipeline: \nSetting up after_batch: Pipeline: \n\nBuilding one batch\nApplying item_tfms to the first sample:\n  Pipeline: <lambda> -> ToTensor\n    starting from\n      (0, 2)\n    applying <lambda> gives\n      (0, 2)\n    applying ToTensor gives\n      (0, 2)\n\nAdding the next 3 samples\n\nNo before_batch transform to apply\n\nCollating items in a batch\n\nNo batch_tfms to apply\n\n\n\nresult.train[0]\n\n(0, 5)\n\n\n\nresult = db.dataloaders(data, bs=3)\n\n\nthing = iter(result.train)\n\n\nnext(thing)\n\n(('0', '0', '0'), ('6', '7', '4'))\n\n\n\nnext(thing)\n\n(('0', '0', '0'), ('9', '5', '3'))\n\n\n\n??TransformBlock\n\n\ndb = DataBlock(blocks=(TransformBlock, tb),\n              get_y=lambda x: str(x),\n              batch_tfms=tracer('batch_tfms'))\n\n\nresult = db.datasets(data)\nresult = db.dataloaders(data, bs=3)\n\n\nresult\n\n<fastai.data.core.DataLoaders>\n\n\n\nthing = iter(result.train)\n\n\nnext(thing)\n\n(('1', '5', '6'), ('1', '5', '6'))\n\n\n\nf = aug_transforms()[0]\n\n\nf\n\nFlip -- {'size': None, 'mode': 'bilinear', 'pad_mode': 'reflection', 'mode_mask': 'nearest', 'align_corners': True, 'p': 0.5}:\nencodes: (TensorImage,object) -> encodes\n(TensorMask,object) -> encodes\n(TensorBBox,object) -> encodes\n(TensorPoint,object) -> encodes\ndecodes:"
  },
  {
    "objectID": "notes/fastai/batch_predicitions.html",
    "href": "notes/fastai/batch_predicitions.html",
    "title": "Batch Predictions",
    "section": "",
    "text": "How to make batch predictions in fastai\nMaking batch predictions on new data is not provided ‚Äúout of the box‚Äù in fastai. This is how you can achieve that:\nAdd this method to learner:\n@patch\ndef predict_batch(self:Learner, item, rm_type_tfms=None, with_input=False):\n    dl = self.dls.test_dl(item, rm_type_tfms=rm_type_tfms, num_workers=0)\n    inp,preds,_,dec_preds = self.get_preds(dl=dl, with_input=True, with_decoded=True)\n    i = getattr(self.dls, 'n_inp', -1)\n    inp = (inp,) if i==1 else tuplify(inp)\n    dec_inp, nm = zip(*self.dls.decode_batch(inp + tuplify(dec_preds)))\n    res = preds,nm,dec_preds\n    if with_input: res = (dec_inp,) + res\n    return res\nYou can then use this method like so:\n>>> from fastai.text.all import *\n>>> from predict_batch import predict_batch # this file.  If you don't import just define in your script.\n>>> dls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test')\n>>> learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=accuracy)\n>>> learn.fine_tune(4, 1e-2)\n>>> learn.predict_batch([\"hello world\"]*4)\n(TensorText([[0.0029, 0.9971],\n         [0.0029, 0.9971],\n         [0.0029, 0.9971],\n         [0.0029, 0.9971]]),\n ('pos', 'pos', 'pos', 'pos'),\n TensorText([1, 1, 1, 1]))\nAlternatively, you can just patch the predict function so it works on batches:\n@patch\ndef predict(self:Learner, item, rm_type_tfms=None, with_input=False):\n    dl = self.dls.test_dl(item, rm_type_tfms=rm_type_tfms, num_workers=0)\n    inp,preds,_,dec_preds = self.get_preds(dl=dl, with_input=True, with_decoded=True)\n    i = getattr(self.dls, 'n_inp', -1)\n    inp = (inp,) if i==1 else tuplify(inp)\n    dec = self.dls.decode_batch(inp + tuplify(dec_preds))\n    dec_inp,dec_targ = (tuple(map(detuplify, d)) for d in zip(*dec.map(lambda x: (x[:i], x[i:]))))\n    res = dec_targ,dec_preds,preds\n    if with_input: res = (dec_inp,) + res\n    return res\nOther notes h/t zach:\nlearn.dls.vocab or learn.dls.categorize.vocab is another way to get the class names."
  },
  {
    "objectID": "notes/fastai/01_fundamentals.html",
    "href": "notes/fastai/01_fundamentals.html",
    "title": "Fundamentals",
    "section": "",
    "text": "import TOCInline from ‚Äò@theme/TOCInline‚Äô;\n\n\n\nDataLoaders is a thin class around DataLoader, and makes them available as train and valid.\nSame thing applies to Datasets and Dataset.\nIn pytorch, Dataset is fed into a DataLoader.\n\n\n\n\nUse this to create DataLoaders\n\nbears = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=Resize(128))\nDataBlocks are a template for creating DataLoaders, and need to be instantiated somehow - for example given a path where to find the data:\ndls = bears.dataloaders(path)\nYou can modify the settings of a DataBlock with new:\nbears = bears.new(item_tfms=RandomResizedCrop(128, min_scale=0.3)) #book has more examples\ndls = bears.dataloaders(path)\nYou can sanity check / see transformed data with show_batch:\n>>> dls.train.show_batch(max_n=8, nrows=2, unique=True)\n... images\nYou also use DataBlocks for data augmentation, with batch_tfms:\nbears = bears.new(\n    item_tfms=Resize(128),        \n    batch_tfms=aug_transforms(mult=2)\n)\ndls = bears.dataloaders(path)\ndls.train.show_batch(max_n=8, nrows=2, unique=True)\n\n\n\nMost things use learn.fine_tune(), when you cannot fine-tune like tabular data, you often use learn.fit_one_cycle\nYou can also do learn.show_results(...)\nfrom fastai.vision.all import *\npath = untar_data(URLs.PETS)/'images'\n\ndef is_cat(x): \n    return x[0].isupper()\ndls = ImageDataLoaders.from_name_func(\n        path=str(path), \n        fnames=get_image_files(path), \n        valid_pct=0.2, \n        seed=42,\n        label_func=is_cat, \n        item_tfms=Resize(224))\n\nlearn = cnn_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(1)\nMore info on what this is in later sections.\n\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\nAlso see top losses:\ninterp.plot_top_losses(5, nrows=1)\n\n\n\nYou can get a ImageClassifierCleaner which allows you to choose (1) a category and (2) data partition (train/val) and shows you the highest loss items so you can decide whether to Keep, Delete, Change etc.\ncleaner = ImageClassifierCleaner(learn)\ncleaner\nThe thing doesn‚Äôt actually delete/change anything but gives you the idxs that allow you to do things with them\nfor idx in cleaner.delete(): cleaner.fns[idx].unlink()\nfor idx,cat in cleaner.change(): shutil.move(str(cleaner.fns[idx]), path/cat)\n\n\n\nSaving a model can be done with learn.export, when you do this, fastai will save a file called ‚Äúexport.pkl‚Äù\nlearn.export()\nload_learner can be used to load a model\nlearn_inf = load_learner(path/'export.pkl')\n\n\n\nWhen you call predict, you will get three things: (1) class, (2) the index of the predicted category (3) Probabilities of each category\n>>> learn_inf.predict('images/grizzly.jpg')\n('grizzly', tensor(1), tensor([9.0767e-06, 9.9999e-01, 1.5748e-07]))\nYou can see all the classes with dls.vocab:\n>>> learn_inf.dls.vocab\n(#3) ['black','grizzly','teddy']\nZach: learn.dls.vocab or learn.dls.categorize.vocab is another way to get the class names.\n\n\n\n\nYou can open an image with Pilow (PIL)\nim3 = Image.open(im3_path)\nim3\n\n#convert to numpy\narray(im3)\n# convert to pytorch tensor\ntensor(im3)\n\n\n\nCompute avg pixel value for 3‚Äôs and 7‚Äôs\nAt inference time, see which one its similar too, using RMSE (L2 Norm) and MAE (L1 Norm)\n\nKind of like KNN\nTaking an inference tensor, a_3 and calculate distance to mean 3 and 7:\n# MAE & RMSE for 3  vs avg3\ndist_3_abs = (a_3 - mean3).abs().mean()\ndist_3_sqr = ((a_3 - mean3)**2).mean().sqrt()\n\n# MAE & RMSE for 3  vs avg7\ndist_7_abs = (a_3 - mean7).abs().mean()\ndist_7_sqr = ((a_3 - mean7)**2).mean().sqrt()\n\n# Use Pytorch Losses to do the same thing for 3 vs avg 7\nF.l1_loss(a_3.float(),mean7), F.mse_loss(a_3,mean7).sqrt()\n\n\n\nTake the mean over an axis:\ndef mnist_distance(a,b): \n    #(-2,1) means take the average of the last 2 axis\n    return (a-b).abs().mean((-2,-1))\n\n\n\n\n\n\n# the loss function\ndef mse(y, yhat): \n    return (y - yhat).square().mean().sqrt()\n\n# the function that produces the data\ndef quadratic(x, params=[.75, -25.5, 15]):\n    a,b,c = params\n    noise = (torch.randn(len(x)) * 3)\n    return a*(x**2) + b*x +c + noise\n\n# generate training data\nx = torch.arange(1, 40, 1)\ny = quadratic(x)\n\n# define the training loop\ndef apply_step(params, pr=True):\n    lr = 1.05e-4\n    preds = quadratic(x, params)\n    loss = mse(preds, y)\n    loss.backward()\n    params.data -= params.grad.data * lr\n    if pr: print(f'loss: {loss}')\n    params.grad = None\n\n# initialize random params\nparams = torch.rand(3)\nparams.requires_grad_()\nassert params.requires_grad\n\n# train the model\nfor _ in range(1000):\n    apply_step(params)\n\n\n\nA Dataset in pytorch is required to return a tuple of (x,y) when indexed. You can do this in python as follows:\n# Turn mnist data into vectors 3dim -> 2dim\ntrain_x = torch.cat([stacked_threes, stacked_sevens]).view(-1, 28*28)\n# Generate label tensor\ntrain_y = tensor([1]*len(threes) + [0]*len(sevens)).unsqueeze(1)\n# Create dataset\ndset = list(zip(train_x,train_y))\n\n# See shapes from first datum in the dataset\n>>> x,y = dset[0]\n>>> x.shape, y.shape\n(torch.Size([784]), torch.Size([1]))\n\n\n# Do the same thing for the validation set\n....\n\n\n# `@` and dot product is the same:\na, b = torch.rand(10), torch.rand(10)\nassert a.dot(b) == a@b\n\n# define model\ndef init_params(size, std=1.0): \n    return (torch.randn(size)*std).requires_grad_()\nweights = init_params((28*28,1))\nbias = init_params(1)\n\ndef linear1(xb): return xb@weights + bias\n\n#naive loss (for illustration)\ncorrects = (preds>0.0).float() == train_y\ncorrects.float().mean().item()\n\n# define loss\ndef mnist_loss(preds, targets):\n    preds = preds.sigmoid() #squash b/w 0 and 1\n    return torch.where(targets==1, 1-preds, preds).mean() # average distance loss\n\n\nYou want to load your data in batches, so you will want to create a dataloader. Recall that in pytorch, a Dataset is required to return a tuple of (x,y) when indexed, which is quite easy to do:\n# define a data loader using `dset`\ndset = list(zip(train_x,train_y))\nPytorch offers a utility to then create a Dataloader from a dataset, but Jeremy basically rolled his own (w/same api):\ndl = DataLoader(dset, batch_size=256)\nvalid_dl = DataLoader(valid_dset, batch_size=256)\n\n\n\n\ndef calc_grad(xb, yb, model):\n    preds = model(xb)\n    loss = mnist_loss(preds, yb)\n    loss.backward()\n\ndef train_epoch(model, lr, params):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        for p in params:\n            p.data -= p.grad*lr\n            p.grad.zero_() #updates in place\n\n### Calculate metrics\ndef batch_accuracy(xb, yb):\n    preds = xb.sigmoid()\n    correct = (preds>0.5) == yb\n    return correct.float().mean()\n\ndef validate_epoch(model):\n    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n    return round(torch.stack(accs).mean().item(), 4)\n\n# Train model\nlr = 1.\nparams = weights,bias\ntrain_epoch(linear1, lr, params)\nvalidate_epoch(linear1)\n\n# Train model w/epochs\nfor i in range(20):\n    train_epoch(linear1, lr, params)\n    print(validate_epoch(linear1), end=' ')\n\n\n\n\nBlueprint: 1. Define a dataset and then a dataloader 2. Create a model, which will have parameters 3. Create an optimizer, that: - Updates the params: params.data -= parmas.grad.data * lr - Zeros out the gradients: setting params.grad = None or zeroing out the gradients with params.grad.zero_() 4. Generate the predictions 5. Calculate the loss 6. Calculate the gradients loss.backward() 7. Using the optimizer, update the weights step and zero out the gradients zero_grad 8. Put 4-7 in a loop.\nCreate an optimizer and use nn.Linear\nlinear_model = nn.Linear(28*28,1)\nw,b = linear_model.parameters()\n\n# Define an optimizer\nclass BasicOptim:\n    def __init__(self,params,lr): self.params,self.lr = list(params),lr\n\n    def step(self, *args, **kwargs):\n        for p in self.params: p.data -= p.grad.data * self.lr\n\n    def zero_grad(self, *args, **kwargs):\n        for p in self.params: p.grad = None\n\nopt = BasicOptim(linear_model.parameters(), lr)\n# alternative, fastai provides SGD\nopt = SGD(linear_model.parameters(), lr)\n\n# Define Metrics\ndef batch_accuracy(xb, yb):\n    preds = xb.sigmoid()\n    correct = (preds>0.5) == yb\n    return correct.float().mean()\n\n# Helper to calculate metrics on validation set\ndef validate_epoch(model):\n    accs = [batch_accuracy(model(xb), yb) for xb,yb in valid_dl]\n    return round(torch.stack(accs).mean().item(), 4)\n\ndef train_epoch(model):\n    for xb,yb in dl:\n        calc_grad(xb, yb, model)\n        opt.step()\n        opt.zero_grad()\n\n\ndef train_model(model, epochs):\n    for i in range(epochs):\n        train_epoch(model)\n        print(validate_epoch(model), end=' ')\n\n\ntrain_model(linear_model, 20)\n\n\nWe can substitute the above with learner.fit from fastai We just have to supply the following:\n\nDataloaders\nModel\nOptimization function\nLoss function\nMetrics\n\ndls = DataLoaders(dl, valid_dl)\nlearn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD, \n                loss_func=mnist_loss,\n                metrics=batch_accuracy)\n\nlearn.fit(10, lr=lr)\nWhat if you used the full power of fastai? It would look like this:\ndls = ImageDataLoaders.from_folder(path)\n# Lots of things have defaults like optimization func\nlearn = cnn_learner(dls, resnet18, pretrained=False,\n                    loss_func=F.cross_entropy, \n                     metrics=accuracy)\nlearn.fit_one_cycle(1, 0.1)\n\n\n\n\n\nThe next step is to introduce a non-linearity\nsimple_net = nn.Sequential(\n    nn.Linear(28*28, 30),\n    nn.ReLU(),\n    nn.Linear(30, 1)\n)\n\n# Construct the learner as before\nlearn = learner(dls, simple_net, opt_func=SGD,\n               loss_func=mnist_loss, metrics=batch_accuracy)\n\nlearner.fit(40, 0.1)\n\n\nThe training history is saved in learn.recorder. You can plot your training progress with:\nplt.plot(learn.recorder.values).itemgot(2)"
  },
  {
    "objectID": "notes/linux/osx.html",
    "href": "notes/linux/osx.html",
    "title": "OSX Shell Tips",
    "section": "",
    "text": "Key Repeat Rate\nAdd days to your lifespan by Increasing the key repeat rate. Run the following in the terminal then restart. Protip by Michael Musson.\ndefaults write -g InitialKeyRepeat -int 13 # normal minimum is 15 (225 ms)\ndefaults write -g KeyRepeat -int 1 # normal minimum is 2 (30 ms)\n\n\nA better way to search text: ack\nInstall ack:\nbrew install ack\nSearch files for text, super fast and returns results in a very nice way. By default will search recursively from the current directory and it skips unimportant files by default.\nack \"search string\"\n\n\nKeyboard Tricks (OS X)\nSet your option key to Esc+ in iTerm under Profiles>Keys\n\n\ncontrol-W delete word backwards\noption-D delete word forwards\ncontrol-K delete until end of line\n\n\n\nMy .zshrc file\nStored at ~/.zshrc\nI used to have ohmyzsh but it made my shell too slow. This is good enough for me.\n# #speed startup time https://medium.com/@dannysmith/little-thing-2-speeding-up-zsh-f1860390f92\nautoload -Uz compinit\nfor dump in ~/.zcompdump(N.mh+24); do\n  compinit\ndone\ncompinit -C\n####\n\nPROMPT='%(?.%F{green}‚àö.%F{red}?%?)%f %B%F{157}%1~%f%b %F{231}%# '\n\nautoload -Uz vcs_info\nprecmd_vcs_info() { vcs_info }\nprecmd_functions+=( precmd_vcs_info )\nsetopt prompt_subst\nRPROMPT=\\$vcs_info_msg_0_\nzstyle ':vcs_info:git:*' formats '%F{141}(%b)%r%f'\nzstyle ':vcs_info:*' enable git\n\nalias ls=\"colorls\"\nalias python=\"python3\"\n\n# install jupyter kernel with pipenv\nfunction install-jupyter {\n  if [ -n \"${PIPENV_ACTIVE+1}\" ]; then\n    VENV_NAME=`echo ${VIRTUAL_ENV} | cut -d '/' -f 7`\n    echo \"creating Jupyter kernel named $VENV_NAME\"\n    pipenv install --skip-lock ipykernel\n    python -m ipykernel install --user --name=$VENV_NAME\n  fi\n}\n\n## automatically activate pipenv shell upon cd\nfunction auto_pipenv_shell {\n    if [ ! -n \"${PIPENV_ACTIVE+1}\" ]; then\n        if [ -f \"Pipfile\" ] ; then\n            pipenv shell\n        fi\n    fi\n}\n\nfunction cd {\n    builtin cd \"$@\"\n    auto_pipenv_shell\n}\n\n#extra stuff\nexport CLICOLOR=1\nexport LSCOLORS=GxFxCxDxBxegedabagaced\nGREP_OPTIONS=\"--color=always\";export GREP_OPTIONS\n__git_files () { \n    _wanted files expl 'local files' _files     \n}"
  },
  {
    "objectID": "notes/linux/bash_scripting.html",
    "href": "notes/linux/bash_scripting.html",
    "title": "Cheatsheet",
    "section": "",
    "text": "Link to class.\nLink to GitHub repo\n\nimport TOCInline from ‚Äò@theme/TOCInline‚Äô;\n;\n\n\n\nwas originally a program called bin/sh\nBourne Shell: introduced more advanced structure into the shell.\n\nBourne Again Shell (Bash): Second iteration of Bourne Shell.\n\n\n\n\nls -a ~/ | grep bash\n    Ôíâ  .bash_history\n    Ôíâ  .bash_profile\n    ÔÖõ  .bash_profile.backup\n    ÔÖõ  .bash_profile.bensherman\n    ÔÖõ  .bash_profile_copy\n    ÔÑï  .bash_sessions/\n    Ôíâ  git-completion.bash\n\n\n\n.bash_profile: executed when you login -> configures the shell when you get an initial command prompt. This is different than .bashrc.\n\ncommonly loads the ~/.bashrc file as well.\nbin is traditionally the folder for binaries.\nbash_profile is designed to run when you login, so if you change it will not refresh until you login next time.\n\n\n\n\n\n.bashrc it is executed simply before the command shell comes up, does not have to wait until you login.\netc/bashrc are system bashrc files which is like a ‚Äútemplate‚Äù for user bashrc files. Anytime a new user is created, it inherits from this template and sometimes automated customizations are applied. This is usually done by simply importing etc/bashrc from each user‚Äôs bashrc file.\nenv will list all env variables.\nto apply .bashrc you just have to run the command bash as it will start another shell from your current one. However, if you run bash you can now exit without closing the shell, because a shell is running inside another shell.\n\n\n\n\n\n~/.bash_history contains lots of history. By default will only capture last 100 but you can change this setting.\n\nyou can exlude something from saving to history (like passwords) by using an ignorespace\nthe environment variable HIST_CONTROL can be used to control how much history to keep and settings about what should not be logged. One way to turn off loggin is: bash     export HISTCONTROL=$HISTCONTROL:ignorespace this allow you to skip logging by adding a space to the the beginning of any command. If you want to see what is in HIST_CONTROL you will see:\n> cat ~/.bash_history | grep HISTCONTROL\nHISTCONTROL=ignoredups:ignorespace\nignoredups was already set to this variable.\n\n\n\n\n\nDoesn‚Äôt always exist on a system. in most cases the contents of the ~/.bash_logout will be empty or contain a comment.\nThe role of this file is to execute things when you exit the shell. If you close the shell it will not work, you have to do a clean exit instead.\nCommon use is to use this to clear out ~/.bashrc with the original to clear out any changes the user may have made. You can accomplish this by copying a backup:\ncp ~/.bashrc.original ~/.bashrc\n\n\n\n\n\nPut your shell scripts in a folder you can find them. We can put them in ~/bin:\n> mkdir bin\nMake sure in ~/.bash_profile you have:\nPATH=$PATH:$HOME/bin\nexport PATH\n\n\n\nTo make test.sh executable run command chmod u+x test.sh\n\nYou can also run chmod 755\n\n\n\n\ncan use any name that is not an environment variable (check with env).\nby convention variable names in ALLCAPS. bash     > FIRSTNAME=\"Hamel\"\n  - No space b/w = and value.\n  - Good idea to __always__ put value in double quotes `\"`, although this is not required in every case.  \nAs a practice you want to use export command to set is as an environment variable. This makes the variable available to any subprocess that starts from the shell. Read more about this here.\n> export FIRSTNAME\n> echo \"Hello, $FIRSTNAME\"\n\"Hello Hamel\"\n\n> export FIRSTNAME=\"Hamel\" # do this in one step\nThe above example could work without export, too just reinforcing that its a good idea to use this as a habit. You can do this in one step:\n\n\n\n> export TODAYSDATE=`date`  # executes date command\n\n\n\n\n```bash\nMYUSERNAME='hamel'\nMYPASSWORD='password'\nSTARTOFSCRIPT=`date`\n\necho \"My login name for this app is $MYUSERNAME\"\necho \"My login password for this app is $MYPASSWORD\"\necho \"I started this script at $STARTOFSCRIPT\"\n\nENDOFSCRIPT=`date`\n\necho \"I ended the script at $ENDOFSCRIPT\"\n```\n\nThese variables only live within the sub-shell that executes the script.\n\n\n\n\n\nMethod 1 (Static): Assign command result to variable. Only runs the command at time of variable assignment.\n\n    TODAYSDATE=`date`\n    USERFILES=`find /home -user user` # find all directories owned by the user \"user\"\n\n    echo \"Today's Date: $TODAYSDATE\"\n    echo \"All files owned by USER: $USERFILES\"\n\nMethod 2: Use an alias, which allows you to run a command every time you call the alias. For aliases to work this way you must use the shopt command, which allows aliases to be useable in shell scripts. Technically referred to as ‚Äúexpanding aliases within a subshell‚Äù.\n\n    #!/bin/bash\n    shopt -s expand_aliases\n\n    # notice that we don't use backticks here because the command we want to execute is put in \"..\"\n    alias TODAY=\"date\" \n    alias UFILES=\"find /home -user user\"\n\n\n    A=`TODAY` #Executes the command date\n    B=`UFILES`#Executes the command \n    echo \"With Alias, TODAY is: $A\" echo \"With Alias, UFILES is: $B\"\n\n\n\n\nValue = 0 means everything is ok\nValue != 0 means something is wrong.\nSee last exit status w/ the $? command:\n\n    > ls\n    > echo $?\n    0\n\n\n\nUnlike python, shell scripts will continue executing even if there is an error. You can prevent this by using set -e\n\n    set -e # means exit the shell if there is an error, don't continue.\n\n\n\n\n    expr 1 + 2\n    expr 2 \\* 2 # you have to escape the *\n    expr \\( 2 + 2 \\) \\* 4  # you must also escape the ( )\n\nCaveat: You need a space on each side of the operator.\n\n\n\n\n\n\nenv and printenv will tell you your global vars\nset will give you things from your session. This will also usually contain everything from your global scope. set is a superset of env.\nReserved names: see study guide or google it.\n\n\n\nunset MY_VAR\n\n\n\n\n\n$ escapes a single character.\nsingle quotes '..' treats something as a string, escapes the whole thing\ndouble quotes do not escape anything.\n\n> echo \"\\$COL\"  # this will escape the $\n$COL\n\n> echo '$COL' # single quotes escape things, means the literal string\n$COL\n\n> echo \"$COL\" # does not escape anything\n250\n\n> echo \"The date is: `date`\" # command substitution with bacticks\nThe date is Mon Jul 25"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#using-devnull",
    "href": "notes/linux/bash_scripting.html#using-devnull",
    "title": "Cheatsheet",
    "section": "Using dev/null",
    "text": "Using dev/null\nUse dev/null when you want to discard output and don‚Äôt want to put in the background. /dev/null is a device, and like everything is a file in linux. Everything you write to dev/null just dissapears.\nFor example:\n#!/bin/bash\n#redirect to dev/null example\n\necho \"This is going to the blackhole.\" >> /dev/null\nNote >> (append) or > (overwrite) will work for dev/null, although out of habit in other scenarios it is better to append when unsure using >>."
  },
  {
    "objectID": "notes/linux/bash_scripting.html#redirect-std-error",
    "href": "notes/linux/bash_scripting.html#redirect-std-error",
    "title": "Cheatsheet",
    "section": "Redirect Std Error",
    "text": "Redirect Std Error\nls -l /bin/usr 2> ls-error.txt"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#redirect-std-out-err-into-one-file",
    "href": "notes/linux/bash_scripting.html#redirect-std-out-err-into-one-file",
    "title": "Cheatsheet",
    "section": "Redirect Std Out & Err into one file",
    "text": "Redirect Std Out & Err into one file\nls  -l /bin/sur > ls-output.txt 2>&1\nShortcut: use &\nls  -l /bin/sur &> ls-output.txt"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#dispose-std-err-output-devnull",
    "href": "notes/linux/bash_scripting.html#dispose-std-err-output-devnull",
    "title": "Cheatsheet",
    "section": "Dispose Std Err output /dev/null",
    "text": "Dispose Std Err output /dev/null\nls -l /bin/sur 2> /dev/null"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#brace-expansion",
    "href": "notes/linux/bash_scripting.html#brace-expansion",
    "title": "Cheatsheet",
    "section": "Brace Expansion",
    "text": "Brace Expansion\n> echo Hello-{Foo,Bar,Baz}-World                             \nHello-Foo-World Hello-Bar-World Hello-Baz-World"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#parameter-expansion-like-coalesce",
    "href": "notes/linux/bash_scripting.html#parameter-expansion-like-coalesce",
    "title": "Cheatsheet",
    "section": "Parameter Expansion, Like Coalesce",
    "text": "Parameter Expansion, Like Coalesce\n{parameter:-word}\nIf parameter is unset (i.e., does not exist) or is empty, this expansion results in the value of word. If parameter is not empty, the expansion results in the value of parameter."
  },
  {
    "objectID": "notes/linux/bash_scripting.html#types-of-variables-1",
    "href": "notes/linux/bash_scripting.html#types-of-variables-1",
    "title": "Cheatsheet",
    "section": "Types of Variables",
    "text": "Types of Variables\n# declare int variable:\n> declare -i NEWVAR=10\n\n# inpsect type of NEWVAR\n> declare -p NEWVAR\ndeclare -i NEWVAR=\"10\"\n\n# declare readonly variable\n> declare -r READONLY=\"This is something we cannot overwrite\"\n\n# try to cancel READONLY type\n> declare +r READONLY\n### will result in an error\nVariables in bash are implicitly typed, the type will be inferred from the value you assign.\n\ndetermine the type of a variable: declare -p $MYVAR\ndeclare variable as integer: bash      declare -i NEWVAR=10\nIf you explicitly declare a variable as an int but assign it to a string, it will implicitly convert the value to 0."
  },
  {
    "objectID": "notes/linux/bash_scripting.html#the-if-statement",
    "href": "notes/linux/bash_scripting.html#the-if-statement",
    "title": "Cheatsheet",
    "section": "The if statement",
    "text": "The if statement\n3\necho ‚ÄúGuess the Secret Number‚Äù\necho ‚Äú======================‚Äú\necho ‚Äú‚Äù\necho ‚ÄúEnter a Number Between 1 and 5‚Äù\nread GUESS\n\n\nif [ $GUESS -eq 3 ]\n    then\n        echo ‚ÄúYou guessed the Correct Number!‚Äù\nfi\nTest if a file exists\nFILENAME=$1\necho ‚ÄúTesting for the existence of a file called $FILENAME‚Äù\n\nif [ -a $FILENAME ]\n    then\n        echo ‚Äú$FILENAME does exist!‚Äù\nfi\n\n# negation operator \nif [! -a $FILENAME ]\n    then\n        echo ‚Äú$FILENAME does not exist!‚Äù\nfi\n\n# test multiple expressions in if statement\n\nif [ -f $FILENAME ] && [ -R $FILENAME]\n    then\n        echo ‚ÄúFile $FILENAME exists and is readable.‚Äù\nfi\n-a is the same as -f w.r.t. testing for the existence of a file."
  },
  {
    "objectID": "notes/linux/bash_scripting.html#ifthenelse",
    "href": "notes/linux/bash_scripting.html#ifthenelse",
    "title": "Cheatsheet",
    "section": "If/Then/Else",
    "text": "If/Then/Else\necho ‚ÄúEnter a number between 1 and 3:‚Äù\nread VALUE\n\n# use semicolons for readability\nif [ ‚Äú$VALUE‚Äù -eq ‚Äú1‚Äù ]; then\n    echo ‚ÄúYou entered $VALUE‚Äù\nfi\nUsing an OR statement:\n# another variation\nif [ ‚Äú$VALUE‚Äù -eq ‚Äú1‚Äù ] || [ ‚Äú$VALUE‚Äù -eq ‚Äú2‚Äù ] || [ ‚Äú$VALUE‚Äù -eq ‚Äú3‚Äù ]; then\n    echo ‚ÄúYou entered $VALUE‚Äù\nelse\n    echo ‚ÄúYou didn‚Äôt follow directions!‚Äù\nfi\nRedirect errors to /dev/null\nif [ ‚Äú$VALUE‚Äù -eq ‚Äú1‚Äù ] 2>/dev/null || [ ‚Äú$VALUE‚Äù -eq ‚Äú2‚Äù ] 2>/dev/null || [ ‚Äú$VALUE‚Äù -eq ‚Äú3‚Äù ] 2>/dev/null; then\n    echo ‚ÄúYou entered $VALUE‚Äù\nelse\n    echo ‚ÄúYou didn‚Äôt follow directions!‚Äù\nfi\n\nif [ ‚Äú$VALUE‚Äù -eq ‚Äú1‚Äù ] 2>/dev/null; then\n    echo ‚ÄúYou entered #1‚Äù\nelif ‚Äú \"$VAL‚ÄùE\" -e‚Äú ‚Äù2\" ] 2>/dev/null; then\n    ech‚Äú \"You entered ‚Äù2\"\nelif ‚Äú \"$VAL‚ÄùE\" -e‚Äú ‚Äù3\" ] 2>/dev/null; then\n    ech‚Äú \"You entered ‚Äù3\"\nelse\n    ech‚Äú \"You di‚Äôn't follow direction‚Äù!\"\nfi"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#file-expressions",
    "href": "notes/linux/bash_scripting.html#file-expressions",
    "title": "Cheatsheet",
    "section": "File Expressions",
    "text": "File Expressions"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#string-expressions",
    "href": "notes/linux/bash_scripting.html#string-expressions",
    "title": "Cheatsheet",
    "section": "String Expressions",
    "text": "String Expressions"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#integer-expressions",
    "href": "notes/linux/bash_scripting.html#integer-expressions",
    "title": "Cheatsheet",
    "section": "Integer Expressions",
    "text": "Integer Expressions"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#for-loop",
    "href": "notes/linux/bash_scripting.html#for-loop",
    "title": "Cheatsheet",
    "section": "For Loop",
    "text": "For Loop\n#!/bin/bash\necho ‚ÄúList all the shell scripts contents of the directory‚Äù\nSHELLSCRIPTS=`ls *.sh`\n\n# alternate using for loop\n\nfor FILE in *.sh; do\n    echo ‚Äú$FILE‚Äù\ndone"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#case-statement",
    "href": "notes/linux/bash_scripting.html#case-statement",
    "title": "Cheatsheet",
    "section": "Case Statement",
    "text": "Case Statement\n#!/bin/bash\n\necho ‚Äú1) Choice 2‚Äù\necho ‚Äú2) Choice 2‚Äù\necho ‚Äú3) Choice 3‚Äù\necho ‚ÄúEnter Choice:‚Äù\n\nread MENUCHOICE\n\ncase $MENUCHOICE in\n    1)\n        echo ‚ÄúYou have choosen the first option‚Äù;;\n    2)\n        echo ‚ÄúYou have chosen the second option‚Äù;;\n    3) \n        echo ‚ÄúYou have selected the third option‚Äù;;\n    *)\n        echo ‚ÄúYou have choosen unwisely‚Äù;;\n\nMatch Multiple Case Statements\nAllow many matches to occur"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#while-loop",
    "href": "notes/linux/bash_scripting.html#while-loop",
    "title": "Cheatsheet",
    "section": "While Loop",
    "text": "While Loop\n#!/bin/bash\n\necho ‚ÄúEnter number of times to display message:‚Äù\nread NUM\n\nCOUNT=1\n\n# -le means less than or equal to\nwhile [ $COUNT -le $NUM ]\ndo\n    echo ‚ÄúHello World $COUNT‚Äù\n    COUNT=‚Äú`expr $COUNT + 1`‚Äù\ndone"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#asynchronous-execution-with-wait",
    "href": "notes/linux/bash_scripting.html#asynchronous-execution-with-wait",
    "title": "Cheatsheet",
    "section": "Asynchronous Execution with wait",
    "text": "Asynchronous Execution with wait\n\nThis is the most straightforward implementation of async I have ever seen. You basically decide when to block and wait for a process that you previously decided to run in a child process."
  },
  {
    "objectID": "notes/linux/bash_scripting.html#short-circuit-expressions",
    "href": "notes/linux/bash_scripting.html#short-circuit-expressions",
    "title": "Cheatsheet",
    "section": "Short Circuit Expressions",
    "text": "Short Circuit Expressions\n\n&&: command1 && command2:\nonly run command2 if command1 is successful\n\n\n||: command1 || command2:\nonly run command2 if command1 fails"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#reading-files",
    "href": "notes/linux/bash_scripting.html#reading-files",
    "title": "Cheatsheet",
    "section": "Reading Files",
    "text": "Reading Files\necho ‚ÄúEnter a filename‚Äù \nread FILE\n\nwhile read -r SUPERHERO; do\n    echo ‚ÄúSuperhero Name: $SUPERHERO‚Äù\ndone < ‚Äú$FILE‚Äù"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#reading-files-with-loops",
    "href": "notes/linux/bash_scripting.html#reading-files-with-loops",
    "title": "Cheatsheet",
    "section": "Reading Files with loops",
    "text": "Reading Files with loops"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#file-descriptors",
    "href": "notes/linux/bash_scripting.html#file-descriptors",
    "title": "Cheatsheet",
    "section": "File Descriptors",
    "text": "File Descriptors\nUse a number >= 3 for file descriptors.\n0 - stdin 1 - stdout 2 - stderr\n/dev/null -> generic place where you can redirect streams into nothing.\n#!/bin/bash\n\necho ‚ÄúEnter file name: ‚Äú\nread FILE\n\n# < means readonly,  > means write only,  <> means allow read & write\n# assign file descriptor to filename\nexec 5<>$FILE\n\nwhile read -r SUPERHERO; do\n    echo ‚ÄúSuperhero Name: $SUPERHERO‚Äù\ndone <&5 #use & to reference the file descriptor\n\n# append to end of file.\necho \"File Was Read On: `date`\" >&5\n\n# close file descriptor\nexec 5>&-"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#delimiters-ifs",
    "href": "notes/linux/bash_scripting.html#delimiters-ifs",
    "title": "Cheatsheet",
    "section": "Delimiters (IFS)",
    "text": "Delimiters (IFS)\nIFS - Internal Field Seperator Default is a space\n# this will return a space\necho $IFS\necho \"Enter filename to parse: \"\nread FILE # spacedelim.txt\n\n# https://stackoverflow.com/questions/24337385/bash-preserve-string-with-spaces-input-on-command-line\n\nwhile read -r CPU MEM DISK; do\n    echo \"CPU: $CPU\"\n    echo \"Memory: $MEM\"\n    echo \"Disk: $DISK\"\ndone <\"$FILE\""
  },
  {
    "objectID": "notes/linux/bash_scripting.html#traps-and-signals",
    "href": "notes/linux/bash_scripting.html#traps-and-signals",
    "title": "Cheatsheet",
    "section": "Traps and Signals",
    "text": "Traps and Signals\nhttps://www.gnu.org/software/libc/manual/html_node/Termination-Signals.html - cntrl+c = SIGINT - cntrl+z = SIGTSTP - kill command (without -9 flag) = SIGTERM - kill -9 = SIGKILL; this signal is not sent to the process, it is just killed.\nclear\n\n# first argument is what to exexute \ntrap 'echo \" - Please Press Q to Exit.\"' SIGINT SIGTERM SIGTSTP\n\n# cntrl+c = SIGINT\n# cntrl+z = SIGTSTP  (Suspend, send to background)\n\n\n\nwhile [ \"$CHOICE\" != \"Q\" ] && [ \"$CHOICE\" != \"q\" ]; do\n    echo \"Main Menu\"\n    echo \"=======\"\n    echo \"1) Choice One\"\n    echo \"2) Choice Two\"\n    echo \"3) Choice Three\"\n    echo \"Q) Quit\"\n    read CHOICE\n\n    clear\ndone"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#structure-of-functions-in-a-shell-script",
    "href": "notes/linux/bash_scripting.html#structure-of-functions-in-a-shell-script",
    "title": "Cheatsheet",
    "section": "structure of functions in a shell script",
    "text": "structure of functions in a shell script\nUnlike python, you must define your functions before you call them."
  },
  {
    "objectID": "notes/linux/bash_scripting.html#scope",
    "href": "notes/linux/bash_scripting.html#scope",
    "title": "Cheatsheet",
    "section": "Scope",
    "text": "Scope\nsetting a variable within a function defines that variable globally after that function is called!!!\nGLOBALVAR=‚ÄúGlobally Visible‚Äù\n\n# sample function for function variable scope\nfuncExample () {\n    # local\n    LOCALVAR=‚ÄúLocally Visible‚Äù\n\n    echo ‚ÄúFrom within the function, the variable‚Äôs value is set to $LOCALVAR ‚Ä¶‚Äù\n}\n\n# script start\n\necho ‚Äúthis happens before the function call‚Äù\necho ‚Äú‚Äù\necho ‚ÄúLocal Variable = $LOCALVAR after the function call.‚Äù\necho ‚ÄúGlobal Variable = $GLOBALVAR (before the function call).‚Äù\n\nfuncExample\n\necho ‚Äúthis happens after the function call‚Äù\necho ‚ÄúLocal Variable = $LOCALVAR after the function call.‚Äù\necho ‚ÄúGlobal Variable = $GLOBALVAR (before the function call).‚Äù\nOutput of above code:\nÓÇ∞ ./scope.sh\nthis happens before the function call\n\nLocal Variable =  after the function call.\nGlobal Variable = Globally Visible (before the function call).\nFrom within the function, the variable‚Äôs value is set to Locally Visible ‚Ä¶\nthis happens after the function call\nLocal Variable = Locally Visible after the function call.\nGlobal Variable = Globally Visible (before the function call)."
  },
  {
    "objectID": "notes/linux/bash_scripting.html#functions-with-parameters",
    "href": "notes/linux/bash_scripting.html#functions-with-parameters",
    "title": "Cheatsheet",
    "section": "Functions With Parameters",
    "text": "Functions With Parameters\n# global\nUSERNAME=$1\n\nfuncAgeInDays () {\n    echo ‚ÄúHello $USERNAME, You are $1 Years old.‚Äù\n    echo ‚ÄúThat makes you approx `expr 365 \\* $1` days old‚Äù\n}\n\n#script - start\nread -r -p ‚ÄúEnter your age:‚Äù AGE\n\n# pass in arguments like this\nfuncAgeInDays $AGE"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#nested-functions",
    "href": "notes/linux/bash_scripting.html#nested-functions",
    "title": "Cheatsheet",
    "section": "Nested Functions",
    "text": "Nested Functions\nAuthor of course uses this for organization purposes. When you call a function if it has nested functions the functions defined within will be exposed to the script also.\n# global\nGENDER=$1\n\nfuncHuman () {\n    ARMS=2\n    LEGS=2\n\n    funcMale () {\n        BEARD=1\n        echo ‚ÄúThis man has $ARMS arms and $LEGS legs with $BEARD beard‚Äù\n    }\n\n    funcFemale () {\n        BEARD=0\n        echo ‚ÄúThis woman has $ARMS arms and $LEGS legs with $BEARD beard‚Äù\n    }\n}\n\n# script start\nclear\n\n# determine the actual gender and display the characteristics.\nif  [ ‚Äú$GENDER‚Äù == ‚Äúmale‚Äù ]; then\n    funcHuman\n    funcMale # this function is available after the parent function is called.\nelse\n    funcHuman\n    funcFemale\nfi"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#function-return-and-exit",
    "href": "notes/linux/bash_scripting.html#function-return-and-exit",
    "title": "Cheatsheet",
    "section": "Function Return and Exit",
    "text": "Function Return and Exit\nThis allows you to get arguments from the command line and then exit with a proper code and also use function returns inside scripts.\n# demo of return values and testing results\n\nYES=0\nNO=1\nFIRST=$1\nSECOND=$2\nTHIRD=$3\n\n# function definitions\n\nfuncCheckParams () {\n    # did we get three\n    # -z equivalent to isnull (in this case means not-null b/c of !)\n    if [ ! -z ‚Äú$THIRD‚Äù ]; then\n        echo ‚ÄúWe got three params‚Äù\n        return $YES\n    else\n        echo ‚ÄúWe did not get three params‚Äù\n        return $NO\n    fi\n}\n\n# script start\n\nfuncCheckParams\n# the return value from the function gets stored in $?\nRETURN_VALS=$?\n\nif [ ‚Äú$RETURN_VALS‚Äù -eq ‚Äú$YES‚Äù ]; then\n    echo ‚ÄúWe received three params and they are:‚Äù\n    echo ‚ÄúParam 1: $FIRST‚Äù\n    echo ‚ÄúParam 2: $SECOND‚Äù\n    echo ‚ÄúParam 3: $THIRD‚Äù\nelse\n    echo ‚ÄúUsage: funcreturn.sh [param1] [param2] [param3]‚Äù\n    exit 1\nfi"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#infobox",
    "href": "notes/linux/bash_scripting.html#infobox",
    "title": "Cheatsheet",
    "section": "Infobox",
    "text": "Infobox\nDissappears unless you sleep (see below). Does not come with any buttons.\n# globals\nINFOBOX=${INFOBOX=dialog}\nTITLE=‚ÄúDefault‚Äù\nMESSAGE=‚ÄúSomething to say‚Äù\nXCOORD=10\nYCOORD=20\n\nfuncDisplayInfoBox () {\n    $INFOBOX ‚Äîtitle ‚Äú$1‚Äù ‚Äîinfobox ‚Äú$2‚Äù ‚Äú$3‚Äù ‚Äú$4‚Äù\n    sleep ‚Äú$5‚Äù\n}"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#msgbox",
    "href": "notes/linux/bash_scripting.html#msgbox",
    "title": "Cheatsheet",
    "section": "Msgbox",
    "text": "Msgbox\nMsgbox - dissapears unless you sleep pass --msgbox argument, comes with default ok button and stays on screen.\n# global\nMSGBOX=${MSGBOX=dialog}\nTITLE=‚ÄúDefault‚Äù\nMESSAGE=‚ÄúSome Message‚Äù\nXCOORD=10\nYCOORD=20\n\nfuncDisplayMsgBox () {\n    $MSGBOX ‚Äîtitle ‚Äú$1‚Äù ‚Äîmsgbox ‚Äú$2‚Äù ‚Äú$3‚Äù ‚Äú$4‚Äù\n}"
  },
  {
    "objectID": "notes/linux/bash_scripting.html#menus",
    "href": "notes/linux/bash_scripting.html#menus",
    "title": "Cheatsheet",
    "section": "Menus",
    "text": "Menus\nSee pdf notes/scripts"
  },
  {
    "objectID": "notes/linux/index.html",
    "href": "notes/linux/index.html",
    "title": "Linux & Bash Scripting",
    "section": "",
    "text": "My personal notes on underrated Linux utilities that are useful when working on machine learning projects.\nPhoto by Arget on Unsplash"
  },
  {
    "objectID": "notes/linux/permprocdata.html",
    "href": "notes/linux/permprocdata.html",
    "title": "Processes, Permissions and Moving Data",
    "section": "",
    "text": "import TOCInline from ‚Äò@theme/TOCInline‚Äô;\n;"
  },
  {
    "objectID": "notes/linux/permprocdata.html#references",
    "href": "notes/linux/permprocdata.html#references",
    "title": "Processes, Permissions and Moving Data",
    "section": "References",
    "text": "References\nFiles associated with this tutorial can be found here."
  },
  {
    "objectID": "notes/linux/permprocdata.html#managing-processes-ps-kill-pkill",
    "href": "notes/linux/permprocdata.html#managing-processes-ps-kill-pkill",
    "title": "Processes, Permissions and Moving Data",
    "section": "Managing Processes (ps, kill, pkill)",
    "text": "Managing Processes (ps, kill, pkill)\n\nKill Single Process (ps, kill)\nA common scenario is that you might run a python script to train a model:\n$ python train.py\nLet‚Äôs say you want to kill this script for whatever reason. You might not always be able to type Cntrl + C to stop it, especially if this process is running in the background. (Aside: A way make a program run in the background is with a & for example:$ python train.py & )\nIn order to find this running program, you can use the command ps\n$ ps Gives you basic information (good enough most of the time)\nFlags:\n\n-e Allows you to see all running processes including from other users\n-f Allows you to see additional information about each process\n\nIn order to kill the process you will want to identify it‚Äôs PID for example, if the PID is 501 you can kill this process with the command:\n$ kill 501\n\n\nKilling Multiple Processes (pkill)\nIf you use process-based threading in python with a library like multi-processing, python will instantiate many processes for you. This is common thing to do in python for a task like data processing.\nLet‚Äôs consider the below example. When you run this in the background it will produce 8 processes:\nfrom multiprocessing import Pool\nfrom time import sleep\n\ndef f(x):\n    sleep(1000) # simulate some computation\n    return x*x\n\nif __name__ == '__main__':\n    with Pool(8) as p:\n        print(p.map(f, range(8)))\n\n$ python train_multi.py &\n\nAfter a few seconds, calling the command ps will yield something like this:\nPID TTY           TIME CMD\n 3982 ttys002    0:00.09 ...MacOS/Python train_multi.py\n 4219 ttys002    0:00.00 ...MacOS/Python train_multi.py\n 4220 ttys002    0:00.00 ...MacOS/Python train_multi.py\n 4221 ttys002    0:00.00 ...MacOS/Python train_multi.py\n 4222 ttys002    0:00.00 ...MacOS/Python train_multi.py\n 4223 ttys002    0:00.00 ...MacOS/Python train_multi.py\n 4224 ttys002    0:00.00 ...MacOS/Python train_multi.py\n 4225 ttys002    0:00.00 ...MacOS/Python train_multi.py\n 4226 ttys002    0:00.00 ...MacOS/Python train_multi.py\nYou can find all processes with the file train_multi.py with the pkill command and the -f flag:\n\n\n\nSee Parent / Child Processes (pstree)\npstree is also a helpful utility to see parent/child relationships between processes. You can install pstree on a mac with brew install pstree\nIn the above example, there are 8 sub-processes created by one python process. Running the command\n$ pstree -s train_multi.py\nWill show the process hierarchy. The -s flag allows you to filter parents and descendants of processes containing a string in their command. In the below example, PID 41592 will kill all the 8 child processes seen below\n\n\n\nKilling Process Options\n\nReminder: view processes with ps or top To show processes from all users ps aux\n\nTo restart pid 6996 kill -1 6996\nkill pid 6996 kill -9 6996\n\nYou can kill processes by name (which is also usually listed as the command that started the processes). killall will search for the string int he relevant process.\n\n\nBringing processes back into the foreground\nReminder you put processes in the background with & example is myscript.sh &\nYou can move processes back into the foreground with fg\nfg 1234 brings process 1234 back into the foreground."
  },
  {
    "objectID": "notes/linux/permprocdata.html#bundling-archiving-files-tar",
    "href": "notes/linux/permprocdata.html#bundling-archiving-files-tar",
    "title": "Processes, Permissions and Moving Data",
    "section": "Bundling & Archiving Files (tar)",
    "text": "Bundling & Archiving Files (tar)\nYou commonly want to package a bunch of files together, such as a collection of photos or CSVs, and optionally compress these with its directory structure intact. A common tool for this is tar . This is how you would bundle and compress a directory of CSV files:\n\n\nSending An Archive To A Remote Machine\nIt is often the case you want to send data to a remote machine. The below command creates a directory called data , compresses all files in a local folder named csv_data , with the exception of the sub-directory csv_data/intermediate_files without creating any temporary files locally:\nOptionally, create the directory on the remote machine:\n\nThen, stream the archive directly to remote. Note that providing a ‚Äî instead of a destination filename allows tar to write to a stream (stdout) that can be sent directly to the remote server.\n\n\n\nMoving Files In Different Directories Into An Archive\nIf your files exist in sibling directories, rather than under one parent directory you can use find along with tar . Suppose you want to archive all csv files relative to a directory:\n\nWhen you archive files on the fly above with find you cannot compress the files until the archive is finished being built, therefore you have to compress the tar file with the gzip command:\n\n$ gzip data.tar\n\nTip: some people like to use locate with updatedb instead of find. There are tradeoffs so make sure you read the documentation carefully!\n\n\nUnpacking & Decompressing Archives\nYou can decompress and unpack a tar file, for example data.tar.gz with the following command:\n\n$ tar -xzvf data.tar.gz\n\nIf the data is not compressed, you can leave out the -z flag:\n\n$ tar -xvf data.tar"
  },
  {
    "objectID": "notes/linux/permprocdata.html#file-permissions",
    "href": "notes/linux/permprocdata.html#file-permissions",
    "title": "Processes, Permissions and Moving Data",
    "section": "File Permissions",
    "text": "File Permissions\nBefore we begin, we must introduce some nomenclature:\n\nIf you run the command ls -a you will see something similar to the below output for all of your files in the current directory.\n\nThe file permissions are shown in three-character groupings for three different groups (nine characters total). These three groups are the owner , group , and other users. In this case, the owner name is hamel and the group name is staff\nFor the owner, the file permissions are rwx which means that the owner has read r , write w , and execute x permissions.\nFor the group, the file permissions are r-x which means the group has read and execute permissions, but not write permissions. A group is a collection of users with common permissions.\nFinally, all other users have file permissions of r‚Äì which means only read permissions.\n\nChanging File Permissions\nThere are several ways to change file permissions.\nMethod 1: Using Characters and +, -\nRefer to the nomenclature above to follow along\n\nchmod o-r csvfiles.tar.gz\nRemoves - the ability of other users o to read r the file.\nchmod g+w csvfiles.tar.gz\nAdds + the ability of the group g to write w to the file.\nchmod u+x csvfiles.tar.gz\nAdds + the ability of the owner u to execute x the file.\nchomd a+x csvfiles.tar.gz\nAdds + the ability of all users a to execute x the file.\n\nMethod 2: using numbers\nThis method works by adding up the numbers corresponding to the permissions separately for each user group (owner, group, others). For example:\n\nchmod 777 csvfiles.tar.gz\nThis gives all users the ability to read (4), write( 2), and execute (1) files. In other words 4+2+1 = 7, for the owner, group and other users.\nchmod 732 csvfiles.tar.gz\nThis gives the owner the ability to read, write and execute ( 4+2+1=7), the group the ability to write and execute (2+1=3) and all other users only the ability to write (2).\n\n\n\nChanging Ownership\nYou can change the owner or group assigned to a file like this:\nchown newuser:newgroup file\nThe :newgroup is optional, if you do not specify that the group will stay the same."
  },
  {
    "objectID": "notes/linux/cookbook.html",
    "href": "notes/linux/cookbook.html",
    "title": "Cookbook",
    "section": "",
    "text": "You should browse the table of contents of this book and use the shell scripts contained within off the shelf if possible.\n\nGitHub: https://github.com/hamelsmu/wicked_cool_shell_scripts_2e/\nLink to book on GitHub: https://github.com/hamelsmu/wicked_cool_shell_scripts_2e/blob/master/WickedCoolShellScripts2E.pdf\nBook: https://nostarch.com/wcss2"
  },
  {
    "objectID": "notes/linux/cookbook.html#shift-and-pop-args-off-and-count-args",
    "href": "notes/linux/cookbook.html#shift-and-pop-args-off-and-count-args",
    "title": "Cookbook",
    "section": "shift and $# pop args off and count args",
    "text": "shift and $# pop args off and count args\nshift.sh\n#!/bin/bash\nwhile (( $# )); do\n    echo \"process args: $1\"\n    shift\ndone\nResults in:\n$ ./shift.sh foo bar bash                                                                             \nprocess args: foo\nprocess args: bar\nprocess args: bash\n\nUsing shift for CLI options:\n#!/bin/bash\n# newquota--A frontend to quota that works with full-word flags a la GNU\n\n# quota has three possible flags, -g, -v, and -q, but this script\n#   allows them to be '--group', '--verbose', and '--quiet' too:\n\nflags=\"\"\nrealquota=\"$(which quota)\"\n\nwhile [ $# -gt 0 ]\ndo\n  case $1\n  in\n    --help)  echo \"Usage: $0 [--group --verbose --quiet -gvq]\" >&2\n                       exit 1 ;;\n    --group )  flags=\"$flags -g\";       shift ;;\n    --verbose)  flags=\"$flags -v\";   shift ;;\n    --quiet)  flags=\"$flags -q\";       shift ;;\n    --)  shift;           break ;;\n    *)  break;          # done with 'while' loop!\n  esac\ndone\n\nexec $realquota $flags \"$@\""
  },
  {
    "objectID": "notes/linux/cookbook.html#collect-all-arguments",
    "href": "notes/linux/cookbook.html#collect-all-arguments",
    "title": "Cookbook",
    "section": "$* collect all arguments",
    "text": "$* collect all arguments\nshift2.sh\n#!/bin/bash\nfor var in $*; do\n    echo $var\ndone\nResults in:\n$ ./shift2.sh foo bar bash                                                                             \nprocess args: foo\nprocess args: bar\nprocess args: bash"
  },
  {
    "objectID": "notes/linux/cookbook.html#multi-option-case-statement",
    "href": "notes/linux/cookbook.html#multi-option-case-statement",
    "title": "Cookbook",
    "section": "Multi Option Case Statement",
    "text": "Multi Option Case Statement\nwhile read command args\ndo\n  case $command\n  in\n    quit|exit) exit 0                                  ;;\n    help|\\?)   show_help                               ;;\n    scale)     scale=$args                             ;;\n    *)         scriptbc -p $scale \"$command\" \"$args\"  ;;\n  esac\n\n  /bin/echo -n \"calc> \"\ndone\n\nAnother example of case statement\n  case $1 in\n    1 ) month=\"Jan\"    ;;  2 ) month=\"Feb\"    ;;\n    3 ) month=\"Mar\"    ;;  4 ) month=\"Apr\"    ;;\n    5 ) month=\"May\"    ;;  6 ) month=\"Jun\"    ;;\n    7 ) month=\"Jul\"    ;;  8 ) month=\"Aug\"    ;;\n    9 ) month=\"Sep\"    ;;  10) month=\"Oct\"    ;;\n    11) month=\"Nov\"    ;;  12) month=\"Dec\"    ;;\n    * ) echo \"$0: Unknown numeric month value $1\" >&2; exit 1\n  esac\n  return 0"
  },
  {
    "objectID": "notes/linux/cookbook.html#collecting-stdout-with--",
    "href": "notes/linux/cookbook.html#collecting-stdout-with--",
    "title": "Cookbook",
    "section": "Collecting stdout with -",
    "text": "Collecting stdout with -\necho \"Enter something: \" | cat -"
  },
  {
    "objectID": "notes/linux/cookbook.html#formatting-long-lines-fmt",
    "href": "notes/linux/cookbook.html#formatting-long-lines-fmt",
    "title": "Cookbook",
    "section": "Formatting Long Lines fmt",
    "text": "Formatting Long Lines fmt\nWill make lines no longer than 30 characters, not cutting off any words.\nfmt -w30 long_text.txt"
  },
  {
    "objectID": "notes/linux/cookbook.html#ifs---internal-field-seperator",
    "href": "notes/linux/cookbook.html#ifs---internal-field-seperator",
    "title": "Cookbook",
    "section": "IFS - Internal Field Seperator",
    "text": "IFS - Internal Field Seperator\nSets the internal delimiter\nifs_variable.sh\n#!/bin/bash\nIFS=\":\"\nvar='a:b-c~d'\nfor n in $var\ndo\n    echo \"$n\"\ndone\nResults in\n$ ./1/ifs_variable.sh\na\nb-c~d\n\nIFS in Great Expectations Action\nI‚Äôm using this in the Great Expectations Action to parse a list of arguments given as a string to an input\n# Loop through checkpoints\nSTATUS=0\nIFS=','\nfor c in $INPUT_CHECKPOINTS;do\n    echo \"\"\n    echo \"Validating Checkpoint: ${c}\"\n    if ! great_expectations checkpoint run $c; then\n        STATUS=1\n    fi\ndone\n\n\nIFS for iterating through $PATH\n#!/bin/bash\nIFS=\":\"\nfor directory in $PATH ; do\n   echo $directory\ndone\n\n\nIFS: Double vs.¬†Single Quotes\nWith double quotes the outcome of the command expansion would be fed as one parameter to the source command. Without quotes it would be broken up into multiple parameters, depending on the value of IFS which contains space, TAB and newline by default.\nvar=\"some value\"\n\n# $var fed into cmd as one parameter\ncmd \"$var\"\n\n# $var is fed into cmd as two parameters\n#  delimted by the default IFS character, space\ncmd '$var'"
  },
  {
    "objectID": "notes/linux/cookbook.html#random",
    "href": "notes/linux/cookbook.html#random",
    "title": "Cookbook",
    "section": "$RANDOM",
    "text": "$RANDOM\necho $RANDOM will print out a random number"
  },
  {
    "objectID": "notes/linux/cookbook.html#debugging-shell-scripts--x",
    "href": "notes/linux/cookbook.html#debugging-shell-scripts--x",
    "title": "Cookbook",
    "section": "Debugging Shell Scripts -x",
    "text": "Debugging Shell Scripts -x\nDebug a script:\nbash -x myscript.sh\nOR, within a script:\nset -x # start debugging\n./myscript.sh\nset +x # stop debugging\nAll variables will be substituted and lines that are run will be printed to screen, showing the control flow of the program"
  },
  {
    "objectID": "notes/linux/cookbook.html#sourcing-files-with-.",
    "href": "notes/linux/cookbook.html#sourcing-files-with-.",
    "title": "Cookbook",
    "section": "Sourcing files with .",
    "text": "Sourcing files with .\nSo you can ‚Äúimport‚Äù scripts\n. myscript.sh\n# is equivalent to\nsource myscript.sh"
  },
  {
    "objectID": "notes/linux/cookbook.html#using-functions-to-set-exit-codes",
    "href": "notes/linux/cookbook.html#using-functions-to-set-exit-codes",
    "title": "Cookbook",
    "section": "Using functions to set exit codes",
    "text": "Using functions to set exit codes\n\nvalidAlphaNum()\n{\n  # Validate arg: returns 0 if all upper+lower+digits, 1 otherwise.\n  # Remove all unacceptable chars.\n  validchars=\"$(echo $1 | sed -e 's/[^[:alnum:]]//g')\"\n\n  if [ \"$validchars\" = \"$1\" ] ; then\n    return 0\n  else\n    return 1\n  fi\n}\n\nexit validAlphaNum"
  },
  {
    "objectID": "notes/linux/cookbook.html#know-if-someone-running-the-script-directly-with-bash_source",
    "href": "notes/linux/cookbook.html#know-if-someone-running-the-script-directly-with-bash_source",
    "title": "Cookbook",
    "section": "Know if someone running the script directly with $BASH_SOURCE",
    "text": "Know if someone running the script directly with $BASH_SOURCE\nThe variable $BASH_SOURCE can let you differentiate between when a script is run standalone vs when its invoked from another script:\nif [ \"$BASH_SOURCE\" = \"$0\" ]"
  },
  {
    "objectID": "notes/linux/cookbook.html#xargs",
    "href": "notes/linux/cookbook.html#xargs",
    "title": "Cookbook",
    "section": "xargs",
    "text": "xargs\nhttps://www.cyberciti.biz/faq/linux-unix-bsd-xargs-construct-argument-lists-utility/\n> echo 1 2 3 4 | xargs -n2 -I {} echo hello {} world                                                                                                                                                                                                                                                   \nhello 1 2 world\nhello 3 4 world"
  },
  {
    "objectID": "notes/linux/misc_utils.html",
    "href": "notes/linux/misc_utils.html",
    "title": "Misc Utilities",
    "section": "",
    "text": "import TOCInline from ‚Äò@theme/TOCInline‚Äô;\n;"
  },
  {
    "objectID": "notes/linux/misc_utils.html#history",
    "href": "notes/linux/misc_utils.html#history",
    "title": "Misc Utilities",
    "section": "History",
    "text": "History\n\nSee history with history command\nYou will get a number for each history item.\n\nYou can replay any number n with command !n\nHistory on OS X is stored in ~/.zsh_history\n\n!n refer to command number n in history when you call history"
  },
  {
    "objectID": "notes/linux/misc_utils.html#diff",
    "href": "notes/linux/misc_utils.html#diff",
    "title": "Misc Utilities",
    "section": "Diff",
    "text": "Diff\nYou can difff two files, you usually want to see a unified diff b/c that is easier to read\ndiff -u file1.txt file2.txt"
  },
  {
    "objectID": "notes/linux/misc_utils.html#here-documents",
    "href": "notes/linux/misc_utils.html#here-documents",
    "title": "Misc Utilities",
    "section": "Here Documents",
    "text": "Here Documents\nInstead of using echo, our script now uses cat and a here document. The string EOF (meaning end of file, a common convention) was selected as the token and marks the end of the embedded text. Note that the token must appear alone and that there must not be trailing spaces on the line.\n\nUnlike Echo, all double quotes and single quotes are escaped. Here is an example of the same thing at the command line.\n[me@linuxbox ~]$ foo=\"some text\"\n[me@linuxbox ~]$ cat << _EOF_\n> $foo\n> \"$foo\"\n> '$foo'\n> \\$foo\n> _EOF_ \nsome text \n\"some text\" \n'some text' \n$foo"
  },
  {
    "objectID": "notes/linux/misc_utils.html#mounting-devices",
    "href": "notes/linux/misc_utils.html#mounting-devices",
    "title": "Misc Utilities",
    "section": "Mounting devices",
    "text": "Mounting devices\nSometimes you need to mount these devices. Two common mount points are /mnt and /media. If you mount the device into an existing directory it will cover the contents of that directory making them invisible and unavailable.\nEx: mount device to /mnt\nmount /dev/sb1 /mnt\nEx: mount flash drive\nmount /dev/sdc1 /media\nYou can unmount a device with unmount:\nunmount /dev/sb1"
  },
  {
    "objectID": "notes/linux/misc_utils.html#getting-information-on-mounted-drives",
    "href": "notes/linux/misc_utils.html#getting-information-on-mounted-drives",
    "title": "Misc Utilities",
    "section": "Getting information on mounted drives",
    "text": "Getting information on mounted drives\ndf -h"
  },
  {
    "objectID": "notes/linux/misc_utils.html#permanently-deleting-files-with-shred",
    "href": "notes/linux/misc_utils.html#permanently-deleting-files-with-shred",
    "title": "Misc Utilities",
    "section": "Permanently deleting files with shred",
    "text": "Permanently deleting files with shred\nThis utility writes over files many times in order to erase things. Helpful for sensitive data."
  },
  {
    "objectID": "notes/actions/resources.html",
    "href": "notes/actions/resources.html",
    "title": "Resources",
    "section": "",
    "text": "Introduction\nThese are resources that can help you get started with GitHub Actions:\n\nTalk: Getting started with Actions\nBlog: An Intro To Actions For Data Scientists\n\n\n\nGoing Deeper\nOnce you have a basic understanding, these resources can help you learn more.\n\nSee mlops-github.com for a collection of resources specifically targeted at Data Scientists using GitHub Actions.\nActions official documentation."
  },
  {
    "objectID": "notes/actions/ocotkit.html",
    "href": "notes/actions/ocotkit.html",
    "title": "ocotokit.js",
    "section": "",
    "text": "ocotokit.js is a javascript library that can help you interact with the GitHub API in a easy manner. Some javascript knowledge is helpful, but not required for many simple tasks.\nYou can use the octokit.js client along with the github-script action to quickly interface with the GitHub API to do useful things in Actions (like commenting on an issue.)\nIt is helpful to install node.js when developing scripts that interface with the GitHub API so you can test them locally."
  },
  {
    "objectID": "notes/actions/ocotkit.html#example-1-create-a-comment-on-a-pr",
    "href": "notes/actions/ocotkit.html#example-1-create-a-comment-on-a-pr",
    "title": "ocotokit.js",
    "section": "Example 1: Create A Comment On A PR",
    "text": "Example 1: Create A Comment On A PR\nLet‚Äôs say you want to programatically make a comment on a pull request with a url that includes the branch name, but you are only given the pull request number. We first lookup the branch name associated with the pull request and pass that to the method call that makes an issue comment:\n//Instantiate octokit client\nconst { Octokit } = require(\"@octokit/rest\");\nconst octokit = new Octokit({\n    auth: \"<YOUR_PERSONAL_ACCESS_TOKEN>\",\n  });\n\n  //Take an action (create a comment) triggered by an issue comment\n\n  // Get information about the pr\n  octokit.pulls.get({\n    owner: 'hamelsmu',\n    repo: 'test_html',\n    pull_number: 1\n  }).then( (pr) => {\n    // use the branch name from the pr to make a pr comment\n    var BRANCH_NAME = pr.data.head.ref\n    octokit.issues.createComment({\n        issue_number: 1,\n        owner: 'hamelsmu',\n        repo: 'test_html',\n        body: `[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/hamelsmu/test_html/${BRANCH_NAME}) :point_left: Launch a binder notebook on this branch`\n      })\n  })"
  },
  {
    "objectID": "notes/actions/ocotkit.html#example-2-issue-comment",
    "href": "notes/actions/ocotkit.html#example-2-issue-comment",
    "title": "ocotokit.js",
    "section": "Example 2: Issue Comment",
    "text": "Example 2: Issue Comment\nThis is a simple example of how you can create an issue comment.\n  //Instantiate octokit client\n  const { Octokit } = require(\"@octokit/rest\");\n  const octokit = new Octokit({\n    auth: \"<YOUR_PERSONAL_ACCESS_TOKEN>\",\n    });\n\n  // Create an issue commment\n  var BRANCH_NAME = 'hamelsmu-patch-1'\n  octokit.issues.createComment({\n      issue_number: 1,\n      owner: 'hamelsmu',\n      repo: 'test_html',\n      body: `[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/hamelsmu/test_html/${BRANCH_NAME}) :point_left: Launch a binder notebook on this branch`\n    })"
  },
  {
    "objectID": "notes/web-scraping/browser-to-python.html",
    "href": "notes/web-scraping/browser-to-python.html",
    "title": "Browser requests to python code with a few clicks",
    "section": "",
    "text": "I learned this from Zachary Blackwood‚Äôs 2022 NormConf Talk."
  },
  {
    "objectID": "notes/web-scraping/browser-to-python.html#example-get-a-list-of-subway-restaurants-with-python",
    "href": "notes/web-scraping/browser-to-python.html#example-get-a-list-of-subway-restaurants-with-python",
    "title": "Browser requests to python code with a few clicks",
    "section": "Example: Get A List of Subway Restaurants With Python",
    "text": "Example: Get A List of Subway Restaurants With Python\n\nGo to https://www.subway.com/en-US/locator in Google Chrome\n\n\n\nOpen developer tools using Option + CMD + I\nGo the the network tab, and hit the clear button\n\n\n\nType in a zipcode and search. Look for a network request that seems like it is getting data, in this case GetLocations.ashx... looks super promising.\n\n\n\nRight click on that particular event and select Copy -> Copy as Curl\n\n\n\nGo to curlconverter.com and paste the curl command there.\n\n\nEnjoy your python code that uses this otherwise undocumented API :)"
  },
  {
    "objectID": "notes/web-scraping/browser-to-python.html#bonus-parse-the-response",
    "href": "notes/web-scraping/browser-to-python.html#bonus-parse-the-response",
    "title": "Browser requests to python code with a few clicks",
    "section": "Bonus: Parse The Response",
    "text": "Bonus: Parse The Response\nYou can parse the response data in a hacky way.\n\n# run the code from curlconverter.com, which will give you a `response` object.\n\n>>> import json\n... response_string = response.text\n... json_string = response_string[response_string.index(\"(\") +1:response_string.index('\"AdditionalData\":')-1]+'}'\n... parsed_string = json.loads(json_string)\n... stores = parsed_string['ResultData']\n\n>>> stores\n[{'LocationId': {'StoreNumber': 21809, 'SatelliteNumber': 0},\n  'Address': {'Address1': '4888 NW Bethany Blvd',\n   'Address2': 'Suite K-1',\n   'Address3': 'Bethany Village Centre',\n   'City': 'Portland',\n   'StateProvCode': 'OR',\n   'PostalCode': '97229',\n   'CountryCode': 'US',\n   'CountryCode3': 'USA'},\n  'Geo': {'Latitude': 45.5548,\n   'Longitude': -122.8358,\n   'TimeZoneId': 'America/Los_Angeles',\n   'CurrentUtcOffset': 0},\n  'ListingNumber': 1,\n  'OrderingUrl': 'http://order.subway.com/Stores/Redirect.aspx?s=21809&sa=0&f=r&scc=US&spc=OR',\n  'CateringUrl': 'https://www.ezcater.com/catering/pvt/subway-portland-nw-bethany-blvd',\n  'ExtendedProperties': None},\n..."
  },
  {
    "objectID": "notes/web-scraping/browser-to-python.html#when-to-use-this-approach",
    "href": "notes/web-scraping/browser-to-python.html#when-to-use-this-approach",
    "title": "Browser requests to python code with a few clicks",
    "section": "When to use this approach",
    "text": "When to use this approach\nThis is great for adhoc things, but you probably want to use a headless browser and actually scrape the HTML if you want to do this in a repeatable way. But many times you want to do a one-off scrape, this isn‚Äôt so bad!"
  },
  {
    "objectID": "notes/video_editing.html",
    "href": "notes/video_editing.html",
    "title": "Video Editing",
    "section": "",
    "text": "Youtube Tutorial: https://www.youtube.com/watch?v=yh77878QDVE His playlist: https://www.youtube.com/playlist?list=PLL6tMzF36ox2c‚ÄìSNKiifuP8kEFh80wPu\nCMD + B -> ‚ÄúBlade‚Äù CMD + SHIFT + [ or ] to cut to location"
  },
  {
    "objectID": "notes/video_editing.html#other-tools-to-look-into",
    "href": "notes/video_editing.html#other-tools-to-look-into",
    "title": "Video Editing",
    "section": "Other tools to look into",
    "text": "Other tools to look into\n\nDescript\nRunwayML\ncapcut - from Rajeev\nAdobe Premiere\nFrame - Video collaboration that you use for Upwork etc"
  },
  {
    "objectID": "notes/jupyter/remote_browser.html",
    "href": "notes/jupyter/remote_browser.html",
    "title": "Remote Browser For Jupyter",
    "section": "",
    "text": "It is very common to connect to a remote Jupyter server with your local browser. However, if you lose connection with your remote server, logs printed to the screen may stop streaming. This is common when training deep learning models where training runs can last days or weeks where progress bars are printed to the screen in a notebook.\nTo avoid the issue with your browser loosing connection you can run the browser remotely on the same machine as the Jupyter server, even if your remote server does not have a desktop/GUI interface."
  },
  {
    "objectID": "notes/jupyter/remote_browser.html#fast.ai",
    "href": "notes/jupyter/remote_browser.html#fast.ai",
    "title": "Remote Browser For Jupyter",
    "section": "fast.ai",
    "text": "fast.ai\nThe below youtube link (at timestamp 1:58:33), from fastai Lesson 10 Part 2 (2018) will walk you through how to accomplish this."
  },
  {
    "objectID": "notes/jupyter/Best Way To Launch Jupyter On A Remote Server.html",
    "href": "notes/jupyter/Best Way To Launch Jupyter On A Remote Server.html",
    "title": "Hamel's Blog",
    "section": "",
    "text": "jupyter lab --ip='*' --NotebookApp.token='' --NotebookApp.password='' --port 8081"
  },
  {
    "objectID": "notes/jupyter/shortcuts.html",
    "href": "notes/jupyter/shortcuts.html",
    "title": "My Jupyter Shortcuts",
    "section": "",
    "text": "People complain about ‚Äústate‚Äù in Jupyter. This can be easily avoided by frequently restarting the kernel and running all cells from the top. Thankfully, you can set a hotkey that allows you to do this effortlessly. In Jupyter Lab, go to Settings then Advanced Settings Editor. Copy and paste the below json into the User Prefences pane. If you already have user-defined shortcuts, modify this appropriately.\n{\n    \"shortcuts\": [\n        {\n            \"args\": {},\n            \"command\": \"application:activate-next-tab\",\n            \"keys\": [\n                \"Ctrl Shift ]\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"application:activate-next-tab-bar\",\n            \"keys\": [\n                \"Ctrl Shift .\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"application:activate-previous-tab\",\n            \"keys\": [\n                \"Ctrl Shift [\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"application:activate-previous-tab-bar\",\n            \"keys\": [\n                \"Ctrl Shift ,\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"application:close\",\n            \"keys\": [\n                \"Alt W\"\n            ],\n            \"selector\": \".jp-Activity\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"application:toggle-left-area\",\n            \"keys\": [\n                \"Accel B\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"application:toggle-mode\",\n            \"keys\": [\n                \"Accel Shift D\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"apputils:activate-command-palette\",\n            \"keys\": [\n                \"Accel Shift C\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"apputils:print\",\n            \"keys\": [\n                \"Accel P\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"completer:invoke-console\",\n            \"keys\": [\n                \"Tab\"\n            ],\n            \"selector\": \".jp-CodeConsole-promptCell .jp-mod-completer-enabled\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"completer:invoke-file\",\n            \"keys\": [\n                \"Tab\"\n            ],\n            \"selector\": \".jp-FileEditor .jp-mod-completer-enabled\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"completer:invoke-notebook\",\n            \"keys\": [\n                \"Tab\"\n            ],\n            \"selector\": \".jp-Notebook.jp-mod-editMode .jp-mod-completer-enabled\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"console:linebreak\",\n            \"keys\": [\n                \"Enter\"\n            ],\n            \"selector\": \".jp-CodeConsole[data-jp-interaction-mode='notebook'] .jp-CodeConsole-promptCell\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"console:linebreak\",\n            \"keys\": [\n                \"Accel Enter\"\n            ],\n            \"selector\": \".jp-CodeConsole[data-jp-interaction-mode='terminal'] .jp-CodeConsole-promptCell\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"console:run-forced\",\n            \"keys\": [\n                \"Shift Enter\"\n            ],\n            \"selector\": \".jp-CodeConsole[data-jp-interaction-mode='notebook'] .jp-CodeConsole-promptCell\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"console:run-forced\",\n            \"keys\": [\n                \"Shift Enter\"\n            ],\n            \"selector\": \".jp-CodeConsole[data-jp-interaction-mode='terminal'] .jp-CodeConsole-promptCell\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"console:run-unforced\",\n            \"keys\": [\n                \"Enter\"\n            ],\n            \"selector\": \".jp-CodeConsole[data-jp-interaction-mode='terminal'] .jp-CodeConsole-promptCell\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"debugger:continue\",\n            \"keys\": [\n                \"F9\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"debugger:debug-console\",\n            \"keys\": [\n                \"Accel Shift I\"\n            ],\n            \"selector\": \".jp-CodeConsole\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"debugger:debug-file\",\n            \"keys\": [\n                \"Accel Shift I\"\n            ],\n            \"selector\": \".jp-FileEditor\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"debugger:debug-notebook\",\n            \"keys\": [\n                \"Accel Shift I\"\n            ],\n            \"selector\": \".jp-Notebook\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"debugger:next\",\n            \"keys\": [\n                \"F10\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"debugger:stepIn\",\n            \"keys\": [\n                \"F11\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"debugger:stepOut\",\n            \"keys\": [\n                \"Shift F11\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"debugger:terminate\",\n            \"keys\": [\n                \"Shift F9\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"docmanager:save\",\n            \"keys\": [\n                \"Accel S\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"docmanager:save-as\",\n            \"keys\": [\n                \"Accel Shift S\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"documentsearch:highlightNext\",\n            \"keys\": [\n                \"Accel G\"\n            ],\n            \"selector\": \".jp-mod-searchable\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"documentsearch:highlightPrevious\",\n            \"keys\": [\n                \"Accel Shift G\"\n            ],\n            \"selector\": \".jp-mod-searchable\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"documentsearch:start\",\n            \"keys\": [\n                \"Accel F\"\n            ],\n            \"selector\": \".jp-mod-searchable\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"editmenu:redo\",\n            \"keys\": [\n                \"Accel Shift Z\"\n            ],\n            \"selector\": \"[data-jp-undoer]\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"editmenu:undo\",\n            \"keys\": [\n                \"Accel Z\"\n            ],\n            \"selector\": \"[data-jp-undoer]\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"filebrowser:copy\",\n            \"keys\": [\n                \"Accel C\"\n            ],\n            \"selector\": \".jp-DirListing-content .jp-DirListing-itemText\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"filebrowser:create-main-launcher\",\n            \"keys\": [\n                \"Accel Shift L\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"filebrowser:cut\",\n            \"keys\": [\n                \"Accel X\"\n            ],\n            \"selector\": \".jp-DirListing-content .jp-DirListing-itemText\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"filebrowser:delete\",\n            \"keys\": [\n                \"Delete\"\n            ],\n            \"selector\": \".jp-DirListing-content .jp-DirListing-itemText\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"filebrowser:duplicate\",\n            \"keys\": [\n                \"Accel D\"\n            ],\n            \"selector\": \".jp-DirListing-content .jp-DirListing-itemText\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"filebrowser:go-up\",\n            \"keys\": [\n                \"Backspace\"\n            ],\n            \"selector\": \".jp-DirListing-content .jp-DirListing-itemText\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"filebrowser:paste\",\n            \"keys\": [\n                \"Accel V\"\n            ],\n            \"selector\": \".jp-DirListing-content .jp-DirListing-itemText\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"filebrowser:rename\",\n            \"keys\": [\n                \"F2\"\n            ],\n            \"selector\": \".jp-DirListing-content .jp-DirListing-itemText\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"filebrowser:toggle-main\",\n            \"keys\": [\n                \"Accel Shift F\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"filemenu:close-and-cleanup\",\n            \"keys\": [\n                \"Ctrl Shift Q\"\n            ],\n            \"selector\": \".jp-Activity\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"imageviewer:flip-horizontal\",\n            \"keys\": [\n                \"H\"\n            ],\n            \"selector\": \".jp-ImageViewer\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"imageviewer:flip-vertical\",\n            \"keys\": [\n                \"V\"\n            ],\n            \"selector\": \".jp-ImageViewer\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"imageviewer:invert-colors\",\n            \"keys\": [\n                \"I\"\n            ],\n            \"selector\": \".jp-ImageViewer\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"imageviewer:reset-image\",\n            \"keys\": [\n                \"0\"\n            ],\n            \"selector\": \".jp-ImageViewer\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"imageviewer:rotate-clockwise\",\n            \"keys\": [\n                \"]\"\n            ],\n            \"selector\": \".jp-ImageViewer\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"imageviewer:rotate-counterclockwise\",\n            \"keys\": [\n                \"[\"\n            ],\n            \"selector\": \".jp-ImageViewer\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"imageviewer:zoom-in\",\n            \"keys\": [\n                \"=\"\n            ],\n            \"selector\": \".jp-ImageViewer\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"imageviewer:zoom-out\",\n            \"keys\": [\n                \"-\"\n            ],\n            \"selector\": \".jp-ImageViewer\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"inspector:open\",\n            \"keys\": [\n                \"Accel I\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"kernelmenu:interrupt\",\n            \"keys\": [\n                \"I\",\n                \"I\"\n            ],\n            \"selector\": \"[data-jp-kernel-user]:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"kernelmenu:restart\",\n            \"keys\": [\n                \"0\",\n                \"0\"\n            ],\n            \"selector\": \"[data-jp-kernel-user]:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"runmenu:restart-and-run-all\",\n            \"keys\": [\n                \"0\",\n                \"R\"\n            ],\n            \"selector\": \"[data-jp-kernel-user]:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:restart-and-run-to-selected\",\n            \"keys\": [\n                \"0\",\n                \"S\"\n            ],\n            \"selector\": \"[data-jp-kernel-user]:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:change-cell-to-code\",\n            \"keys\": [\n                \"Y\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:change-cell-to-heading-1\",\n            \"keys\": [\n                \"1\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:change-cell-to-heading-2\",\n            \"keys\": [\n                \"2\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:change-cell-to-heading-3\",\n            \"keys\": [\n                \"3\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:change-cell-to-heading-4\",\n            \"keys\": [\n                \"4\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:change-cell-to-heading-5\",\n            \"keys\": [\n                \"5\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:change-cell-to-heading-6\",\n            \"keys\": [\n                \"6\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:change-cell-to-markdown\",\n            \"keys\": [\n                \"M\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:change-cell-to-raw\",\n            \"keys\": [\n                \"R\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:copy-cell\",\n            \"keys\": [\n                \"C\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:cut-cell\",\n            \"keys\": [\n                \"X\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:delete-cell\",\n            \"keys\": [\n                \"D\",\n                \"D\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:enter-command-mode\",\n            \"keys\": [\n                \"Escape\"\n            ],\n            \"selector\": \".jp-Notebook.jp-mod-editMode\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:enter-command-mode\",\n            \"keys\": [\n                \"Ctrl M\"\n            ],\n            \"selector\": \".jp-Notebook.jp-mod-editMode\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:enter-edit-mode\",\n            \"keys\": [\n                \"Enter\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:extend-marked-cells-above\",\n            \"keys\": [\n                \"Shift ArrowUp\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:extend-marked-cells-above\",\n            \"keys\": [\n                \"Shift K\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:extend-marked-cells-below\",\n            \"keys\": [\n                \"Shift ArrowDown\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:extend-marked-cells-below\",\n            \"keys\": [\n                \"Shift J\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:extend-marked-cells-bottom\",\n            \"keys\": [\n                \"Shift End\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:extend-marked-cells-top\",\n            \"keys\": [\n                \"Shift Home\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:insert-cell-above\",\n            \"keys\": [\n                \"A\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:insert-cell-below\",\n            \"keys\": [\n                \"B\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:merge-cell-above\",\n            \"keys\": [\n                \"Ctrl Backspace\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:merge-cell-below\",\n            \"keys\": [\n                \"Ctrl Shift M\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:merge-cells\",\n            \"keys\": [\n                \"Shift M\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:move-cursor-down\",\n            \"keys\": [\n                \"ArrowDown\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:move-cursor-down\",\n            \"keys\": [\n                \"J\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:move-cursor-up\",\n            \"keys\": [\n                \"ArrowUp\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:move-cursor-up\",\n            \"keys\": [\n                \"K\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:paste-cell-below\",\n            \"keys\": [\n                \"V\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:redo-cell-action\",\n            \"keys\": [\n                \"Shift Z\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:run-cell\",\n            \"keys\": [],\n            \"macKeys\": [\n                \"Ctrl Enter\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:run-cell\",\n            \"keys\": [],\n            \"macKeys\": [\n                \"Ctrl Enter\"\n            ],\n            \"selector\": \".jp-Notebook.jp-mod-editMode\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:run-cell\",\n            \"keys\": [\n                \"Accel Enter\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:run-cell\",\n            \"keys\": [\n                \"Accel Enter\"\n            ],\n            \"selector\": \".jp-Notebook.jp-mod-editMode\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:run-cell-and-insert-below\",\n            \"keys\": [\n                \"Alt Enter\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:run-cell-and-insert-below\",\n            \"keys\": [\n                \"Alt Enter\"\n            ],\n            \"selector\": \".jp-Notebook.jp-mod-editMode\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:run-cell-and-select-next\",\n            \"keys\": [\n                \"Shift Enter\"\n            ],\n            \"selector\": \".jp-Notebook.jp-mod-editMode\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"runmenu:run\",\n            \"keys\": [\n                \"Shift Enter\"\n            ],\n            \"macKeys\": [\n                \"Ctrl Enter\"\n            ],\n            \"selector\": \"[data-jp-code-runner]\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"settingeditor:open\",\n            \"keys\": [\n                \"Accel ,\"\n            ],\n            \"macKeys\": [\n                \"Ctrl Enter\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"settingeditor:save\",\n            \"keys\": [\n                \"Accel S\"\n            ],\n            \"selector\": \".jp-SettingEditor\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"tabsmenu:activate-previously-used-tab\",\n            \"keys\": [\n                \"Accel Shift '\"\n            ],\n            \"selector\": \"body\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"tooltip:dismiss\",\n            \"keys\": [\n                \"Escape\"\n            ],\n            \"selector\": \"body.jp-mod-tooltip .jp-Notebook\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"tooltip:dismiss\",\n            \"keys\": [\n                \"Escape\"\n            ],\n            \"selector\": \"body.jp-mod-tooltip .jp-CodeConsole-promptCell\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"tooltip:launch-console\",\n            \"keys\": [\n                \"Shift Tab\"\n            ],\n            \"selector\": \".jp-CodeConsole-promptCell .jp-InputArea-editor:not(.jp-mod-has-primary-selection):not(.jp-mod-in-leading-whitespace)\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"tooltip:launch-file\",\n            \"keys\": [\n                \"Shift Tab\"\n            ],\n            \"selector\": \".jp-FileEditor .jp-CodeMirrorEditor:not(.jp-mod-has-primary-selection):not(.jp-mod-in-leading-whitespace)\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"tooltip:launch-notebook\",\n            \"keys\": [\n                \"Shift Tab\"\n            ],\n            \"selector\": \".jp-Notebook.jp-mod-editMode .jp-InputArea-editor:not(.jp-mod-has-primary-selection):not(.jp-mod-in-leading-whitespace):not(.jp-mod-completer-active)\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:toggle-all-cell-line-numbers\",\n            \"keys\": [\n                \"Shift L\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:toggle-cell-line-numbers\",\n            \"keys\": [\n                \"L\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:split-cell-at-cursor\",\n            \"keys\": [\n                \"Ctrl Shift -\"\n            ],\n            \"selector\": \".jp-Notebook.jp-mod-editMode\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:toggle-render-side-by-side\",\n            \"keys\": [\n                \"Ctrl Shift R\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:select-all\",\n            \"keys\": [\n                \"Accel A\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:toggle-render-side-by-side-current\",\n            \"keys\": [\n                \"Shift R\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        },\n        {\n            \"args\": {},\n            \"command\": \"notebook:undo-cell-action\",\n            \"keys\": [\n                \"Z\"\n            ],\n            \"selector\": \".jp-Notebook:focus\"\n        }\n    ]\n}"
  },
  {
    "objectID": "notes/jupyter/Fix Jupyter CUDA cache.html",
    "href": "notes/jupyter/Fix Jupyter CUDA cache.html",
    "title": "Hamel's Blog",
    "section": "",
    "text": "[[CUDA]] [[Jupyter tip]]\napparently this is meant to work %config ZMQInteractiveShell.cache_size = 0 %reset -f out is meant to remove all stuff in the cache\nhttps://discord.com/channels/689892369998676007/766837559920951316/1037245359027658762"
  },
  {
    "objectID": "hidden_blog/posts/nbdev2.html",
    "href": "hidden_blog/posts/nbdev2.html",
    "title": "nbdev+Quarto: A new secret weapon for productivity",
    "section": "",
    "text": "Foo bar bee boom"
  },
  {
    "objectID": "hidden_blog/index.html",
    "href": "hidden_blog/index.html",
    "title": "Hamel‚Äôs Blog",
    "section": "",
    "text": "This page is supposed to be secret!\n\n\n\nA listing of all my blog posts can be found here\n\n\n\n\n\n\n\n\n\n\n  \n\n\n\n\nnbdev+Quarto: A new secret weapon for productivity\n\n\n\n\n\nOur favorite tool for software engineering productivity‚Äìnbdev, now re-written with Quarto\n\n\n\n\n\n\nJul 28, 2022\n\n\nHamel Husain, Jeremy Howard\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "opensource.html",
    "href": "opensource.html",
    "title": " Open Source",
    "section": "",
    "text": "My open soruce work has been focused on developer tools and infrastructure. I‚Äôve contributed to projects such as fastai, Metaflow, Kubeflow, Jupyter, and Great Expectations, as well as many others. I list some of these below:"
  },
  {
    "objectID": "opensource.html#fastai",
    "href": "opensource.html#fastai",
    "title": " Open Source",
    "section": " fastai",
    "text": "fastai\nI maintain and contribute to a variety of fastai projects. Below are the projects I‚Äôve been very involved in:\n\n\n\nProject\nDescription\nRole\nOther References\n\n\n\n\nfastpages \nAn easy to use blogging platform for Jupyter Notebooks )\nCreator\nBlog, Talk\n\n\nnbdev \nWrite, test, document, and distribute software packages and technical articles all in one place, your notebook.\nCore Contributor\nBlog, Talk\n\n\nfastcore \nA Python language extension for exploratory and functional programming\nCore Contributor\nBlog\n\n\nghapi \nA Python client for the GitHub API\nCore Contributor\nBlog"
  },
  {
    "objectID": "opensource.html#metaflow",
    "href": "opensource.html#metaflow",
    "title": " Open Source",
    "section": " Metaflow",
    "text": "Metaflow\nI created notebook cards: A tool that allows you to use notebooks to generate reports, visualizations and diagnostics in Metaflow production workflows. Blog"
  },
  {
    "objectID": "opensource.html#kubeflow",
    "href": "opensource.html#kubeflow",
    "title": " Open Source",
    "section": " Kubeflow",
    "text": "Kubeflow\nI‚Äôve worked on several projects related to Kubeflow, mainly around examples and documentation:\n\n\n\nProject\nDescription\nRole\nOther References\n\n\n\n\nGitHub Issue Summarization\nAn end-to-end example of using Kubeflow to summarize GitHub Issues. Became one of the most popular tutorials of Kubeflow.\nAuthor\nInterview with Jeremy Lewi\n\n\nkubeflow/codei-intelligence\nVarious tutorials and applied examples of Kubeflow.\nCore Contributor\nTalk\n\n\nThe Kubeflow Blog\nI used fastpages to create the official Kubeflow blog.\nCore Contributor\nSite"
  },
  {
    "objectID": "opensource.html#jupyter",
    "href": "opensource.html#jupyter",
    "title": " Open Source",
    "section": " Jupyter",
    "text": "Jupyter\nI created the Repo2Docker GitHub Action, which allows you to trigger repo2docker to build a Jupyter enabled Docker images from your GitHub repository. This Action allows you to pre-cache images for your own BinderHub cluster or for mybinder.org.\nThis project was accepted into the official JupyterHub GitHub org."
  },
  {
    "objectID": "opensource.html#great-expectations",
    "href": "opensource.html#great-expectations",
    "title": " Open Source",
    "section": " Great Expectations",
    "text": "Great Expectations\nI developed the Great Expectations GitHub Action that allows you to use Great Expectations in CI/CD Workflows. Blog."
  },
  {
    "objectID": "opensource.html#other",
    "href": "opensource.html#other",
    "title": " Open Source",
    "section": " Other",
    "text": "Other\nI worked as a staff machine learning engineer at GitHub from 2017 - 2022. I led or created the following open source projects that explored the intersection of machine learning, data and the developer workflow:\n\n\n\nProject\nDescription\nRole\nOther References\n\n\n\n\nCode Search Net \nDatasets, tools, and benchmarks for representation learning of code. This was a big part of the inspiration for GitHub‚Äôs eventual work on CoPilot.\nLead\nBlog, Talk\n\n\nMachine Learning Ops\nA collection of resources on how to facilitate Machine Learning Ops with GitHub. This project explored integrations with a wide variety of data science tools with GitHub Actions.\nCreator\nBlog\n\n\nIssue Label Bot\nA GitHub App powered by machine learning that auto-labels issues.\nCreator\nBlog, Talk\n\n\nCovid19-dashboard \nA dashboard of Covid-19 analysis powered by Jupyter notebooks.\nCreator\nNews Article"
  }
]